{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 11: Long Short Term Memory (LSTM) Models\n",
    "# st121395 | Chanapa Pananookooln\n",
    "\n",
    "Today we will build a chatbot using the PyTorch LSTM cell.\n",
    "\n",
    "For the homework we need to find other dataset and train a chatbot with the same code.\n",
    "\n",
    "The dataset I chose is the MultiWOZ dataset.\n",
    "\n",
    "Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics. At a size of 10k dialogues, it is at least one order of magnitude larger than all previous annotated task-oriented corpora.\n",
    "\n",
    "Each dialogue consists of a goal, multiple user and system utterances as well as a belief state.\n",
    "\n",
    "I used MultiWOZ 2.2.\n",
    "\n",
    "This dataset consists of a schema file schema.json describing the ontology and dialogue files dialogues_*.json of dialogue data under the train, dev, and test folders.\n",
    "\n",
    "I only took the data from the train folder to do this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download all train json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in ['001','002','003','004','005','006','007','008','009','010','011','012','013','014','015','016','017']:\n",
    "#     !wget 'https://raw.githubusercontent.com/budzianowski/multiwoz/master/data/MultiWOZ_2.2/train/dialogues_{i}.json'\n",
    "#     !mv 'dialogues_{i}.json' '../../data/multiwoz/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the dialogue from all json files in the train folder and combine them into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "  \n",
    "# for i in ['001','002','003','004','005','006','007','008','009','010','011','012','013','014','015','016','017']:\n",
    "#     if i =='001':\n",
    "#         f = open(f'../../data/multiwoz/dialogues_{i}.json')\n",
    "#         data = json.load(f)\n",
    "#         all_question, all_answer = [], []\n",
    "\n",
    "#         for qa in data:\n",
    "#             all_question.append(qa[u'turns'][0][u'utterance'])\n",
    "#             all_answer.append(qa[u'turns'][1][u'utterance'])\n",
    "\n",
    "#             all_ = pd.DataFrame([all_question,all_answer]).T\n",
    "\n",
    "#     else : \n",
    "#         f = open(f'../../data/multiwoz/dialogues_{i}.json')\n",
    "#         data = json.load(f)\n",
    "#         all_question, all_answer = [], []\n",
    "\n",
    "#         for qa in data:\n",
    "#             all_question.append(qa[u'turns'][0][u'utterance'])\n",
    "#             all_answer.append(qa[u'turns'][1][u'utterance'])\n",
    "\n",
    "#         new = pd.DataFrame([all_question,all_answer]).T\n",
    "\n",
    "#         all_ = pd.concat([all_, new], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i need a place to dine in the center thats exp...</td>\n",
       "      <td>I have several options for you; do you prefer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Guten Tag, I am staying overnight in Cambridge...</td>\n",
       "      <td>I have 4 different options for you. I have two...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hi there! Can you give me some info on Cityroomz?</td>\n",
       "      <td>Cityroomz is located at Sleeperz Hotel, Statio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I am looking for a hotel named alyesbray lodge...</td>\n",
       "      <td>i have their info, what would you like to know?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i need a train on tuesday out of kings lynn</td>\n",
       "      <td>What time of day would you like to leave?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>I would like to visit some of the architecture...</td>\n",
       "      <td>I can indeed. We have several churches and a s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>Hi, can you help me find a moderately priced 3...</td>\n",
       "      <td>I have 4 guesthouses with 3 stars that are mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>Hi! I am looking for a train that arrives by 1...</td>\n",
       "      <td>I have 122 trains that are arriving by 11:30. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>I would like the phone number of an expensive ...</td>\n",
       "      <td>There are five restaurants in the expensive pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Hello, I am looking for a cheap restaurant tha...</td>\n",
       "      <td>It looks like there are no German restaurants ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8437 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  \\\n",
       "0    i need a place to dine in the center thats exp...   \n",
       "1    Guten Tag, I am staying overnight in Cambridge...   \n",
       "2    Hi there! Can you give me some info on Cityroomz?   \n",
       "3    I am looking for a hotel named alyesbray lodge...   \n",
       "4          i need a train on tuesday out of kings lynn   \n",
       "..                                                 ...   \n",
       "241  I would like to visit some of the architecture...   \n",
       "242  Hi, can you help me find a moderately priced 3...   \n",
       "243  Hi! I am looking for a train that arrives by 1...   \n",
       "244  I would like the phone number of an expensive ...   \n",
       "245  Hello, I am looking for a cheap restaurant tha...   \n",
       "\n",
       "                                                     1  \n",
       "0    I have several options for you; do you prefer ...  \n",
       "1    I have 4 different options for you. I have two...  \n",
       "2    Cityroomz is located at Sleeperz Hotel, Statio...  \n",
       "3      i have their info, what would you like to know?  \n",
       "4            What time of day would you like to leave?  \n",
       "..                                                 ...  \n",
       "241  I can indeed. We have several churches and a s...  \n",
       "242  I have 4 guesthouses with 3 stars that are mod...  \n",
       "243  I have 122 trains that are arriving by 11:30. ...  \n",
       "244  There are five restaurants in the expensive pr...  \n",
       "245  It looks like there are no German restaurants ...  \n",
       "\n",
       "[8437 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# all_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# np.savetxt(r'../../data/hotpotqa/multiwoz_all.txt', all.values, fmt='%s', delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class NaiveCustomLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_sz: int, hidden_sz: int):\n",
    "        super().__init__()\n",
    "        self.input_size = input_sz\n",
    "        self.hidden_size = hidden_sz\n",
    "        \n",
    "        # Parameters for computing i_t\n",
    "        self.U_i = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_i = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_i = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        # Parameters for computing f_t\n",
    "        self.U_f = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_f = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_f = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        # Parameters for computing c_t\n",
    "        self.U_c = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_c = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_c = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        # Parameters for computing o_t\n",
    "        self.U_o = nn.Parameter(torch.Tensor(input_sz, hidden_sz))\n",
    "        self.V_o = nn.Parameter(torch.Tensor(hidden_sz, hidden_sz))\n",
    "        self.b_o = nn.Parameter(torch.Tensor(hidden_sz))\n",
    "        \n",
    "        self.init_weights()\n",
    "                \n",
    "    def init_weights(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "         \n",
    "    def forward(self, x, init_states=None):\n",
    "        \"\"\"\n",
    "        forward: Run input x through the cell. Assumes x.shape is (batch_size, sequence_length, input_size)\n",
    "        \"\"\"\n",
    "        bs, seq_sz, _ = x.size()\n",
    "        hidden_seq = []\n",
    "        \n",
    "        if init_states is None:\n",
    "            h_t, c_t = (\n",
    "                torch.zeros(bs, self.hidden_size).to(x.device),\n",
    "                torch.zeros(bs, self.hidden_size).to(x.device),\n",
    "            )\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "            \n",
    "        for t in range(seq_sz):\n",
    "            x_t = x[:, t, :]\n",
    "            \n",
    "            i_t = torch.sigmoid(x_t @ self.U_i + h_t @ self.V_i + self.b_i)\n",
    "            f_t = torch.sigmoid(x_t @ self.U_f + h_t @ self.V_f + self.b_f)\n",
    "            g_t = torch.tanh(x_t @ self.U_c + h_t @ self.V_c + self.b_c)\n",
    "            o_t = torch.sigmoid(x_t @ self.U_o + h_t @ self.V_o + self.b_o)\n",
    "            c_t = f_t * c_t + i_t * g_t\n",
    "            h_t = o_t * torch.tanh(c_t)\n",
    "            \n",
    "            hidden_seq.append(h_t.unsqueeze(0))\n",
    "        \n",
    "        # Reshape hidden_seq tensor to (batch size, sequence length, hidden_size)\n",
    "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
    "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
    "\n",
    "        return hidden_seq, (h_t, c_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our chatbot with Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "from get_free_gpu import get_free_gpu\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "device = get_free_gpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reserved word tokens\n",
    "\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "class Voc:\n",
    "\n",
    "    def __init__(self):        \n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 30  # Maximum sentence length to consider *** The sentences in this dataset is very long so I changed this\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Read query/response pairs and return a Voc object\n",
    "\n",
    "def readVocs(datafile):\n",
    "    print(\"Reading lines...\")    \n",
    "    # Read the file and split into lines\n",
    "    lines = open(datafile, encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    voc = Voc()\n",
    "    return voc, pairs\n",
    "\n",
    "# Boolean function returning True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "\n",
    "def filterPair(p):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "# Filter pairs using the filterPair predicate\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# Using the functions defined above, return a populated voc object and pairs list\n",
    "\n",
    "def loadPrepareData(datafile):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(datafile)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data into voc object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 6389 sentence pairs\n",
      "Trimmed to 5998 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 2220\n",
      "\n",
      "pairs:\n",
      "['hello ! i am looking for the address and phone number of a lebanese restaurant in the centre .', 'ali baba is a lebanese restaurant in the centre area . its address is hills road city . phone number is']\n",
      "['i m looking for a train from london liverpool street .', 'i have many trains that leave from london liverpool street . when would you like to leave ?']\n",
      "['yes i am planning a trip and could use some information on attractions .', 'i can help you with that ! what type of attraction are you looking for ?']\n",
      "['i need help i have robbed', 'please contact the parkside cambridge police station']\n",
      "['i am looking for a particular hotel . its name is called arbury lodge guesthouse', 'would you like me to check availability for arbury lodge guesthouse on a particular date ?']\n",
      "['can i find the parkside police station', 'the parkside police station is in parkside cambridge postcode cb jg . do you need the phone number ?']\n",
      "['i m coming to town and looking for a good japanese restaurant to try near the town centre . do you have any suggestions ?', 'the japanese restaurant near centre is wagamama . the address is saint andrews street .']\n",
      "['know anywhere that has european food ?', 'sure there are lots of good european restaurants in town . is there a price range that you prefer ?']\n",
      "['can you help me find a cheap hotelld you like to arrive ?', 'there are cheap hotels is there an area you prefer ?']\n",
      "['hi . i m looking for a hotel in the east . the internet is not needed .', 'there are no hotels that do not have internet but which do have it . do you have a specific price range you d prefer ?']\n"
     ]
    }
   ],
   "source": [
    "# Load/Assemble Voc and pairs\n",
    "\n",
    "datafile = '../../data/multiwoz/multiwoz_all.txt'\n",
    "voc, pairs = loadPrepareData(datafile)\n",
    "\n",
    "# Print some pairs to validate\n",
    "\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As already mentioned, we will trim out rarely-used\n",
    "words from the vocabulary. This will help improve convergence during\n",
    "training, because with a lower-dimensional input feature space, it will be easier\n",
    "to estimate the probability model $P(y \\mid x)$. We trim as a two-step\n",
    "process:\n",
    "\n",
    "1. Trim words appearing fewer than `MIN_COUNT` times with the previously-given `Voc.trim`\n",
    "   method.\n",
    "\n",
    "2. Filter out all sentence pairs containing trimmed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 1535 / 2217 = 0.6924\n",
      "Trimmed from 5998 pairs to 5417, 0.9031 of total\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 2    # Minimum word count threshold for trimming\n",
    "\n",
    "def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "    # Trim words used under the MIN_COUNT from the voc\n",
    "    voc.trim(MIN_COUNT)\n",
    "    # Filter out pairs with trimmed words\n",
    "    keep_pairs = []\n",
    "    for pair in pairs:\n",
    "        input_sentence = pair[0]\n",
    "        output_sentence = pair[1]\n",
    "        keep_input = True\n",
    "        keep_output = True\n",
    "        # Check input sentence\n",
    "        for word in input_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_input = False\n",
    "                break\n",
    "        # Check output sentence\n",
    "        for word in output_sentence.split(' '):\n",
    "            if word not in voc.word2index:\n",
    "                keep_output = False\n",
    "                break\n",
    "\n",
    "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "        if keep_input and keep_output:\n",
    "            keep_pairs.append(pair)\n",
    "\n",
    "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "    return keep_pairs\n",
    "\n",
    "# Trim vocabulary and pairs\n",
    "\n",
    "pairs_ = trimRareWords(voc, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset into testing and training pair sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "417\n"
     ]
    }
   ],
   "source": [
    "testpairs = pairs_[5000:]\n",
    "pairs  = pairs_[:5000]\n",
    "\n",
    "print(len(pairs))\n",
    "print(len(testpairs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert pairs to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# Return a padded input sequence tensor and the lengths of each original sequence\n",
    "\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# Return a padded target sequence tensor, a padding mask, and the max target length\n",
    "\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.ByteTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# Return all items for a given batch of pairs\n",
    "\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "# Example for validation\n",
    "\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As LSTM takes time series data so we need to convert our pairs of sentences into time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['hello ! i am looking for the address and phone number of a lebanese restaurant in the centre .', 'ali baba is a lebanese restaurant in the centre area . its address is hills road city . phone number is'], ['i m looking for a train from london liverpool street .', 'i have many trains that leave from london liverpool street . when would you like to leave ?'], ['yes i am planning a trip and could use some information on attractions .', 'i can help you with that ! what type of attraction are you looking for ?'], ['i need help i have robbed', 'please contact the parkside cambridge police station'], ['i am looking for a particular hotel . its name is called arbury lodge guesthouse', 'would you like me to check availability for arbury lodge guesthouse on a particular date ?']]\n",
      "============ Target Variable ==============\n",
      "tensor([[  5,   5,  85,   5,   5],\n",
      "        [ 35,  35,  42,  35,  35],\n",
      "        [368, 309,  35, 132,  36],\n",
      "        [  8,  53,  92,  37,  37],\n",
      "        [ 37,   9, 101,  38,  38],\n",
      "        [143, 114, 787, 152, 474],\n",
      "        [ 44, 183,  45, 204, 710],\n",
      "        [ 67,  14,   2,  42,  20],\n",
      "        [ 53,  87,   0,  44, 156],\n",
      "        [616,  20,   0,  67, 152],\n",
      "        [156,  41,   0, 155,  42],\n",
      "        [152,  42,   0,  20, 153],\n",
      "        [ 42,  43,   0,  58, 140],\n",
      "        [153, 325,   0, 149,  31],\n",
      "        [140,  52,   0,  41,  11],\n",
      "        [ 31,  53,   0,  42, 156],\n",
      "        [ 45, 213,   0,  43,  23],\n",
      "        [  2,  14,   0,  77, 185],\n",
      "        [  0, 590,   0,  44, 219],\n",
      "        [  0,  45,   0, 189,  45],\n",
      "        [  0,   2,   0,  45,   2],\n",
      "        [  0,   0,   0,   2,   0]])\n",
      "============ Mask ==============\n",
      "tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 0, 1, 1],\n",
      "        [1, 1, 0, 1, 1],\n",
      "        [1, 1, 0, 1, 1],\n",
      "        [1, 1, 0, 1, 1],\n",
      "        [1, 1, 0, 1, 1],\n",
      "        [1, 1, 0, 1, 1],\n",
      "        [1, 1, 0, 1, 1],\n",
      "        [1, 1, 0, 1, 1],\n",
      "        [1, 1, 0, 1, 1],\n",
      "        [1, 1, 0, 1, 1],\n",
      "        [0, 1, 0, 1, 1],\n",
      "        [0, 1, 0, 1, 1],\n",
      "        [0, 1, 0, 1, 1],\n",
      "        [0, 0, 0, 1, 0]], dtype=torch.uint8)\n",
      "============ Max Target Len ==============\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "pair_batch = pairs[:5]\n",
    "print(pair_batch) # sentence pairs\n",
    "\n",
    "pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "\n",
    "print(\"============ Target Variable ==============\")\n",
    "print(target_variable)\n",
    "print(\"============ Mask ==============\")\n",
    "print(mask)\n",
    "print(\"============ Max Target Len ==============\")\n",
    "print(max_target_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define models\n",
    "Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.LSTM(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        \n",
    "        embedded = self.embedding(input_seq)\n",
    "\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths.cpu())\n",
    "        \n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        \n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        \n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "        \n",
    "        return outputs, hidden\n",
    "    \n",
    "    def init_hidden(self):  \n",
    "        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),\n",
    "                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "        attn_energies = attn_energies.t()\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.LSTM(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))#, bidirectional=True)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "    \n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input_step)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        \n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        \n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure\n",
    "\n",
    "Masked loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train function\n",
    "with With teacher forcing and gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "    \n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            \n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            \n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            \n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "    \n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " max_target_len "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    losslist = []\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    \n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        \n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "            losslist.append(print_loss_avg)\n",
    "        \n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            print(directory)\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n",
    "    return losslist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        \n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        \n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        \n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "       \n",
    "        for _ in range(max_length):\n",
    "            \n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            \n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        \n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "  \n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    \n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    \n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    \n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    \n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    \n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            \n",
    "            input_sentence = input('> ')\n",
    "            \n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            \n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            \n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            \n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "\n",
    "hidden_size = 512\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.5\n",
    "batch_size = 256 \n",
    "loadFilename = None\n",
    "\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n",
      "ipykernel_launcher:4: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
      "Iteration: 10; Percent complete: 0.2%; Average loss: 6.9168\n",
      "Iteration: 20; Percent complete: 0.3%; Average loss: 5.4861\n",
      "Iteration: 30; Percent complete: 0.5%; Average loss: 5.0905\n",
      "Iteration: 40; Percent complete: 0.7%; Average loss: 5.0543\n",
      "Iteration: 50; Percent complete: 0.8%; Average loss: 4.9184\n",
      "Iteration: 60; Percent complete: 1.0%; Average loss: 4.8833\n",
      "Iteration: 70; Percent complete: 1.2%; Average loss: 4.8074\n",
      "Iteration: 80; Percent complete: 1.3%; Average loss: 4.7451\n",
      "Iteration: 90; Percent complete: 1.5%; Average loss: 4.6761\n",
      "Iteration: 100; Percent complete: 1.7%; Average loss: 4.6368\n",
      "Iteration: 110; Percent complete: 1.8%; Average loss: 4.5569\n",
      "Iteration: 120; Percent complete: 2.0%; Average loss: 4.5316\n",
      "Iteration: 130; Percent complete: 2.2%; Average loss: 4.5032\n",
      "Iteration: 140; Percent complete: 2.3%; Average loss: 4.4285\n",
      "Iteration: 150; Percent complete: 2.5%; Average loss: 4.4036\n",
      "Iteration: 160; Percent complete: 2.7%; Average loss: 4.3667\n",
      "Iteration: 170; Percent complete: 2.8%; Average loss: 4.3318\n",
      "Iteration: 180; Percent complete: 3.0%; Average loss: 4.3013\n",
      "Iteration: 190; Percent complete: 3.2%; Average loss: 4.2505\n",
      "Iteration: 200; Percent complete: 3.3%; Average loss: 4.2312\n",
      "Iteration: 210; Percent complete: 3.5%; Average loss: 4.1737\n",
      "Iteration: 220; Percent complete: 3.7%; Average loss: 4.1163\n",
      "Iteration: 230; Percent complete: 3.8%; Average loss: 4.0867\n",
      "Iteration: 240; Percent complete: 4.0%; Average loss: 4.0202\n",
      "Iteration: 250; Percent complete: 4.2%; Average loss: 3.9885\n",
      "Iteration: 260; Percent complete: 4.3%; Average loss: 3.9154\n",
      "Iteration: 270; Percent complete: 4.5%; Average loss: 3.8573\n",
      "Iteration: 280; Percent complete: 4.7%; Average loss: 3.8076\n",
      "Iteration: 290; Percent complete: 4.8%; Average loss: 3.7440\n",
      "Iteration: 300; Percent complete: 5.0%; Average loss: 3.7299\n",
      "Iteration: 310; Percent complete: 5.2%; Average loss: 3.6640\n",
      "Iteration: 320; Percent complete: 5.3%; Average loss: 3.6296\n",
      "Iteration: 330; Percent complete: 5.5%; Average loss: 3.5804\n",
      "Iteration: 340; Percent complete: 5.7%; Average loss: 3.5441\n",
      "Iteration: 350; Percent complete: 5.8%; Average loss: 3.4941\n",
      "Iteration: 360; Percent complete: 6.0%; Average loss: 3.4410\n",
      "Iteration: 370; Percent complete: 6.2%; Average loss: 3.4118\n",
      "Iteration: 380; Percent complete: 6.3%; Average loss: 3.3595\n",
      "Iteration: 390; Percent complete: 6.5%; Average loss: 3.3034\n",
      "Iteration: 400; Percent complete: 6.7%; Average loss: 3.3116\n",
      "Iteration: 410; Percent complete: 6.8%; Average loss: 3.2553\n",
      "Iteration: 420; Percent complete: 7.0%; Average loss: 3.2081\n",
      "Iteration: 430; Percent complete: 7.2%; Average loss: 3.1948\n",
      "Iteration: 440; Percent complete: 7.3%; Average loss: 3.1530\n",
      "Iteration: 450; Percent complete: 7.5%; Average loss: 3.0925\n",
      "Iteration: 460; Percent complete: 7.7%; Average loss: 3.0464\n",
      "Iteration: 470; Percent complete: 7.8%; Average loss: 3.0316\n",
      "Iteration: 480; Percent complete: 8.0%; Average loss: 2.9983\n",
      "Iteration: 490; Percent complete: 8.2%; Average loss: 2.9406\n",
      "Iteration: 500; Percent complete: 8.3%; Average loss: 2.9083\n",
      "Iteration: 510; Percent complete: 8.5%; Average loss: 2.8455\n",
      "Iteration: 520; Percent complete: 8.7%; Average loss: 2.8250\n",
      "Iteration: 530; Percent complete: 8.8%; Average loss: 2.7790\n",
      "Iteration: 540; Percent complete: 9.0%; Average loss: 2.7459\n",
      "Iteration: 550; Percent complete: 9.2%; Average loss: 2.7011\n",
      "Iteration: 560; Percent complete: 9.3%; Average loss: 2.6751\n",
      "Iteration: 570; Percent complete: 9.5%; Average loss: 2.6435\n",
      "Iteration: 580; Percent complete: 9.7%; Average loss: 2.6111\n",
      "Iteration: 590; Percent complete: 9.8%; Average loss: 2.5734\n",
      "Iteration: 600; Percent complete: 10.0%; Average loss: 2.5275\n",
      "Iteration: 610; Percent complete: 10.2%; Average loss: 2.5163\n",
      "Iteration: 620; Percent complete: 10.3%; Average loss: 2.4950\n",
      "Iteration: 630; Percent complete: 10.5%; Average loss: 2.4577\n",
      "Iteration: 640; Percent complete: 10.7%; Average loss: 2.4259\n",
      "Iteration: 650; Percent complete: 10.8%; Average loss: 2.4026\n",
      "Iteration: 660; Percent complete: 11.0%; Average loss: 2.3707\n",
      "Iteration: 670; Percent complete: 11.2%; Average loss: 2.3398\n",
      "Iteration: 680; Percent complete: 11.3%; Average loss: 2.3284\n",
      "Iteration: 690; Percent complete: 11.5%; Average loss: 2.2911\n",
      "Iteration: 700; Percent complete: 11.7%; Average loss: 2.2822\n",
      "Iteration: 710; Percent complete: 11.8%; Average loss: 2.2397\n",
      "Iteration: 720; Percent complete: 12.0%; Average loss: 2.2455\n",
      "Iteration: 730; Percent complete: 12.2%; Average loss: 2.1991\n",
      "Iteration: 740; Percent complete: 12.3%; Average loss: 2.1643\n",
      "Iteration: 750; Percent complete: 12.5%; Average loss: 2.1553\n",
      "Iteration: 760; Percent complete: 12.7%; Average loss: 2.1557\n",
      "Iteration: 770; Percent complete: 12.8%; Average loss: 2.1213\n",
      "Iteration: 780; Percent complete: 13.0%; Average loss: 2.0888\n",
      "Iteration: 790; Percent complete: 13.2%; Average loss: 2.1113\n",
      "Iteration: 800; Percent complete: 13.3%; Average loss: 2.0413\n",
      "Iteration: 810; Percent complete: 13.5%; Average loss: 2.0647\n",
      "Iteration: 820; Percent complete: 13.7%; Average loss: 2.0199\n",
      "Iteration: 830; Percent complete: 13.8%; Average loss: 1.9934\n",
      "Iteration: 840; Percent complete: 14.0%; Average loss: 2.0050\n",
      "Iteration: 850; Percent complete: 14.2%; Average loss: 1.9761\n",
      "Iteration: 860; Percent complete: 14.3%; Average loss: 1.9880\n",
      "Iteration: 870; Percent complete: 14.5%; Average loss: 1.9438\n",
      "Iteration: 880; Percent complete: 14.7%; Average loss: 1.9388\n",
      "Iteration: 890; Percent complete: 14.8%; Average loss: 1.9270\n",
      "Iteration: 900; Percent complete: 15.0%; Average loss: 1.9039\n",
      "Iteration: 910; Percent complete: 15.2%; Average loss: 1.8820\n",
      "Iteration: 920; Percent complete: 15.3%; Average loss: 1.8596\n",
      "Iteration: 930; Percent complete: 15.5%; Average loss: 1.8555\n",
      "Iteration: 940; Percent complete: 15.7%; Average loss: 1.8296\n",
      "Iteration: 950; Percent complete: 15.8%; Average loss: 1.8323\n",
      "Iteration: 960; Percent complete: 16.0%; Average loss: 1.7891\n",
      "Iteration: 970; Percent complete: 16.2%; Average loss: 1.7960\n",
      "Iteration: 980; Percent complete: 16.3%; Average loss: 1.7818\n",
      "Iteration: 990; Percent complete: 16.5%; Average loss: 1.7670\n",
      "Iteration: 1000; Percent complete: 16.7%; Average loss: 1.7629\n",
      "Iteration: 1010; Percent complete: 16.8%; Average loss: 1.7324\n",
      "Iteration: 1020; Percent complete: 17.0%; Average loss: 1.7066\n",
      "Iteration: 1030; Percent complete: 17.2%; Average loss: 1.7346\n",
      "Iteration: 1040; Percent complete: 17.3%; Average loss: 1.7194\n",
      "Iteration: 1050; Percent complete: 17.5%; Average loss: 1.6743\n",
      "Iteration: 1060; Percent complete: 17.7%; Average loss: 1.6859\n",
      "Iteration: 1070; Percent complete: 17.8%; Average loss: 1.6705\n",
      "Iteration: 1080; Percent complete: 18.0%; Average loss: 1.6378\n",
      "Iteration: 1090; Percent complete: 18.2%; Average loss: 1.6418\n",
      "Iteration: 1100; Percent complete: 18.3%; Average loss: 1.6250\n",
      "Iteration: 1110; Percent complete: 18.5%; Average loss: 1.5892\n",
      "Iteration: 1120; Percent complete: 18.7%; Average loss: 1.5658\n",
      "Iteration: 1130; Percent complete: 18.8%; Average loss: 1.5703\n",
      "Iteration: 1140; Percent complete: 19.0%; Average loss: 1.5484\n",
      "Iteration: 1150; Percent complete: 19.2%; Average loss: 1.5450\n",
      "Iteration: 1160; Percent complete: 19.3%; Average loss: 1.5349\n",
      "Iteration: 1170; Percent complete: 19.5%; Average loss: 1.5195\n",
      "Iteration: 1180; Percent complete: 19.7%; Average loss: 1.4884\n",
      "Iteration: 1190; Percent complete: 19.8%; Average loss: 1.4794\n",
      "Iteration: 1200; Percent complete: 20.0%; Average loss: 1.4890\n",
      "Iteration: 1210; Percent complete: 20.2%; Average loss: 1.4703\n",
      "Iteration: 1220; Percent complete: 20.3%; Average loss: 1.4510\n",
      "Iteration: 1230; Percent complete: 20.5%; Average loss: 1.4425\n",
      "Iteration: 1240; Percent complete: 20.7%; Average loss: 1.4416\n",
      "Iteration: 1250; Percent complete: 20.8%; Average loss: 1.3872\n",
      "Iteration: 1260; Percent complete: 21.0%; Average loss: 1.4120\n",
      "Iteration: 1270; Percent complete: 21.2%; Average loss: 1.3884\n",
      "Iteration: 1280; Percent complete: 21.3%; Average loss: 1.3959\n",
      "Iteration: 1290; Percent complete: 21.5%; Average loss: 1.3682\n",
      "Iteration: 1300; Percent complete: 21.7%; Average loss: 1.3743\n",
      "Iteration: 1310; Percent complete: 21.8%; Average loss: 1.3442\n",
      "Iteration: 1320; Percent complete: 22.0%; Average loss: 1.3312\n",
      "Iteration: 1330; Percent complete: 22.2%; Average loss: 1.3330\n",
      "Iteration: 1340; Percent complete: 22.3%; Average loss: 1.3071\n",
      "Iteration: 1350; Percent complete: 22.5%; Average loss: 1.3033\n",
      "Iteration: 1360; Percent complete: 22.7%; Average loss: 1.2873\n",
      "Iteration: 1370; Percent complete: 22.8%; Average loss: 1.2896\n",
      "Iteration: 1380; Percent complete: 23.0%; Average loss: 1.2646\n",
      "Iteration: 1390; Percent complete: 23.2%; Average loss: 1.2569\n",
      "Iteration: 1400; Percent complete: 23.3%; Average loss: 1.2377\n",
      "Iteration: 1410; Percent complete: 23.5%; Average loss: 1.2174\n",
      "Iteration: 1420; Percent complete: 23.7%; Average loss: 1.2109\n",
      "Iteration: 1430; Percent complete: 23.8%; Average loss: 1.2105\n",
      "Iteration: 1440; Percent complete: 24.0%; Average loss: 1.1748\n",
      "Iteration: 1450; Percent complete: 24.2%; Average loss: 1.2035\n",
      "Iteration: 1460; Percent complete: 24.3%; Average loss: 1.1675\n",
      "Iteration: 1470; Percent complete: 24.5%; Average loss: 1.1558\n",
      "Iteration: 1480; Percent complete: 24.7%; Average loss: 1.1370\n",
      "Iteration: 1490; Percent complete: 24.8%; Average loss: 1.1317\n",
      "Iteration: 1500; Percent complete: 25.0%; Average loss: 1.1067\n",
      "Iteration: 1510; Percent complete: 25.2%; Average loss: 1.1195\n",
      "Iteration: 1520; Percent complete: 25.3%; Average loss: 1.0998\n",
      "Iteration: 1530; Percent complete: 25.5%; Average loss: 1.0971\n",
      "Iteration: 1540; Percent complete: 25.7%; Average loss: 1.0779\n",
      "Iteration: 1550; Percent complete: 25.8%; Average loss: 1.0601\n",
      "Iteration: 1560; Percent complete: 26.0%; Average loss: 1.0585\n",
      "Iteration: 1570; Percent complete: 26.2%; Average loss: 1.0725\n",
      "Iteration: 1580; Percent complete: 26.3%; Average loss: 1.0303\n",
      "Iteration: 1590; Percent complete: 26.5%; Average loss: 1.0297\n",
      "Iteration: 1600; Percent complete: 26.7%; Average loss: 0.9962\n",
      "Iteration: 1610; Percent complete: 26.8%; Average loss: 0.9858\n",
      "Iteration: 1620; Percent complete: 27.0%; Average loss: 0.9858\n",
      "Iteration: 1630; Percent complete: 27.2%; Average loss: 0.9688\n",
      "Iteration: 1640; Percent complete: 27.3%; Average loss: 0.9603\n",
      "Iteration: 1650; Percent complete: 27.5%; Average loss: 0.9641\n",
      "Iteration: 1660; Percent complete: 27.7%; Average loss: 0.9530\n",
      "Iteration: 1670; Percent complete: 27.8%; Average loss: 0.9403\n",
      "Iteration: 1680; Percent complete: 28.0%; Average loss: 0.9119\n",
      "Iteration: 1690; Percent complete: 28.2%; Average loss: 0.9037\n",
      "Iteration: 1700; Percent complete: 28.3%; Average loss: 0.8840\n",
      "Iteration: 1710; Percent complete: 28.5%; Average loss: 0.8864\n",
      "Iteration: 1720; Percent complete: 28.7%; Average loss: 0.8771\n",
      "Iteration: 1730; Percent complete: 28.8%; Average loss: 0.8678\n",
      "Iteration: 1740; Percent complete: 29.0%; Average loss: 0.8674\n",
      "Iteration: 1750; Percent complete: 29.2%; Average loss: 0.8568\n",
      "Iteration: 1760; Percent complete: 29.3%; Average loss: 0.8402\n",
      "Iteration: 1770; Percent complete: 29.5%; Average loss: 0.8320\n",
      "Iteration: 1780; Percent complete: 29.7%; Average loss: 0.8028\n",
      "Iteration: 1790; Percent complete: 29.8%; Average loss: 0.8167\n",
      "Iteration: 1800; Percent complete: 30.0%; Average loss: 0.8015\n",
      "Iteration: 1810; Percent complete: 30.2%; Average loss: 0.7930\n",
      "Iteration: 1820; Percent complete: 30.3%; Average loss: 0.7763\n",
      "Iteration: 1830; Percent complete: 30.5%; Average loss: 0.7684\n",
      "Iteration: 1840; Percent complete: 30.7%; Average loss: 0.7499\n",
      "Iteration: 1850; Percent complete: 30.8%; Average loss: 0.7498\n",
      "Iteration: 1860; Percent complete: 31.0%; Average loss: 0.7423\n",
      "Iteration: 1870; Percent complete: 31.2%; Average loss: 0.7246\n",
      "Iteration: 1880; Percent complete: 31.3%; Average loss: 0.7127\n",
      "Iteration: 1890; Percent complete: 31.5%; Average loss: 0.7163\n",
      "Iteration: 1900; Percent complete: 31.7%; Average loss: 0.7021\n",
      "Iteration: 1910; Percent complete: 31.8%; Average loss: 0.6933\n",
      "Iteration: 1920; Percent complete: 32.0%; Average loss: 0.6923\n",
      "Iteration: 1930; Percent complete: 32.2%; Average loss: 0.6780\n",
      "Iteration: 1940; Percent complete: 32.3%; Average loss: 0.6700\n",
      "Iteration: 1950; Percent complete: 32.5%; Average loss: 0.6673\n",
      "Iteration: 1960; Percent complete: 32.7%; Average loss: 0.6482\n",
      "Iteration: 1970; Percent complete: 32.8%; Average loss: 0.6332\n",
      "Iteration: 1980; Percent complete: 33.0%; Average loss: 0.6299\n",
      "Iteration: 1990; Percent complete: 33.2%; Average loss: 0.6282\n",
      "Iteration: 2000; Percent complete: 33.3%; Average loss: 0.6157\n",
      "content/cb_model/Chat/2-4_512\n",
      "Iteration: 2010; Percent complete: 33.5%; Average loss: 0.6112\n",
      "Iteration: 2020; Percent complete: 33.7%; Average loss: 0.5968\n",
      "Iteration: 2030; Percent complete: 33.8%; Average loss: 0.5980\n",
      "Iteration: 2040; Percent complete: 34.0%; Average loss: 0.5878\n",
      "Iteration: 2050; Percent complete: 34.2%; Average loss: 0.5813\n",
      "Iteration: 2060; Percent complete: 34.3%; Average loss: 0.5760\n",
      "Iteration: 2070; Percent complete: 34.5%; Average loss: 0.5645\n",
      "Iteration: 2080; Percent complete: 34.7%; Average loss: 0.5554\n",
      "Iteration: 2090; Percent complete: 34.8%; Average loss: 0.5497\n",
      "Iteration: 2100; Percent complete: 35.0%; Average loss: 0.5350\n",
      "Iteration: 2110; Percent complete: 35.2%; Average loss: 0.5245\n",
      "Iteration: 2120; Percent complete: 35.3%; Average loss: 0.5155\n",
      "Iteration: 2130; Percent complete: 35.5%; Average loss: 0.5134\n",
      "Iteration: 2140; Percent complete: 35.7%; Average loss: 0.5060\n",
      "Iteration: 2150; Percent complete: 35.8%; Average loss: 0.4933\n",
      "Iteration: 2160; Percent complete: 36.0%; Average loss: 0.5006\n",
      "Iteration: 2170; Percent complete: 36.2%; Average loss: 0.4909\n",
      "Iteration: 2180; Percent complete: 36.3%; Average loss: 0.4849\n",
      "Iteration: 2190; Percent complete: 36.5%; Average loss: 0.4780\n",
      "Iteration: 2200; Percent complete: 36.7%; Average loss: 0.4749\n",
      "Iteration: 2210; Percent complete: 36.8%; Average loss: 0.4579\n",
      "Iteration: 2220; Percent complete: 37.0%; Average loss: 0.4402\n",
      "Iteration: 2230; Percent complete: 37.2%; Average loss: 0.4426\n",
      "Iteration: 2240; Percent complete: 37.3%; Average loss: 0.4387\n",
      "Iteration: 2250; Percent complete: 37.5%; Average loss: 0.4320\n",
      "Iteration: 2260; Percent complete: 37.7%; Average loss: 0.4222\n",
      "Iteration: 2270; Percent complete: 37.8%; Average loss: 0.4131\n",
      "Iteration: 2280; Percent complete: 38.0%; Average loss: 0.4169\n",
      "Iteration: 2290; Percent complete: 38.2%; Average loss: 0.4111\n",
      "Iteration: 2300; Percent complete: 38.3%; Average loss: 0.3957\n",
      "Iteration: 2310; Percent complete: 38.5%; Average loss: 0.4006\n",
      "Iteration: 2320; Percent complete: 38.7%; Average loss: 0.4043\n",
      "Iteration: 2330; Percent complete: 38.8%; Average loss: 0.3935\n",
      "Iteration: 2340; Percent complete: 39.0%; Average loss: 0.3887\n",
      "Iteration: 2350; Percent complete: 39.2%; Average loss: 0.3803\n",
      "Iteration: 2360; Percent complete: 39.3%; Average loss: 0.3632\n",
      "Iteration: 2370; Percent complete: 39.5%; Average loss: 0.3619\n",
      "Iteration: 2380; Percent complete: 39.7%; Average loss: 0.3612\n",
      "Iteration: 2390; Percent complete: 39.8%; Average loss: 0.3526\n",
      "Iteration: 2400; Percent complete: 40.0%; Average loss: 0.3523\n",
      "Iteration: 2410; Percent complete: 40.2%; Average loss: 0.3412\n",
      "Iteration: 2420; Percent complete: 40.3%; Average loss: 0.3446\n",
      "Iteration: 2430; Percent complete: 40.5%; Average loss: 0.3357\n",
      "Iteration: 2440; Percent complete: 40.7%; Average loss: 0.3360\n",
      "Iteration: 2450; Percent complete: 40.8%; Average loss: 0.3246\n",
      "Iteration: 2460; Percent complete: 41.0%; Average loss: 0.3186\n",
      "Iteration: 2470; Percent complete: 41.2%; Average loss: 0.3188\n",
      "Iteration: 2480; Percent complete: 41.3%; Average loss: 0.3177\n",
      "Iteration: 2490; Percent complete: 41.5%; Average loss: 0.3123\n",
      "Iteration: 2500; Percent complete: 41.7%; Average loss: 0.2992\n",
      "Iteration: 2510; Percent complete: 41.8%; Average loss: 0.3009\n",
      "Iteration: 2520; Percent complete: 42.0%; Average loss: 0.2950\n",
      "Iteration: 2530; Percent complete: 42.2%; Average loss: 0.2925\n",
      "Iteration: 2540; Percent complete: 42.3%; Average loss: 0.2884\n",
      "Iteration: 2550; Percent complete: 42.5%; Average loss: 0.2852\n",
      "Iteration: 2560; Percent complete: 42.7%; Average loss: 0.2794\n",
      "Iteration: 2570; Percent complete: 42.8%; Average loss: 0.2703\n",
      "Iteration: 2580; Percent complete: 43.0%; Average loss: 0.2735\n",
      "Iteration: 2590; Percent complete: 43.2%; Average loss: 0.2639\n",
      "Iteration: 2600; Percent complete: 43.3%; Average loss: 0.2632\n",
      "Iteration: 2610; Percent complete: 43.5%; Average loss: 0.2623\n",
      "Iteration: 2620; Percent complete: 43.7%; Average loss: 0.2570\n",
      "Iteration: 2630; Percent complete: 43.8%; Average loss: 0.2469\n",
      "Iteration: 2640; Percent complete: 44.0%; Average loss: 0.2443\n",
      "Iteration: 2650; Percent complete: 44.2%; Average loss: 0.2400\n",
      "Iteration: 2660; Percent complete: 44.3%; Average loss: 0.2460\n",
      "Iteration: 2670; Percent complete: 44.5%; Average loss: 0.2354\n",
      "Iteration: 2680; Percent complete: 44.7%; Average loss: 0.2316\n",
      "Iteration: 2690; Percent complete: 44.8%; Average loss: 0.2274\n",
      "Iteration: 2700; Percent complete: 45.0%; Average loss: 0.2252\n",
      "Iteration: 2710; Percent complete: 45.2%; Average loss: 0.2236\n",
      "Iteration: 2720; Percent complete: 45.3%; Average loss: 0.2231\n",
      "Iteration: 2730; Percent complete: 45.5%; Average loss: 0.2204\n",
      "Iteration: 2740; Percent complete: 45.7%; Average loss: 0.2181\n",
      "Iteration: 2750; Percent complete: 45.8%; Average loss: 0.2182\n",
      "Iteration: 2760; Percent complete: 46.0%; Average loss: 0.2081\n",
      "Iteration: 2770; Percent complete: 46.2%; Average loss: 0.2056\n",
      "Iteration: 2780; Percent complete: 46.3%; Average loss: 0.2056\n",
      "Iteration: 2790; Percent complete: 46.5%; Average loss: 0.2009\n",
      "Iteration: 2800; Percent complete: 46.7%; Average loss: 0.2009\n",
      "Iteration: 2810; Percent complete: 46.8%; Average loss: 0.1957\n",
      "Iteration: 2820; Percent complete: 47.0%; Average loss: 0.1921\n",
      "Iteration: 2830; Percent complete: 47.2%; Average loss: 0.1895\n",
      "Iteration: 2840; Percent complete: 47.3%; Average loss: 0.1934\n",
      "Iteration: 2850; Percent complete: 47.5%; Average loss: 0.1925\n",
      "Iteration: 2860; Percent complete: 47.7%; Average loss: 0.1862\n",
      "Iteration: 2870; Percent complete: 47.8%; Average loss: 0.1822\n",
      "Iteration: 2880; Percent complete: 48.0%; Average loss: 0.1779\n",
      "Iteration: 2890; Percent complete: 48.2%; Average loss: 0.1767\n",
      "Iteration: 2900; Percent complete: 48.3%; Average loss: 0.1731\n",
      "Iteration: 2910; Percent complete: 48.5%; Average loss: 0.1722\n",
      "Iteration: 2920; Percent complete: 48.7%; Average loss: 0.1682\n",
      "Iteration: 2930; Percent complete: 48.8%; Average loss: 0.1717\n",
      "Iteration: 2940; Percent complete: 49.0%; Average loss: 0.1678\n",
      "Iteration: 2950; Percent complete: 49.2%; Average loss: 0.1626\n",
      "Iteration: 2960; Percent complete: 49.3%; Average loss: 0.1687\n",
      "Iteration: 2970; Percent complete: 49.5%; Average loss: 0.1606\n",
      "Iteration: 2980; Percent complete: 49.7%; Average loss: 0.1633\n",
      "Iteration: 2990; Percent complete: 49.8%; Average loss: 0.1607\n",
      "Iteration: 3000; Percent complete: 50.0%; Average loss: 0.1547\n",
      "Iteration: 3010; Percent complete: 50.2%; Average loss: 0.1563\n",
      "Iteration: 3020; Percent complete: 50.3%; Average loss: 0.1550\n",
      "Iteration: 3030; Percent complete: 50.5%; Average loss: 0.1598\n",
      "Iteration: 3040; Percent complete: 50.7%; Average loss: 0.1548\n",
      "Iteration: 3050; Percent complete: 50.8%; Average loss: 0.1504\n",
      "Iteration: 3060; Percent complete: 51.0%; Average loss: 0.1512\n",
      "Iteration: 3070; Percent complete: 51.2%; Average loss: 0.1401\n",
      "Iteration: 3080; Percent complete: 51.3%; Average loss: 0.1361\n",
      "Iteration: 3090; Percent complete: 51.5%; Average loss: 0.1368\n",
      "Iteration: 3100; Percent complete: 51.7%; Average loss: 0.1341\n",
      "Iteration: 3110; Percent complete: 51.8%; Average loss: 0.1345\n",
      "Iteration: 3120; Percent complete: 52.0%; Average loss: 0.1329\n",
      "Iteration: 3130; Percent complete: 52.2%; Average loss: 0.1290\n",
      "Iteration: 3140; Percent complete: 52.3%; Average loss: 0.1253\n",
      "Iteration: 3150; Percent complete: 52.5%; Average loss: 0.1285\n",
      "Iteration: 3160; Percent complete: 52.7%; Average loss: 0.1237\n",
      "Iteration: 3170; Percent complete: 52.8%; Average loss: 0.1225\n",
      "Iteration: 3180; Percent complete: 53.0%; Average loss: 0.1241\n",
      "Iteration: 3190; Percent complete: 53.2%; Average loss: 0.1193\n",
      "Iteration: 3200; Percent complete: 53.3%; Average loss: 0.1178\n",
      "Iteration: 3210; Percent complete: 53.5%; Average loss: 0.1221\n",
      "Iteration: 3220; Percent complete: 53.7%; Average loss: 0.1168\n",
      "Iteration: 3230; Percent complete: 53.8%; Average loss: 0.1171\n",
      "Iteration: 3240; Percent complete: 54.0%; Average loss: 0.1167\n",
      "Iteration: 3250; Percent complete: 54.2%; Average loss: 0.1156\n",
      "Iteration: 3260; Percent complete: 54.3%; Average loss: 0.1167\n",
      "Iteration: 3270; Percent complete: 54.5%; Average loss: 0.1109\n",
      "Iteration: 3280; Percent complete: 54.7%; Average loss: 0.1089\n",
      "Iteration: 3290; Percent complete: 54.8%; Average loss: 0.1117\n",
      "Iteration: 3300; Percent complete: 55.0%; Average loss: 0.1100\n",
      "Iteration: 3310; Percent complete: 55.2%; Average loss: 0.1107\n",
      "Iteration: 3320; Percent complete: 55.3%; Average loss: 0.1101\n",
      "Iteration: 3330; Percent complete: 55.5%; Average loss: 0.1087\n",
      "Iteration: 3340; Percent complete: 55.7%; Average loss: 0.1076\n",
      "Iteration: 3350; Percent complete: 55.8%; Average loss: 0.1026\n",
      "Iteration: 3360; Percent complete: 56.0%; Average loss: 0.1037\n",
      "Iteration: 3370; Percent complete: 56.2%; Average loss: 0.1013\n",
      "Iteration: 3380; Percent complete: 56.3%; Average loss: 0.1031\n",
      "Iteration: 3390; Percent complete: 56.5%; Average loss: 0.1023\n",
      "Iteration: 3400; Percent complete: 56.7%; Average loss: 0.1001\n",
      "Iteration: 3410; Percent complete: 56.8%; Average loss: 0.0978\n",
      "Iteration: 3420; Percent complete: 57.0%; Average loss: 0.1039\n",
      "Iteration: 3430; Percent complete: 57.2%; Average loss: 0.0976\n",
      "Iteration: 3440; Percent complete: 57.3%; Average loss: 0.0974\n",
      "Iteration: 3450; Percent complete: 57.5%; Average loss: 0.0964\n",
      "Iteration: 3460; Percent complete: 57.7%; Average loss: 0.0929\n",
      "Iteration: 3470; Percent complete: 57.8%; Average loss: 0.0932\n",
      "Iteration: 3480; Percent complete: 58.0%; Average loss: 0.0935\n",
      "Iteration: 3490; Percent complete: 58.2%; Average loss: 0.0926\n",
      "Iteration: 3500; Percent complete: 58.3%; Average loss: 0.0952\n",
      "Iteration: 3510; Percent complete: 58.5%; Average loss: 0.0908\n",
      "Iteration: 3520; Percent complete: 58.7%; Average loss: 0.0869\n",
      "Iteration: 3530; Percent complete: 58.8%; Average loss: 0.0863\n",
      "Iteration: 3540; Percent complete: 59.0%; Average loss: 0.0868\n",
      "Iteration: 3550; Percent complete: 59.2%; Average loss: 0.0850\n",
      "Iteration: 3560; Percent complete: 59.3%; Average loss: 0.0857\n",
      "Iteration: 3570; Percent complete: 59.5%; Average loss: 0.0840\n",
      "Iteration: 3580; Percent complete: 59.7%; Average loss: 0.0831\n",
      "Iteration: 3590; Percent complete: 59.8%; Average loss: 0.0833\n",
      "Iteration: 3600; Percent complete: 60.0%; Average loss: 0.0805\n",
      "Iteration: 3610; Percent complete: 60.2%; Average loss: 0.0857\n",
      "Iteration: 3620; Percent complete: 60.3%; Average loss: 0.0807\n",
      "Iteration: 3630; Percent complete: 60.5%; Average loss: 0.0796\n",
      "Iteration: 3640; Percent complete: 60.7%; Average loss: 0.0779\n",
      "Iteration: 3650; Percent complete: 60.8%; Average loss: 0.0791\n",
      "Iteration: 3660; Percent complete: 61.0%; Average loss: 0.0747\n",
      "Iteration: 3670; Percent complete: 61.2%; Average loss: 0.0749\n",
      "Iteration: 3680; Percent complete: 61.3%; Average loss: 0.0760\n",
      "Iteration: 3690; Percent complete: 61.5%; Average loss: 0.0779\n",
      "Iteration: 3700; Percent complete: 61.7%; Average loss: 0.0749\n",
      "Iteration: 3710; Percent complete: 61.8%; Average loss: 0.0710\n",
      "Iteration: 3720; Percent complete: 62.0%; Average loss: 0.0744\n",
      "Iteration: 3730; Percent complete: 62.2%; Average loss: 0.0737\n",
      "Iteration: 3740; Percent complete: 62.3%; Average loss: 0.0713\n",
      "Iteration: 3750; Percent complete: 62.5%; Average loss: 0.0708\n",
      "Iteration: 3760; Percent complete: 62.7%; Average loss: 0.0714\n",
      "Iteration: 3770; Percent complete: 62.8%; Average loss: 0.0718\n",
      "Iteration: 3780; Percent complete: 63.0%; Average loss: 0.0703\n",
      "Iteration: 3790; Percent complete: 63.2%; Average loss: 0.0688\n",
      "Iteration: 3800; Percent complete: 63.3%; Average loss: 0.0683\n",
      "Iteration: 3810; Percent complete: 63.5%; Average loss: 0.0680\n",
      "Iteration: 3820; Percent complete: 63.7%; Average loss: 0.0669\n",
      "Iteration: 3830; Percent complete: 63.8%; Average loss: 0.0707\n",
      "Iteration: 3840; Percent complete: 64.0%; Average loss: 0.0729\n",
      "Iteration: 3850; Percent complete: 64.2%; Average loss: 0.0695\n",
      "Iteration: 3860; Percent complete: 64.3%; Average loss: 0.0702\n",
      "Iteration: 3870; Percent complete: 64.5%; Average loss: 0.0685\n",
      "Iteration: 3880; Percent complete: 64.7%; Average loss: 0.0685\n",
      "Iteration: 3890; Percent complete: 64.8%; Average loss: 0.0641\n",
      "Iteration: 3900; Percent complete: 65.0%; Average loss: 0.0635\n",
      "Iteration: 3910; Percent complete: 65.2%; Average loss: 0.0660\n",
      "Iteration: 3920; Percent complete: 65.3%; Average loss: 0.0657\n",
      "Iteration: 3930; Percent complete: 65.5%; Average loss: 0.0652\n",
      "Iteration: 3940; Percent complete: 65.7%; Average loss: 0.0623\n",
      "Iteration: 3950; Percent complete: 65.8%; Average loss: 0.0622\n",
      "Iteration: 3960; Percent complete: 66.0%; Average loss: 0.0600\n",
      "Iteration: 3970; Percent complete: 66.2%; Average loss: 0.0608\n",
      "Iteration: 3980; Percent complete: 66.3%; Average loss: 0.0611\n",
      "Iteration: 3990; Percent complete: 66.5%; Average loss: 0.0614\n",
      "Iteration: 4000; Percent complete: 66.7%; Average loss: 0.0623\n",
      "content/cb_model/Chat/2-4_512\n",
      "Iteration: 4010; Percent complete: 66.8%; Average loss: 0.0587\n",
      "Iteration: 4020; Percent complete: 67.0%; Average loss: 0.0603\n",
      "Iteration: 4030; Percent complete: 67.2%; Average loss: 0.0656\n",
      "Iteration: 4040; Percent complete: 67.3%; Average loss: 0.0664\n",
      "Iteration: 4050; Percent complete: 67.5%; Average loss: 0.0655\n",
      "Iteration: 4060; Percent complete: 67.7%; Average loss: 0.0642\n",
      "Iteration: 4070; Percent complete: 67.8%; Average loss: 0.0669\n",
      "Iteration: 4080; Percent complete: 68.0%; Average loss: 0.0703\n",
      "Iteration: 4090; Percent complete: 68.2%; Average loss: 0.0689\n",
      "Iteration: 4100; Percent complete: 68.3%; Average loss: 0.0668\n",
      "Iteration: 4110; Percent complete: 68.5%; Average loss: 0.0677\n",
      "Iteration: 4120; Percent complete: 68.7%; Average loss: 0.0659\n",
      "Iteration: 4130; Percent complete: 68.8%; Average loss: 0.0631\n",
      "Iteration: 4140; Percent complete: 69.0%; Average loss: 0.0613\n",
      "Iteration: 4150; Percent complete: 69.2%; Average loss: 0.0619\n",
      "Iteration: 4160; Percent complete: 69.3%; Average loss: 0.0591\n",
      "Iteration: 4170; Percent complete: 69.5%; Average loss: 0.0590\n",
      "Iteration: 4180; Percent complete: 69.7%; Average loss: 0.0586\n",
      "Iteration: 4190; Percent complete: 69.8%; Average loss: 0.0539\n",
      "Iteration: 4200; Percent complete: 70.0%; Average loss: 0.0538\n",
      "Iteration: 4210; Percent complete: 70.2%; Average loss: 0.0574\n",
      "Iteration: 4220; Percent complete: 70.3%; Average loss: 0.0542\n",
      "Iteration: 4230; Percent complete: 70.5%; Average loss: 0.0562\n",
      "Iteration: 4240; Percent complete: 70.7%; Average loss: 0.0559\n",
      "Iteration: 4250; Percent complete: 70.8%; Average loss: 0.0555\n",
      "Iteration: 4260; Percent complete: 71.0%; Average loss: 0.0567\n",
      "Iteration: 4270; Percent complete: 71.2%; Average loss: 0.0523\n",
      "Iteration: 4280; Percent complete: 71.3%; Average loss: 0.0519\n",
      "Iteration: 4290; Percent complete: 71.5%; Average loss: 0.0521\n",
      "Iteration: 4300; Percent complete: 71.7%; Average loss: 0.0535\n",
      "Iteration: 4310; Percent complete: 71.8%; Average loss: 0.0522\n",
      "Iteration: 4320; Percent complete: 72.0%; Average loss: 0.0481\n",
      "Iteration: 4330; Percent complete: 72.2%; Average loss: 0.0485\n",
      "Iteration: 4340; Percent complete: 72.3%; Average loss: 0.0463\n",
      "Iteration: 4350; Percent complete: 72.5%; Average loss: 0.0473\n",
      "Iteration: 4360; Percent complete: 72.7%; Average loss: 0.0484\n",
      "Iteration: 4370; Percent complete: 72.8%; Average loss: 0.0460\n",
      "Iteration: 4380; Percent complete: 73.0%; Average loss: 0.0466\n",
      "Iteration: 4390; Percent complete: 73.2%; Average loss: 0.0498\n",
      "Iteration: 4400; Percent complete: 73.3%; Average loss: 0.0488\n",
      "Iteration: 4410; Percent complete: 73.5%; Average loss: 0.0460\n",
      "Iteration: 4420; Percent complete: 73.7%; Average loss: 0.0487\n",
      "Iteration: 4430; Percent complete: 73.8%; Average loss: 0.0479\n",
      "Iteration: 4440; Percent complete: 74.0%; Average loss: 0.0470\n",
      "Iteration: 4450; Percent complete: 74.2%; Average loss: 0.0480\n",
      "Iteration: 4460; Percent complete: 74.3%; Average loss: 0.0446\n",
      "Iteration: 4470; Percent complete: 74.5%; Average loss: 0.0442\n",
      "Iteration: 4480; Percent complete: 74.7%; Average loss: 0.0450\n",
      "Iteration: 4490; Percent complete: 74.8%; Average loss: 0.0455\n",
      "Iteration: 4500; Percent complete: 75.0%; Average loss: 0.0456\n",
      "Iteration: 4510; Percent complete: 75.2%; Average loss: 0.0419\n",
      "Iteration: 4520; Percent complete: 75.3%; Average loss: 0.0433\n",
      "Iteration: 4530; Percent complete: 75.5%; Average loss: 0.0438\n",
      "Iteration: 4540; Percent complete: 75.7%; Average loss: 0.0414\n",
      "Iteration: 4550; Percent complete: 75.8%; Average loss: 0.0425\n",
      "Iteration: 4560; Percent complete: 76.0%; Average loss: 0.0421\n",
      "Iteration: 4570; Percent complete: 76.2%; Average loss: 0.0392\n",
      "Iteration: 4580; Percent complete: 76.3%; Average loss: 0.0427\n",
      "Iteration: 4590; Percent complete: 76.5%; Average loss: 0.0456\n",
      "Iteration: 4600; Percent complete: 76.7%; Average loss: 0.0472\n",
      "Iteration: 4610; Percent complete: 76.8%; Average loss: 0.0471\n",
      "Iteration: 4620; Percent complete: 77.0%; Average loss: 0.0462\n",
      "Iteration: 4630; Percent complete: 77.2%; Average loss: 0.0489\n",
      "Iteration: 4640; Percent complete: 77.3%; Average loss: 0.0468\n",
      "Iteration: 4650; Percent complete: 77.5%; Average loss: 0.0426\n",
      "Iteration: 4660; Percent complete: 77.7%; Average loss: 0.0398\n",
      "Iteration: 4670; Percent complete: 77.8%; Average loss: 0.0438\n",
      "Iteration: 4680; Percent complete: 78.0%; Average loss: 0.0462\n",
      "Iteration: 4690; Percent complete: 78.2%; Average loss: 0.0445\n",
      "Iteration: 4700; Percent complete: 78.3%; Average loss: 0.0440\n",
      "Iteration: 4710; Percent complete: 78.5%; Average loss: 0.0431\n",
      "Iteration: 4720; Percent complete: 78.7%; Average loss: 0.0410\n",
      "Iteration: 4730; Percent complete: 78.8%; Average loss: 0.0417\n",
      "Iteration: 4740; Percent complete: 79.0%; Average loss: 0.0392\n",
      "Iteration: 4750; Percent complete: 79.2%; Average loss: 0.0407\n",
      "Iteration: 4760; Percent complete: 79.3%; Average loss: 0.0411\n",
      "Iteration: 4770; Percent complete: 79.5%; Average loss: 0.0402\n",
      "Iteration: 4780; Percent complete: 79.7%; Average loss: 0.0406\n",
      "Iteration: 4790; Percent complete: 79.8%; Average loss: 0.0402\n",
      "Iteration: 4800; Percent complete: 80.0%; Average loss: 0.0379\n",
      "Iteration: 4810; Percent complete: 80.2%; Average loss: 0.0378\n",
      "Iteration: 4820; Percent complete: 80.3%; Average loss: 0.0381\n",
      "Iteration: 4830; Percent complete: 80.5%; Average loss: 0.0380\n",
      "Iteration: 4840; Percent complete: 80.7%; Average loss: 0.0388\n",
      "Iteration: 4850; Percent complete: 80.8%; Average loss: 0.0402\n",
      "Iteration: 4860; Percent complete: 81.0%; Average loss: 0.0410\n",
      "Iteration: 4870; Percent complete: 81.2%; Average loss: 0.0403\n",
      "Iteration: 4880; Percent complete: 81.3%; Average loss: 0.0391\n",
      "Iteration: 4890; Percent complete: 81.5%; Average loss: 0.0379\n",
      "Iteration: 4900; Percent complete: 81.7%; Average loss: 0.0399\n",
      "Iteration: 4910; Percent complete: 81.8%; Average loss: 0.0374\n",
      "Iteration: 4920; Percent complete: 82.0%; Average loss: 0.0398\n",
      "Iteration: 4930; Percent complete: 82.2%; Average loss: 0.0422\n",
      "Iteration: 4940; Percent complete: 82.3%; Average loss: 0.0380\n",
      "Iteration: 4950; Percent complete: 82.5%; Average loss: 0.0370\n",
      "Iteration: 4960; Percent complete: 82.7%; Average loss: 0.0354\n",
      "Iteration: 4970; Percent complete: 82.8%; Average loss: 0.0387\n",
      "Iteration: 4980; Percent complete: 83.0%; Average loss: 0.0399\n",
      "Iteration: 4990; Percent complete: 83.2%; Average loss: 0.0400\n",
      "Iteration: 5000; Percent complete: 83.3%; Average loss: 0.0412\n",
      "Iteration: 5010; Percent complete: 83.5%; Average loss: 0.0392\n",
      "Iteration: 5020; Percent complete: 83.7%; Average loss: 0.0409\n",
      "Iteration: 5030; Percent complete: 83.8%; Average loss: 0.0382\n",
      "Iteration: 5040; Percent complete: 84.0%; Average loss: 0.0397\n",
      "Iteration: 5050; Percent complete: 84.2%; Average loss: 0.0369\n",
      "Iteration: 5060; Percent complete: 84.3%; Average loss: 0.0364\n",
      "Iteration: 5070; Percent complete: 84.5%; Average loss: 0.0373\n",
      "Iteration: 5080; Percent complete: 84.7%; Average loss: 0.0347\n",
      "Iteration: 5090; Percent complete: 84.8%; Average loss: 0.0360\n",
      "Iteration: 5100; Percent complete: 85.0%; Average loss: 0.0339\n",
      "Iteration: 5110; Percent complete: 85.2%; Average loss: 0.0350\n",
      "Iteration: 5120; Percent complete: 85.3%; Average loss: 0.0352\n",
      "Iteration: 5130; Percent complete: 85.5%; Average loss: 0.0346\n",
      "Iteration: 5140; Percent complete: 85.7%; Average loss: 0.0342\n",
      "Iteration: 5150; Percent complete: 85.8%; Average loss: 0.0364\n",
      "Iteration: 5160; Percent complete: 86.0%; Average loss: 0.0356\n",
      "Iteration: 5170; Percent complete: 86.2%; Average loss: 0.0364\n",
      "Iteration: 5180; Percent complete: 86.3%; Average loss: 0.0362\n",
      "Iteration: 5190; Percent complete: 86.5%; Average loss: 0.0349\n",
      "Iteration: 5200; Percent complete: 86.7%; Average loss: 0.0336\n",
      "Iteration: 5210; Percent complete: 86.8%; Average loss: 0.0318\n",
      "Iteration: 5220; Percent complete: 87.0%; Average loss: 0.0318\n",
      "Iteration: 5230; Percent complete: 87.2%; Average loss: 0.0312\n",
      "Iteration: 5240; Percent complete: 87.3%; Average loss: 0.0320\n",
      "Iteration: 5250; Percent complete: 87.5%; Average loss: 0.0320\n",
      "Iteration: 5260; Percent complete: 87.7%; Average loss: 0.0312\n",
      "Iteration: 5270; Percent complete: 87.8%; Average loss: 0.0317\n",
      "Iteration: 5280; Percent complete: 88.0%; Average loss: 0.0326\n",
      "Iteration: 5290; Percent complete: 88.2%; Average loss: 0.0317\n",
      "Iteration: 5300; Percent complete: 88.3%; Average loss: 0.0331\n",
      "Iteration: 5310; Percent complete: 88.5%; Average loss: 0.0310\n",
      "Iteration: 5320; Percent complete: 88.7%; Average loss: 0.0331\n",
      "Iteration: 5330; Percent complete: 88.8%; Average loss: 0.0300\n",
      "Iteration: 5340; Percent complete: 89.0%; Average loss: 0.0337\n",
      "Iteration: 5350; Percent complete: 89.2%; Average loss: 0.0308\n",
      "Iteration: 5360; Percent complete: 89.3%; Average loss: 0.0317\n",
      "Iteration: 5370; Percent complete: 89.5%; Average loss: 0.0306\n",
      "Iteration: 5380; Percent complete: 89.7%; Average loss: 0.0327\n",
      "Iteration: 5390; Percent complete: 89.8%; Average loss: 0.0326\n",
      "Iteration: 5400; Percent complete: 90.0%; Average loss: 0.0325\n",
      "Iteration: 5410; Percent complete: 90.2%; Average loss: 0.0345\n",
      "Iteration: 5420; Percent complete: 90.3%; Average loss: 0.0347\n",
      "Iteration: 5430; Percent complete: 90.5%; Average loss: 0.0329\n",
      "Iteration: 5440; Percent complete: 90.7%; Average loss: 0.0337\n",
      "Iteration: 5450; Percent complete: 90.8%; Average loss: 0.0323\n",
      "Iteration: 5460; Percent complete: 91.0%; Average loss: 0.0334\n",
      "Iteration: 5470; Percent complete: 91.2%; Average loss: 0.0321\n",
      "Iteration: 5480; Percent complete: 91.3%; Average loss: 0.0357\n",
      "Iteration: 5490; Percent complete: 91.5%; Average loss: 0.0322\n",
      "Iteration: 5500; Percent complete: 91.7%; Average loss: 0.0331\n",
      "Iteration: 5510; Percent complete: 91.8%; Average loss: 0.0323\n",
      "Iteration: 5520; Percent complete: 92.0%; Average loss: 0.0351\n",
      "Iteration: 5530; Percent complete: 92.2%; Average loss: 0.0439\n",
      "Iteration: 5540; Percent complete: 92.3%; Average loss: 0.0393\n",
      "Iteration: 5550; Percent complete: 92.5%; Average loss: 0.0371\n",
      "Iteration: 5560; Percent complete: 92.7%; Average loss: 0.0333\n",
      "Iteration: 5570; Percent complete: 92.8%; Average loss: 0.0337\n",
      "Iteration: 5580; Percent complete: 93.0%; Average loss: 0.0339\n",
      "Iteration: 5590; Percent complete: 93.2%; Average loss: 0.0355\n",
      "Iteration: 5600; Percent complete: 93.3%; Average loss: 0.0379\n",
      "Iteration: 5610; Percent complete: 93.5%; Average loss: 0.0403\n",
      "Iteration: 5620; Percent complete: 93.7%; Average loss: 0.0361\n",
      "Iteration: 5630; Percent complete: 93.8%; Average loss: 0.0339\n",
      "Iteration: 5640; Percent complete: 94.0%; Average loss: 0.0321\n",
      "Iteration: 5650; Percent complete: 94.2%; Average loss: 0.0361\n",
      "Iteration: 5660; Percent complete: 94.3%; Average loss: 0.0365\n",
      "Iteration: 5670; Percent complete: 94.5%; Average loss: 0.0364\n",
      "Iteration: 5680; Percent complete: 94.7%; Average loss: 0.0358\n",
      "Iteration: 5690; Percent complete: 94.8%; Average loss: 0.0381\n",
      "Iteration: 5700; Percent complete: 95.0%; Average loss: 0.0368\n",
      "Iteration: 5710; Percent complete: 95.2%; Average loss: 0.0372\n",
      "Iteration: 5720; Percent complete: 95.3%; Average loss: 0.0376\n",
      "Iteration: 5730; Percent complete: 95.5%; Average loss: 0.0355\n",
      "Iteration: 5740; Percent complete: 95.7%; Average loss: 0.0327\n",
      "Iteration: 5750; Percent complete: 95.8%; Average loss: 0.0311\n",
      "Iteration: 5760; Percent complete: 96.0%; Average loss: 0.0299\n",
      "Iteration: 5770; Percent complete: 96.2%; Average loss: 0.0307\n",
      "Iteration: 5780; Percent complete: 96.3%; Average loss: 0.0315\n",
      "Iteration: 5790; Percent complete: 96.5%; Average loss: 0.0320\n",
      "Iteration: 5800; Percent complete: 96.7%; Average loss: 0.0284\n",
      "Iteration: 5810; Percent complete: 96.8%; Average loss: 0.0283\n",
      "Iteration: 5820; Percent complete: 97.0%; Average loss: 0.0292\n",
      "Iteration: 5830; Percent complete: 97.2%; Average loss: 0.0292\n",
      "Iteration: 5840; Percent complete: 97.3%; Average loss: 0.0301\n",
      "Iteration: 5850; Percent complete: 97.5%; Average loss: 0.0282\n",
      "Iteration: 5860; Percent complete: 97.7%; Average loss: 0.0295\n",
      "Iteration: 5870; Percent complete: 97.8%; Average loss: 0.0298\n",
      "Iteration: 5880; Percent complete: 98.0%; Average loss: 0.0306\n",
      "Iteration: 5890; Percent complete: 98.2%; Average loss: 0.0295\n",
      "Iteration: 5900; Percent complete: 98.3%; Average loss: 0.0289\n",
      "Iteration: 5910; Percent complete: 98.5%; Average loss: 0.0278\n",
      "Iteration: 5920; Percent complete: 98.7%; Average loss: 0.0250\n",
      "Iteration: 5930; Percent complete: 98.8%; Average loss: 0.0255\n",
      "Iteration: 5940; Percent complete: 99.0%; Average loss: 0.0255\n",
      "Iteration: 5950; Percent complete: 99.2%; Average loss: 0.0241\n",
      "Iteration: 5960; Percent complete: 99.3%; Average loss: 0.0255\n",
      "Iteration: 5970; Percent complete: 99.5%; Average loss: 0.0240\n",
      "Iteration: 5980; Percent complete: 99.7%; Average loss: 0.0227\n",
      "Iteration: 5990; Percent complete: 99.8%; Average loss: 0.0232\n",
      "Iteration: 6000; Percent complete: 100.0%; Average loss: 0.0224\n",
      "content/cb_model/Chat/2-4_512\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'content/'\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 6000\n",
    "print_every = 10\n",
    "save_every = 2000\n",
    "loadFilename = None\n",
    "corpus_name=\"Chat\"\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "print(\"Starting Training!\")\n",
    "lossvalues = trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f53283af160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(lossvalues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bleu score Calculation** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.18518518518518515 0.02668802563418119\n",
      "Total Bleu Score for 1 grams on testing pairs:  0.269192600205076\n",
      "Total Bleu Score for 2 grams on testing pairs:  0.1459588921461553\n"
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "gram1_bleu_score = []\n",
    "gram2_bleu_score = []\n",
    "for i in range(0,len(testpairs),1):\n",
    "  \n",
    "      input_sentence = testpairs[i][0]\n",
    "\n",
    "      reference = testpairs[i][1:]\n",
    "      templist = []\n",
    "      for k in range(len(reference)):\n",
    "        if(reference[k]!=''):\n",
    "          temp = reference[k].split(' ')\n",
    "          templist.append(temp)\n",
    "\n",
    "\n",
    "      input_sentence = normalizeString(input_sentence)\n",
    "      output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "      output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "      chencherry = SmoothingFunction()\n",
    "    #   print(output_words)\n",
    "    #   print(templist)\n",
    "      score1 = sentence_bleu(templist,output_words,weights=(1, 0, 0, 0) ,smoothing_function=chencherry.method1)\n",
    "      score2 = sentence_bleu(templist,output_words,weights=(0.5, 0.5, 0, 0),smoothing_function=chencherry.method1) \n",
    "      gram1_bleu_score.append(score1)\n",
    "      gram2_bleu_score.append(score2)\n",
    "      if i%1000 == 0:\n",
    "        print(i,sum(gram1_bleu_score)/len(gram1_bleu_score),sum(gram2_bleu_score)/len(gram2_bleu_score))\n",
    "print(\"Total Bleu Score for 1 grams on testing pairs: \", sum(gram1_bleu_score)/len(gram1_bleu_score) )  \n",
    "print(\"Total Bleu Score for 2 grams on testing pairs: \", sum(gram2_bleu_score)/len(gram2_bleu_score) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: what type of food would you like ? asian oriental ? what area ? is available centre .\n",
      "Bot: what price range and in what area would you like to stay ? wifi ? wifi town . parking ? is a priced rating .\n",
      "Error: Encountered unknown word.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ed6594a68986>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Begin chatting (uncomment and run the following line to begin)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mevaluateInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m# input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Hi, how are you?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-ec18c2cf8348>\u001b[0m in \u001b[0;36mevaluateInput\u001b[0;34m(encoder, decoder, searcher, voc)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'q'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         )\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, voc)\n",
    "\n",
    "# ME : I'm looking for a restaurant.\n",
    "# Bot: what type of food would you like ? asian oriental ? what area ? is available centre .\n",
    "# ME : I'm looking for a place to stay.\n",
    "# Bot: what price range and in what area would you like to stay ? wifi ? wifi town . parking ? is a priced rating .\n",
    "# ME : I need a lift.\n",
    "# Error: Encountered unknown word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Beam Decoder**\n",
    "\n",
    "The difference between greedy search and beam search is decoder function. Thus, greedy search function name is greedy_decode, and beam search function name is beam_decode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence:\n",
    "    def __init__(self, decoder_hidden, last_idx=SOS_token, sentence_idxes=[], sentence_scores=[]):\n",
    "        if(len(sentence_idxes) != len(sentence_scores)):\n",
    "            raise ValueError(\"length of indexes and scores should be the same\")\n",
    "        self.decoder_hidden = decoder_hidden\n",
    "        self.last_idx = last_idx\n",
    "        self.sentence_idxes =  sentence_idxes\n",
    "        self.sentence_scores = sentence_scores\n",
    "\n",
    "    def avgScore(self):\n",
    "        if len(self.sentence_scores) == 0:\n",
    "            raise ValueError(\"Calculate average score of sentence, but got no word\")\n",
    "        # return mean of sentence_score\n",
    "        return sum(self.sentence_scores) / len(self.sentence_scores)\n",
    "\n",
    "    def addTopk(self, topi, topv, decoder_hidden, beam_size, voc):\n",
    "        topv = torch.log(topv)\n",
    "        terminates, sentences = [], []\n",
    "        for i in range(beam_size):\n",
    "            if topi[0][i] == EOS_token:\n",
    "                terminates.append(([voc.index2word[idx.item()] for idx in self.sentence_idxes] + ['<EOS>'],\n",
    "                                   self.avgScore())) \n",
    "                continue\n",
    "            idxes = self.sentence_idxes[:] \n",
    "            scores = self.sentence_scores[:] \n",
    "            idxes.append(topi[0][i])\n",
    "            scores.append(topv[0][i])\n",
    "            sentences.append(Sentence(decoder_hidden, topi[0][i], idxes, scores))\n",
    "        return terminates, sentences\n",
    "\n",
    "    def toWordScore(self, voc):\n",
    "        \n",
    "        words = []\n",
    "        for i in range(len(self.sentence_idxes)):\n",
    "            if self.sentence_idxes[i] == EOS_token:\n",
    "                words.append('<EOS>')\n",
    "            else:\n",
    "                words.append(voc.index2word[self.sentence_idxes[i].item()])\n",
    "       \n",
    "        if self.sentence_idxes[-1] != EOS_token:\n",
    "            words.append('<EOS>')\n",
    "        return (words, self.avgScore())\n",
    "\n",
    "    def __repr__(self):\n",
    "        res = f\"Sentence with indices {self.sentence_idxes} \"\n",
    "        res += f\"and scores {self.sentence_scores}\"\n",
    "        return res\n",
    "    \n",
    "def beam_decode(decoder, decoder_hidden, encoder_outputs, voc, beam_size, max_length=MAX_LENGTH):\n",
    "    terminal_sentences, prev_top_sentences, next_top_sentences = [], [], []\n",
    "    prev_top_sentences.append(Sentence(decoder_hidden))\n",
    "    for i in range(max_length):\n",
    "        \n",
    "        for sentence in prev_top_sentences:\n",
    "            decoder_input = torch.LongTensor([[sentence.last_idx]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "\n",
    "            decoder_hidden = sentence.decoder_hidden\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(beam_size)\n",
    "            term, top = sentence.addTopk(topi, topv, decoder_hidden, beam_size, voc)\n",
    "            terminal_sentences.extend(term)\n",
    "            next_top_sentences.extend(top)\n",
    "           \n",
    "        \n",
    "        next_top_sentences.sort(key=lambda s: s.avgScore(), reverse=True)\n",
    "        prev_top_sentences = next_top_sentences[:beam_size]\n",
    "        next_top_sentences = []\n",
    "        \n",
    "\n",
    "    terminal_sentences += [sentence.toWordScore(voc) for sentence in prev_top_sentences]\n",
    "    terminal_sentences.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    n = min(len(terminal_sentences), 15)\n",
    "    return terminal_sentences[:n]\n",
    "\n",
    "\n",
    "\n",
    "class BeamSearchDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder, decoder, voc, beam_size=10):\n",
    "        super(BeamSearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.voc = voc\n",
    "        self.beam_size = beam_size\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden[:self.decoder.n_layers]\n",
    "        \n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        sentences = beam_decode(self.decoder, decoder_hidden, encoder_outputs, self.voc, self.beam_size, max_length)\n",
    "        \n",
    "        \n",
    "        all_tokens = [torch.tensor(self.voc.word2index.get(w, 0)) for w in sentences[0][0]]\n",
    "        return all_tokens, None\n",
    "\n",
    "    def __str__(self):\n",
    "        res = f\"BeamSearchDecoder with beam size {self.beam_size}\"\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "\n",
    "hidden_size = 512\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.5\n",
    "batch_size = 256 \n",
    "loadFilename = None\n",
    "\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training!\n",
      "Initializing ...\n",
      "Training...\n",
      "ipykernel_launcher:4: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:25.)\n",
      "Iteration: 10; Percent complete: 0.2%; Average loss: 6.9570\n",
      "Iteration: 20; Percent complete: 0.3%; Average loss: 5.5268\n",
      "Iteration: 30; Percent complete: 0.5%; Average loss: 5.0958\n",
      "Iteration: 40; Percent complete: 0.7%; Average loss: 4.9760\n",
      "Iteration: 50; Percent complete: 0.8%; Average loss: 4.8879\n",
      "Iteration: 60; Percent complete: 1.0%; Average loss: 4.8355\n",
      "Iteration: 70; Percent complete: 1.2%; Average loss: 4.7691\n",
      "Iteration: 80; Percent complete: 1.3%; Average loss: 4.6698\n",
      "Iteration: 90; Percent complete: 1.5%; Average loss: 4.6232\n",
      "Iteration: 100; Percent complete: 1.7%; Average loss: 4.5372\n",
      "Iteration: 110; Percent complete: 1.8%; Average loss: 4.4451\n",
      "Iteration: 120; Percent complete: 2.0%; Average loss: 4.4164\n",
      "Iteration: 130; Percent complete: 2.2%; Average loss: 4.3491\n",
      "Iteration: 140; Percent complete: 2.3%; Average loss: 4.3035\n",
      "Iteration: 150; Percent complete: 2.5%; Average loss: 4.2254\n",
      "Iteration: 160; Percent complete: 2.7%; Average loss: 4.1238\n",
      "Iteration: 170; Percent complete: 2.8%; Average loss: 4.0540\n",
      "Iteration: 180; Percent complete: 3.0%; Average loss: 3.9935\n",
      "Iteration: 190; Percent complete: 3.2%; Average loss: 3.9240\n",
      "Iteration: 200; Percent complete: 3.3%; Average loss: 3.8168\n",
      "Iteration: 210; Percent complete: 3.5%; Average loss: 3.7739\n",
      "Iteration: 220; Percent complete: 3.7%; Average loss: 3.6678\n",
      "Iteration: 230; Percent complete: 3.8%; Average loss: 3.6069\n",
      "Iteration: 240; Percent complete: 4.0%; Average loss: 3.5565\n",
      "Iteration: 250; Percent complete: 4.2%; Average loss: 3.4932\n",
      "Iteration: 260; Percent complete: 4.3%; Average loss: 3.3831\n",
      "Iteration: 270; Percent complete: 4.5%; Average loss: 3.3539\n",
      "Iteration: 280; Percent complete: 4.7%; Average loss: 3.2880\n",
      "Iteration: 290; Percent complete: 4.8%; Average loss: 3.2247\n",
      "Iteration: 300; Percent complete: 5.0%; Average loss: 3.1531\n",
      "Iteration: 310; Percent complete: 5.2%; Average loss: 3.0952\n",
      "Iteration: 320; Percent complete: 5.3%; Average loss: 3.0592\n",
      "Iteration: 330; Percent complete: 5.5%; Average loss: 2.9815\n",
      "Iteration: 340; Percent complete: 5.7%; Average loss: 2.9127\n",
      "Iteration: 350; Percent complete: 5.8%; Average loss: 2.8569\n",
      "Iteration: 360; Percent complete: 6.0%; Average loss: 2.8226\n",
      "Iteration: 370; Percent complete: 6.2%; Average loss: 2.7494\n",
      "Iteration: 380; Percent complete: 6.3%; Average loss: 2.6857\n",
      "Iteration: 390; Percent complete: 6.5%; Average loss: 2.6865\n",
      "Iteration: 400; Percent complete: 6.7%; Average loss: 2.6601\n",
      "Iteration: 410; Percent complete: 6.8%; Average loss: 2.6064\n",
      "Iteration: 420; Percent complete: 7.0%; Average loss: 2.5813\n",
      "Iteration: 430; Percent complete: 7.2%; Average loss: 2.5305\n",
      "Iteration: 440; Percent complete: 7.3%; Average loss: 2.5426\n",
      "Iteration: 450; Percent complete: 7.5%; Average loss: 2.4720\n",
      "Iteration: 460; Percent complete: 7.7%; Average loss: 2.4725\n",
      "Iteration: 470; Percent complete: 7.8%; Average loss: 2.4141\n",
      "Iteration: 480; Percent complete: 8.0%; Average loss: 2.4003\n",
      "Iteration: 490; Percent complete: 8.2%; Average loss: 2.3753\n",
      "Iteration: 500; Percent complete: 8.3%; Average loss: 2.3332\n",
      "Iteration: 510; Percent complete: 8.5%; Average loss: 2.3311\n",
      "Iteration: 520; Percent complete: 8.7%; Average loss: 2.3089\n",
      "Iteration: 530; Percent complete: 8.8%; Average loss: 2.2869\n",
      "Iteration: 540; Percent complete: 9.0%; Average loss: 2.2583\n",
      "Iteration: 550; Percent complete: 9.2%; Average loss: 2.2494\n",
      "Iteration: 560; Percent complete: 9.3%; Average loss: 2.2276\n",
      "Iteration: 570; Percent complete: 9.5%; Average loss: 2.1997\n",
      "Iteration: 580; Percent complete: 9.7%; Average loss: 2.1647\n",
      "Iteration: 590; Percent complete: 9.8%; Average loss: 2.1539\n",
      "Iteration: 600; Percent complete: 10.0%; Average loss: 2.1377\n",
      "Iteration: 610; Percent complete: 10.2%; Average loss: 2.1159\n",
      "Iteration: 620; Percent complete: 10.3%; Average loss: 2.0655\n",
      "Iteration: 630; Percent complete: 10.5%; Average loss: 2.0789\n",
      "Iteration: 640; Percent complete: 10.7%; Average loss: 2.0640\n",
      "Iteration: 650; Percent complete: 10.8%; Average loss: 2.0237\n",
      "Iteration: 660; Percent complete: 11.0%; Average loss: 2.0147\n",
      "Iteration: 670; Percent complete: 11.2%; Average loss: 1.9916\n",
      "Iteration: 680; Percent complete: 11.3%; Average loss: 1.9881\n",
      "Iteration: 690; Percent complete: 11.5%; Average loss: 1.9785\n",
      "Iteration: 700; Percent complete: 11.7%; Average loss: 1.9433\n",
      "Iteration: 710; Percent complete: 11.8%; Average loss: 1.9441\n",
      "Iteration: 720; Percent complete: 12.0%; Average loss: 1.9337\n",
      "Iteration: 730; Percent complete: 12.2%; Average loss: 1.9361\n",
      "Iteration: 740; Percent complete: 12.3%; Average loss: 1.9061\n",
      "Iteration: 750; Percent complete: 12.5%; Average loss: 1.8798\n",
      "Iteration: 760; Percent complete: 12.7%; Average loss: 1.8343\n",
      "Iteration: 770; Percent complete: 12.8%; Average loss: 1.8187\n",
      "Iteration: 780; Percent complete: 13.0%; Average loss: 1.8349\n",
      "Iteration: 790; Percent complete: 13.2%; Average loss: 1.8001\n",
      "Iteration: 800; Percent complete: 13.3%; Average loss: 1.7927\n",
      "Iteration: 810; Percent complete: 13.5%; Average loss: 1.7991\n",
      "Iteration: 820; Percent complete: 13.7%; Average loss: 1.7707\n",
      "Iteration: 830; Percent complete: 13.8%; Average loss: 1.7776\n",
      "Iteration: 840; Percent complete: 14.0%; Average loss: 1.7654\n",
      "Iteration: 850; Percent complete: 14.2%; Average loss: 1.7381\n",
      "Iteration: 860; Percent complete: 14.3%; Average loss: 1.6995\n",
      "Iteration: 870; Percent complete: 14.5%; Average loss: 1.7078\n",
      "Iteration: 880; Percent complete: 14.7%; Average loss: 1.6990\n",
      "Iteration: 890; Percent complete: 14.8%; Average loss: 1.6615\n",
      "Iteration: 900; Percent complete: 15.0%; Average loss: 1.6697\n",
      "Iteration: 910; Percent complete: 15.2%; Average loss: 1.6715\n",
      "Iteration: 920; Percent complete: 15.3%; Average loss: 1.6509\n",
      "Iteration: 930; Percent complete: 15.5%; Average loss: 1.6411\n",
      "Iteration: 940; Percent complete: 15.7%; Average loss: 1.6108\n",
      "Iteration: 950; Percent complete: 15.8%; Average loss: 1.6151\n",
      "Iteration: 960; Percent complete: 16.0%; Average loss: 1.5934\n",
      "Iteration: 970; Percent complete: 16.2%; Average loss: 1.5860\n",
      "Iteration: 980; Percent complete: 16.3%; Average loss: 1.5590\n",
      "Iteration: 990; Percent complete: 16.5%; Average loss: 1.5634\n",
      "Iteration: 1000; Percent complete: 16.7%; Average loss: 1.5389\n",
      "Iteration: 1010; Percent complete: 16.8%; Average loss: 1.5401\n",
      "Iteration: 1020; Percent complete: 17.0%; Average loss: 1.5066\n",
      "Iteration: 1030; Percent complete: 17.2%; Average loss: 1.5125\n",
      "Iteration: 1040; Percent complete: 17.3%; Average loss: 1.4971\n",
      "Iteration: 1050; Percent complete: 17.5%; Average loss: 1.4789\n",
      "Iteration: 1060; Percent complete: 17.7%; Average loss: 1.4627\n",
      "Iteration: 1070; Percent complete: 17.8%; Average loss: 1.4629\n",
      "Iteration: 1080; Percent complete: 18.0%; Average loss: 1.4324\n",
      "Iteration: 1090; Percent complete: 18.2%; Average loss: 1.4233\n",
      "Iteration: 1100; Percent complete: 18.3%; Average loss: 1.4170\n",
      "Iteration: 1110; Percent complete: 18.5%; Average loss: 1.3878\n",
      "Iteration: 1120; Percent complete: 18.7%; Average loss: 1.4071\n",
      "Iteration: 1130; Percent complete: 18.8%; Average loss: 1.3967\n",
      "Iteration: 1140; Percent complete: 19.0%; Average loss: 1.3761\n",
      "Iteration: 1150; Percent complete: 19.2%; Average loss: 1.3874\n",
      "Iteration: 1160; Percent complete: 19.3%; Average loss: 1.3636\n",
      "Iteration: 1170; Percent complete: 19.5%; Average loss: 1.3255\n",
      "Iteration: 1180; Percent complete: 19.7%; Average loss: 1.3223\n",
      "Iteration: 1190; Percent complete: 19.8%; Average loss: 1.2935\n",
      "Iteration: 1200; Percent complete: 20.0%; Average loss: 1.3018\n",
      "Iteration: 1210; Percent complete: 20.2%; Average loss: 1.2947\n",
      "Iteration: 1220; Percent complete: 20.3%; Average loss: 1.2753\n",
      "Iteration: 1230; Percent complete: 20.5%; Average loss: 1.2871\n",
      "Iteration: 1240; Percent complete: 20.7%; Average loss: 1.2562\n",
      "Iteration: 1250; Percent complete: 20.8%; Average loss: 1.2545\n",
      "Iteration: 1260; Percent complete: 21.0%; Average loss: 1.2390\n",
      "Iteration: 1270; Percent complete: 21.2%; Average loss: 1.1923\n",
      "Iteration: 1280; Percent complete: 21.3%; Average loss: 1.2044\n",
      "Iteration: 1290; Percent complete: 21.5%; Average loss: 1.2010\n",
      "Iteration: 1300; Percent complete: 21.7%; Average loss: 1.1760\n",
      "Iteration: 1310; Percent complete: 21.8%; Average loss: 1.1723\n",
      "Iteration: 1320; Percent complete: 22.0%; Average loss: 1.1630\n",
      "Iteration: 1330; Percent complete: 22.2%; Average loss: 1.1652\n",
      "Iteration: 1340; Percent complete: 22.3%; Average loss: 1.1406\n",
      "Iteration: 1350; Percent complete: 22.5%; Average loss: 1.1391\n",
      "Iteration: 1360; Percent complete: 22.7%; Average loss: 1.1062\n",
      "Iteration: 1370; Percent complete: 22.8%; Average loss: 1.1081\n",
      "Iteration: 1380; Percent complete: 23.0%; Average loss: 1.0848\n",
      "Iteration: 1390; Percent complete: 23.2%; Average loss: 1.1004\n",
      "Iteration: 1400; Percent complete: 23.3%; Average loss: 1.0698\n",
      "Iteration: 1410; Percent complete: 23.5%; Average loss: 1.0682\n",
      "Iteration: 1420; Percent complete: 23.7%; Average loss: 1.0735\n",
      "Iteration: 1430; Percent complete: 23.8%; Average loss: 1.0392\n",
      "Iteration: 1440; Percent complete: 24.0%; Average loss: 1.0218\n",
      "Iteration: 1450; Percent complete: 24.2%; Average loss: 1.0235\n",
      "Iteration: 1460; Percent complete: 24.3%; Average loss: 1.0373\n",
      "Iteration: 1470; Percent complete: 24.5%; Average loss: 1.0028\n",
      "Iteration: 1480; Percent complete: 24.7%; Average loss: 1.0022\n",
      "Iteration: 1490; Percent complete: 24.8%; Average loss: 0.9953\n",
      "Iteration: 1500; Percent complete: 25.0%; Average loss: 0.9705\n",
      "Iteration: 1510; Percent complete: 25.2%; Average loss: 0.9627\n",
      "Iteration: 1520; Percent complete: 25.3%; Average loss: 0.9695\n",
      "Iteration: 1530; Percent complete: 25.5%; Average loss: 0.9446\n",
      "Iteration: 1540; Percent complete: 25.7%; Average loss: 0.9493\n",
      "Iteration: 1550; Percent complete: 25.8%; Average loss: 0.9184\n",
      "Iteration: 1560; Percent complete: 26.0%; Average loss: 0.9184\n",
      "Iteration: 1570; Percent complete: 26.2%; Average loss: 0.8992\n",
      "Iteration: 1580; Percent complete: 26.3%; Average loss: 0.8870\n",
      "Iteration: 1590; Percent complete: 26.5%; Average loss: 0.8877\n",
      "Iteration: 1600; Percent complete: 26.7%; Average loss: 0.8905\n",
      "Iteration: 1610; Percent complete: 26.8%; Average loss: 0.8661\n",
      "Iteration: 1620; Percent complete: 27.0%; Average loss: 0.8681\n",
      "Iteration: 1630; Percent complete: 27.2%; Average loss: 0.8233\n",
      "Iteration: 1640; Percent complete: 27.3%; Average loss: 0.8538\n",
      "Iteration: 1650; Percent complete: 27.5%; Average loss: 0.8266\n",
      "Iteration: 1660; Percent complete: 27.7%; Average loss: 0.8055\n",
      "Iteration: 1670; Percent complete: 27.8%; Average loss: 0.8000\n",
      "Iteration: 1680; Percent complete: 28.0%; Average loss: 0.8005\n",
      "Iteration: 1690; Percent complete: 28.2%; Average loss: 0.7848\n",
      "Iteration: 1700; Percent complete: 28.3%; Average loss: 0.7735\n",
      "Iteration: 1710; Percent complete: 28.5%; Average loss: 0.7719\n",
      "Iteration: 1720; Percent complete: 28.7%; Average loss: 0.7568\n",
      "Iteration: 1730; Percent complete: 28.8%; Average loss: 0.7526\n",
      "Iteration: 1740; Percent complete: 29.0%; Average loss: 0.7384\n",
      "Iteration: 1750; Percent complete: 29.2%; Average loss: 0.7256\n",
      "Iteration: 1760; Percent complete: 29.3%; Average loss: 0.7283\n",
      "Iteration: 1770; Percent complete: 29.5%; Average loss: 0.7197\n",
      "Iteration: 1780; Percent complete: 29.7%; Average loss: 0.6992\n",
      "Iteration: 1790; Percent complete: 29.8%; Average loss: 0.6740\n",
      "Iteration: 1800; Percent complete: 30.0%; Average loss: 0.6882\n",
      "Iteration: 1810; Percent complete: 30.2%; Average loss: 0.6722\n",
      "Iteration: 1820; Percent complete: 30.3%; Average loss: 0.6691\n",
      "Iteration: 1830; Percent complete: 30.5%; Average loss: 0.6550\n",
      "Iteration: 1840; Percent complete: 30.7%; Average loss: 0.6650\n",
      "Iteration: 1850; Percent complete: 30.8%; Average loss: 0.6351\n",
      "Iteration: 1860; Percent complete: 31.0%; Average loss: 0.6325\n",
      "Iteration: 1870; Percent complete: 31.2%; Average loss: 0.6366\n",
      "Iteration: 1880; Percent complete: 31.3%; Average loss: 0.6155\n",
      "Iteration: 1890; Percent complete: 31.5%; Average loss: 0.6256\n",
      "Iteration: 1900; Percent complete: 31.7%; Average loss: 0.6055\n",
      "Iteration: 1910; Percent complete: 31.8%; Average loss: 0.5941\n",
      "Iteration: 1920; Percent complete: 32.0%; Average loss: 0.5774\n",
      "Iteration: 1930; Percent complete: 32.2%; Average loss: 0.5728\n",
      "Iteration: 1940; Percent complete: 32.3%; Average loss: 0.5712\n",
      "Iteration: 1950; Percent complete: 32.5%; Average loss: 0.5634\n",
      "Iteration: 1960; Percent complete: 32.7%; Average loss: 0.5569\n",
      "Iteration: 1970; Percent complete: 32.8%; Average loss: 0.5615\n",
      "Iteration: 1980; Percent complete: 33.0%; Average loss: 0.5547\n",
      "Iteration: 1990; Percent complete: 33.2%; Average loss: 0.5370\n",
      "Iteration: 2000; Percent complete: 33.3%; Average loss: 0.5269\n",
      "content/cb_model/Chat/2-4_512\n",
      "Iteration: 2010; Percent complete: 33.5%; Average loss: 0.5269\n",
      "Iteration: 2020; Percent complete: 33.7%; Average loss: 0.5077\n",
      "Iteration: 2030; Percent complete: 33.8%; Average loss: 0.5211\n",
      "Iteration: 2040; Percent complete: 34.0%; Average loss: 0.5016\n",
      "Iteration: 2050; Percent complete: 34.2%; Average loss: 0.4887\n",
      "Iteration: 2060; Percent complete: 34.3%; Average loss: 0.4839\n",
      "Iteration: 2070; Percent complete: 34.5%; Average loss: 0.4656\n",
      "Iteration: 2080; Percent complete: 34.7%; Average loss: 0.4733\n",
      "Iteration: 2090; Percent complete: 34.8%; Average loss: 0.4595\n",
      "Iteration: 2100; Percent complete: 35.0%; Average loss: 0.4667\n",
      "Iteration: 2110; Percent complete: 35.2%; Average loss: 0.4528\n",
      "Iteration: 2120; Percent complete: 35.3%; Average loss: 0.4499\n",
      "Iteration: 2130; Percent complete: 35.5%; Average loss: 0.4385\n",
      "Iteration: 2140; Percent complete: 35.7%; Average loss: 0.4477\n",
      "Iteration: 2150; Percent complete: 35.8%; Average loss: 0.4282\n",
      "Iteration: 2160; Percent complete: 36.0%; Average loss: 0.4203\n",
      "Iteration: 2170; Percent complete: 36.2%; Average loss: 0.4159\n",
      "Iteration: 2180; Percent complete: 36.3%; Average loss: 0.4256\n",
      "Iteration: 2190; Percent complete: 36.5%; Average loss: 0.3992\n",
      "Iteration: 2200; Percent complete: 36.7%; Average loss: 0.4169\n",
      "Iteration: 2210; Percent complete: 36.8%; Average loss: 0.3985\n",
      "Iteration: 2220; Percent complete: 37.0%; Average loss: 0.4018\n",
      "Iteration: 2230; Percent complete: 37.2%; Average loss: 0.3896\n",
      "Iteration: 2240; Percent complete: 37.3%; Average loss: 0.3869\n",
      "Iteration: 2250; Percent complete: 37.5%; Average loss: 0.3746\n",
      "Iteration: 2260; Percent complete: 37.7%; Average loss: 0.3696\n",
      "Iteration: 2270; Percent complete: 37.8%; Average loss: 0.3675\n",
      "Iteration: 2280; Percent complete: 38.0%; Average loss: 0.3628\n",
      "Iteration: 2290; Percent complete: 38.2%; Average loss: 0.3601\n",
      "Iteration: 2300; Percent complete: 38.3%; Average loss: 0.3493\n",
      "Iteration: 2310; Percent complete: 38.5%; Average loss: 0.3485\n",
      "Iteration: 2320; Percent complete: 38.7%; Average loss: 0.3459\n",
      "Iteration: 2330; Percent complete: 38.8%; Average loss: 0.3384\n",
      "Iteration: 2340; Percent complete: 39.0%; Average loss: 0.3297\n",
      "Iteration: 2350; Percent complete: 39.2%; Average loss: 0.3296\n",
      "Iteration: 2360; Percent complete: 39.3%; Average loss: 0.3236\n",
      "Iteration: 2370; Percent complete: 39.5%; Average loss: 0.3178\n",
      "Iteration: 2380; Percent complete: 39.7%; Average loss: 0.3203\n",
      "Iteration: 2390; Percent complete: 39.8%; Average loss: 0.3188\n",
      "Iteration: 2400; Percent complete: 40.0%; Average loss: 0.3158\n",
      "Iteration: 2410; Percent complete: 40.2%; Average loss: 0.3104\n",
      "Iteration: 2420; Percent complete: 40.3%; Average loss: 0.2982\n",
      "Iteration: 2430; Percent complete: 40.5%; Average loss: 0.2944\n",
      "Iteration: 2440; Percent complete: 40.7%; Average loss: 0.2874\n",
      "Iteration: 2450; Percent complete: 40.8%; Average loss: 0.2805\n",
      "Iteration: 2460; Percent complete: 41.0%; Average loss: 0.2868\n",
      "Iteration: 2470; Percent complete: 41.2%; Average loss: 0.2783\n",
      "Iteration: 2480; Percent complete: 41.3%; Average loss: 0.2791\n",
      "Iteration: 2490; Percent complete: 41.5%; Average loss: 0.2616\n",
      "Iteration: 2500; Percent complete: 41.7%; Average loss: 0.2719\n",
      "Iteration: 2510; Percent complete: 41.8%; Average loss: 0.2639\n",
      "Iteration: 2520; Percent complete: 42.0%; Average loss: 0.2684\n",
      "Iteration: 2530; Percent complete: 42.2%; Average loss: 0.2582\n",
      "Iteration: 2540; Percent complete: 42.3%; Average loss: 0.2480\n",
      "Iteration: 2550; Percent complete: 42.5%; Average loss: 0.2505\n",
      "Iteration: 2560; Percent complete: 42.7%; Average loss: 0.2488\n",
      "Iteration: 2570; Percent complete: 42.8%; Average loss: 0.2501\n",
      "Iteration: 2580; Percent complete: 43.0%; Average loss: 0.2429\n",
      "Iteration: 2590; Percent complete: 43.2%; Average loss: 0.2452\n",
      "Iteration: 2600; Percent complete: 43.3%; Average loss: 0.2359\n",
      "Iteration: 2610; Percent complete: 43.5%; Average loss: 0.2372\n",
      "Iteration: 2620; Percent complete: 43.7%; Average loss: 0.2293\n",
      "Iteration: 2630; Percent complete: 43.8%; Average loss: 0.2368\n",
      "Iteration: 2640; Percent complete: 44.0%; Average loss: 0.2338\n",
      "Iteration: 2650; Percent complete: 44.2%; Average loss: 0.2275\n",
      "Iteration: 2660; Percent complete: 44.3%; Average loss: 0.2327\n",
      "Iteration: 2670; Percent complete: 44.5%; Average loss: 0.2255\n",
      "Iteration: 2680; Percent complete: 44.7%; Average loss: 0.2259\n",
      "Iteration: 2690; Percent complete: 44.8%; Average loss: 0.2191\n",
      "Iteration: 2700; Percent complete: 45.0%; Average loss: 0.2174\n",
      "Iteration: 2710; Percent complete: 45.2%; Average loss: 0.2098\n",
      "Iteration: 2720; Percent complete: 45.3%; Average loss: 0.2093\n",
      "Iteration: 2730; Percent complete: 45.5%; Average loss: 0.2039\n",
      "Iteration: 2740; Percent complete: 45.7%; Average loss: 0.1993\n",
      "Iteration: 2750; Percent complete: 45.8%; Average loss: 0.2023\n",
      "Iteration: 2760; Percent complete: 46.0%; Average loss: 0.2004\n",
      "Iteration: 2770; Percent complete: 46.2%; Average loss: 0.1939\n",
      "Iteration: 2780; Percent complete: 46.3%; Average loss: 0.1961\n",
      "Iteration: 2790; Percent complete: 46.5%; Average loss: 0.1878\n",
      "Iteration: 2800; Percent complete: 46.7%; Average loss: 0.1820\n",
      "Iteration: 2810; Percent complete: 46.8%; Average loss: 0.1836\n",
      "Iteration: 2820; Percent complete: 47.0%; Average loss: 0.1807\n",
      "Iteration: 2830; Percent complete: 47.2%; Average loss: 0.1842\n",
      "Iteration: 2840; Percent complete: 47.3%; Average loss: 0.1776\n",
      "Iteration: 2850; Percent complete: 47.5%; Average loss: 0.1782\n",
      "Iteration: 2860; Percent complete: 47.7%; Average loss: 0.1774\n",
      "Iteration: 2870; Percent complete: 47.8%; Average loss: 0.1690\n",
      "Iteration: 2880; Percent complete: 48.0%; Average loss: 0.1665\n",
      "Iteration: 2890; Percent complete: 48.2%; Average loss: 0.1665\n",
      "Iteration: 2900; Percent complete: 48.3%; Average loss: 0.1598\n",
      "Iteration: 2910; Percent complete: 48.5%; Average loss: 0.1603\n",
      "Iteration: 2920; Percent complete: 48.7%; Average loss: 0.1537\n",
      "Iteration: 2930; Percent complete: 48.8%; Average loss: 0.1547\n",
      "Iteration: 2940; Percent complete: 49.0%; Average loss: 0.1528\n",
      "Iteration: 2950; Percent complete: 49.2%; Average loss: 0.1538\n",
      "Iteration: 2960; Percent complete: 49.3%; Average loss: 0.1503\n",
      "Iteration: 2970; Percent complete: 49.5%; Average loss: 0.1477\n",
      "Iteration: 2980; Percent complete: 49.7%; Average loss: 0.1440\n",
      "Iteration: 2990; Percent complete: 49.8%; Average loss: 0.1403\n",
      "Iteration: 3000; Percent complete: 50.0%; Average loss: 0.1428\n",
      "Iteration: 3010; Percent complete: 50.2%; Average loss: 0.1399\n",
      "Iteration: 3020; Percent complete: 50.3%; Average loss: 0.1374\n",
      "Iteration: 3030; Percent complete: 50.5%; Average loss: 0.1389\n",
      "Iteration: 3040; Percent complete: 50.7%; Average loss: 0.1362\n",
      "Iteration: 3050; Percent complete: 50.8%; Average loss: 0.1320\n",
      "Iteration: 3060; Percent complete: 51.0%; Average loss: 0.1340\n",
      "Iteration: 3070; Percent complete: 51.2%; Average loss: 0.1339\n",
      "Iteration: 3080; Percent complete: 51.3%; Average loss: 0.1309\n",
      "Iteration: 3090; Percent complete: 51.5%; Average loss: 0.1287\n",
      "Iteration: 3100; Percent complete: 51.7%; Average loss: 0.1279\n",
      "Iteration: 3110; Percent complete: 51.8%; Average loss: 0.1230\n",
      "Iteration: 3120; Percent complete: 52.0%; Average loss: 0.1231\n",
      "Iteration: 3130; Percent complete: 52.2%; Average loss: 0.1198\n",
      "Iteration: 3140; Percent complete: 52.3%; Average loss: 0.1207\n",
      "Iteration: 3150; Percent complete: 52.5%; Average loss: 0.1203\n",
      "Iteration: 3160; Percent complete: 52.7%; Average loss: 0.1193\n",
      "Iteration: 3170; Percent complete: 52.8%; Average loss: 0.1166\n",
      "Iteration: 3180; Percent complete: 53.0%; Average loss: 0.1141\n",
      "Iteration: 3190; Percent complete: 53.2%; Average loss: 0.1158\n",
      "Iteration: 3200; Percent complete: 53.3%; Average loss: 0.1136\n",
      "Iteration: 3210; Percent complete: 53.5%; Average loss: 0.1147\n",
      "Iteration: 3220; Percent complete: 53.7%; Average loss: 0.1167\n",
      "Iteration: 3230; Percent complete: 53.8%; Average loss: 0.1148\n",
      "Iteration: 3240; Percent complete: 54.0%; Average loss: 0.1090\n",
      "Iteration: 3250; Percent complete: 54.2%; Average loss: 0.1070\n",
      "Iteration: 3260; Percent complete: 54.3%; Average loss: 0.1060\n",
      "Iteration: 3270; Percent complete: 54.5%; Average loss: 0.1074\n",
      "Iteration: 3280; Percent complete: 54.7%; Average loss: 0.1055\n",
      "Iteration: 3290; Percent complete: 54.8%; Average loss: 0.1054\n",
      "Iteration: 3300; Percent complete: 55.0%; Average loss: 0.1073\n",
      "Iteration: 3310; Percent complete: 55.2%; Average loss: 0.1030\n",
      "Iteration: 3320; Percent complete: 55.3%; Average loss: 0.1028\n",
      "Iteration: 3330; Percent complete: 55.5%; Average loss: 0.1003\n",
      "Iteration: 3340; Percent complete: 55.7%; Average loss: 0.1005\n",
      "Iteration: 3350; Percent complete: 55.8%; Average loss: 0.0975\n",
      "Iteration: 3360; Percent complete: 56.0%; Average loss: 0.0985\n",
      "Iteration: 3370; Percent complete: 56.2%; Average loss: 0.0981\n",
      "Iteration: 3380; Percent complete: 56.3%; Average loss: 0.0992\n",
      "Iteration: 3390; Percent complete: 56.5%; Average loss: 0.0953\n",
      "Iteration: 3400; Percent complete: 56.7%; Average loss: 0.0935\n",
      "Iteration: 3410; Percent complete: 56.8%; Average loss: 0.0909\n",
      "Iteration: 3420; Percent complete: 57.0%; Average loss: 0.0963\n",
      "Iteration: 3430; Percent complete: 57.2%; Average loss: 0.0938\n",
      "Iteration: 3440; Percent complete: 57.3%; Average loss: 0.0925\n",
      "Iteration: 3450; Percent complete: 57.5%; Average loss: 0.0879\n",
      "Iteration: 3460; Percent complete: 57.7%; Average loss: 0.0926\n",
      "Iteration: 3470; Percent complete: 57.8%; Average loss: 0.0882\n",
      "Iteration: 3480; Percent complete: 58.0%; Average loss: 0.0874\n",
      "Iteration: 3490; Percent complete: 58.2%; Average loss: 0.0874\n",
      "Iteration: 3500; Percent complete: 58.3%; Average loss: 0.0827\n",
      "Iteration: 3510; Percent complete: 58.5%; Average loss: 0.0846\n",
      "Iteration: 3520; Percent complete: 58.7%; Average loss: 0.0868\n",
      "Iteration: 3530; Percent complete: 58.8%; Average loss: 0.0857\n",
      "Iteration: 3540; Percent complete: 59.0%; Average loss: 0.0862\n",
      "Iteration: 3550; Percent complete: 59.2%; Average loss: 0.0802\n",
      "Iteration: 3560; Percent complete: 59.3%; Average loss: 0.0810\n",
      "Iteration: 3570; Percent complete: 59.5%; Average loss: 0.0808\n",
      "Iteration: 3580; Percent complete: 59.7%; Average loss: 0.0780\n",
      "Iteration: 3590; Percent complete: 59.8%; Average loss: 0.0818\n",
      "Iteration: 3600; Percent complete: 60.0%; Average loss: 0.0753\n",
      "Iteration: 3610; Percent complete: 60.2%; Average loss: 0.0733\n",
      "Iteration: 3620; Percent complete: 60.3%; Average loss: 0.0806\n",
      "Iteration: 3630; Percent complete: 60.5%; Average loss: 0.0778\n",
      "Iteration: 3640; Percent complete: 60.7%; Average loss: 0.0770\n",
      "Iteration: 3650; Percent complete: 60.8%; Average loss: 0.0751\n",
      "Iteration: 3660; Percent complete: 61.0%; Average loss: 0.0746\n",
      "Iteration: 3670; Percent complete: 61.2%; Average loss: 0.0754\n",
      "Iteration: 3680; Percent complete: 61.3%; Average loss: 0.0764\n",
      "Iteration: 3690; Percent complete: 61.5%; Average loss: 0.0792\n",
      "Iteration: 3700; Percent complete: 61.7%; Average loss: 0.0793\n",
      "Iteration: 3710; Percent complete: 61.8%; Average loss: 0.0743\n",
      "Iteration: 3720; Percent complete: 62.0%; Average loss: 0.0732\n",
      "Iteration: 3730; Percent complete: 62.2%; Average loss: 0.0735\n",
      "Iteration: 3740; Percent complete: 62.3%; Average loss: 0.0712\n",
      "Iteration: 3750; Percent complete: 62.5%; Average loss: 0.0729\n",
      "Iteration: 3760; Percent complete: 62.7%; Average loss: 0.0686\n",
      "Iteration: 3770; Percent complete: 62.8%; Average loss: 0.0704\n",
      "Iteration: 3780; Percent complete: 63.0%; Average loss: 0.0712\n",
      "Iteration: 3790; Percent complete: 63.2%; Average loss: 0.0700\n",
      "Iteration: 3800; Percent complete: 63.3%; Average loss: 0.0684\n",
      "Iteration: 3810; Percent complete: 63.5%; Average loss: 0.0681\n",
      "Iteration: 3820; Percent complete: 63.7%; Average loss: 0.0667\n",
      "Iteration: 3830; Percent complete: 63.8%; Average loss: 0.0673\n",
      "Iteration: 3840; Percent complete: 64.0%; Average loss: 0.0669\n",
      "Iteration: 3850; Percent complete: 64.2%; Average loss: 0.0661\n",
      "Iteration: 3860; Percent complete: 64.3%; Average loss: 0.0632\n",
      "Iteration: 3870; Percent complete: 64.5%; Average loss: 0.0639\n",
      "Iteration: 3880; Percent complete: 64.7%; Average loss: 0.0631\n",
      "Iteration: 3890; Percent complete: 64.8%; Average loss: 0.0614\n",
      "Iteration: 3900; Percent complete: 65.0%; Average loss: 0.0621\n",
      "Iteration: 3910; Percent complete: 65.2%; Average loss: 0.0601\n",
      "Iteration: 3920; Percent complete: 65.3%; Average loss: 0.0601\n",
      "Iteration: 3930; Percent complete: 65.5%; Average loss: 0.0602\n",
      "Iteration: 3940; Percent complete: 65.7%; Average loss: 0.0614\n",
      "Iteration: 3950; Percent complete: 65.8%; Average loss: 0.0598\n",
      "Iteration: 3960; Percent complete: 66.0%; Average loss: 0.0594\n",
      "Iteration: 3970; Percent complete: 66.2%; Average loss: 0.0595\n",
      "Iteration: 3980; Percent complete: 66.3%; Average loss: 0.0580\n",
      "Iteration: 3990; Percent complete: 66.5%; Average loss: 0.0581\n",
      "Iteration: 4000; Percent complete: 66.7%; Average loss: 0.0554\n",
      "content/cb_model/Chat/2-4_512\n",
      "Iteration: 4010; Percent complete: 66.8%; Average loss: 0.0585\n",
      "Iteration: 4020; Percent complete: 67.0%; Average loss: 0.0569\n",
      "Iteration: 4030; Percent complete: 67.2%; Average loss: 0.0561\n",
      "Iteration: 4040; Percent complete: 67.3%; Average loss: 0.0558\n",
      "Iteration: 4050; Percent complete: 67.5%; Average loss: 0.0561\n",
      "Iteration: 4060; Percent complete: 67.7%; Average loss: 0.0558\n",
      "Iteration: 4070; Percent complete: 67.8%; Average loss: 0.0558\n",
      "Iteration: 4080; Percent complete: 68.0%; Average loss: 0.0527\n",
      "Iteration: 4090; Percent complete: 68.2%; Average loss: 0.0539\n",
      "Iteration: 4100; Percent complete: 68.3%; Average loss: 0.0554\n",
      "Iteration: 4110; Percent complete: 68.5%; Average loss: 0.0567\n",
      "Iteration: 4120; Percent complete: 68.7%; Average loss: 0.0602\n",
      "Iteration: 4130; Percent complete: 68.8%; Average loss: 0.0574\n",
      "Iteration: 4140; Percent complete: 69.0%; Average loss: 0.0611\n",
      "Iteration: 4150; Percent complete: 69.2%; Average loss: 0.0595\n",
      "Iteration: 4160; Percent complete: 69.3%; Average loss: 0.0588\n",
      "Iteration: 4170; Percent complete: 69.5%; Average loss: 0.0569\n",
      "Iteration: 4180; Percent complete: 69.7%; Average loss: 0.0546\n",
      "Iteration: 4190; Percent complete: 69.8%; Average loss: 0.0587\n",
      "Iteration: 4200; Percent complete: 70.0%; Average loss: 0.0553\n",
      "Iteration: 4210; Percent complete: 70.2%; Average loss: 0.0566\n",
      "Iteration: 4220; Percent complete: 70.3%; Average loss: 0.0610\n",
      "Iteration: 4230; Percent complete: 70.5%; Average loss: 0.0596\n",
      "Iteration: 4240; Percent complete: 70.7%; Average loss: 0.0597\n",
      "Iteration: 4250; Percent complete: 70.8%; Average loss: 0.0574\n",
      "Iteration: 4260; Percent complete: 71.0%; Average loss: 0.0554\n",
      "Iteration: 4270; Percent complete: 71.2%; Average loss: 0.0535\n",
      "Iteration: 4280; Percent complete: 71.3%; Average loss: 0.0532\n",
      "Iteration: 4290; Percent complete: 71.5%; Average loss: 0.0516\n",
      "Iteration: 4300; Percent complete: 71.7%; Average loss: 0.0566\n",
      "Iteration: 4310; Percent complete: 71.8%; Average loss: 0.0598\n",
      "Iteration: 4320; Percent complete: 72.0%; Average loss: 0.0672\n",
      "Iteration: 4330; Percent complete: 72.2%; Average loss: 0.0611\n",
      "Iteration: 4340; Percent complete: 72.3%; Average loss: 0.0575\n",
      "Iteration: 4350; Percent complete: 72.5%; Average loss: 0.0545\n",
      "Iteration: 4360; Percent complete: 72.7%; Average loss: 0.0562\n",
      "Iteration: 4370; Percent complete: 72.8%; Average loss: 0.0605\n",
      "Iteration: 4380; Percent complete: 73.0%; Average loss: 0.0548\n",
      "Iteration: 4390; Percent complete: 73.2%; Average loss: 0.0545\n",
      "Iteration: 4400; Percent complete: 73.3%; Average loss: 0.0528\n",
      "Iteration: 4410; Percent complete: 73.5%; Average loss: 0.0557\n",
      "Iteration: 4420; Percent complete: 73.7%; Average loss: 0.0495\n",
      "Iteration: 4430; Percent complete: 73.8%; Average loss: 0.0497\n",
      "Iteration: 4440; Percent complete: 74.0%; Average loss: 0.0480\n",
      "Iteration: 4450; Percent complete: 74.2%; Average loss: 0.0486\n",
      "Iteration: 4460; Percent complete: 74.3%; Average loss: 0.0454\n",
      "Iteration: 4470; Percent complete: 74.5%; Average loss: 0.0468\n",
      "Iteration: 4480; Percent complete: 74.7%; Average loss: 0.0473\n",
      "Iteration: 4490; Percent complete: 74.8%; Average loss: 0.0529\n",
      "Iteration: 4500; Percent complete: 75.0%; Average loss: 0.0594\n",
      "Iteration: 4510; Percent complete: 75.2%; Average loss: 0.0536\n",
      "Iteration: 4520; Percent complete: 75.3%; Average loss: 0.0516\n",
      "Iteration: 4530; Percent complete: 75.5%; Average loss: 0.0510\n",
      "Iteration: 4540; Percent complete: 75.7%; Average loss: 0.0508\n",
      "Iteration: 4550; Percent complete: 75.8%; Average loss: 0.0500\n",
      "Iteration: 4560; Percent complete: 76.0%; Average loss: 0.0510\n",
      "Iteration: 4570; Percent complete: 76.2%; Average loss: 0.0507\n",
      "Iteration: 4580; Percent complete: 76.3%; Average loss: 0.0509\n",
      "Iteration: 4590; Percent complete: 76.5%; Average loss: 0.0467\n",
      "Iteration: 4600; Percent complete: 76.7%; Average loss: 0.0451\n",
      "Iteration: 4610; Percent complete: 76.8%; Average loss: 0.0452\n",
      "Iteration: 4620; Percent complete: 77.0%; Average loss: 0.0440\n",
      "Iteration: 4630; Percent complete: 77.2%; Average loss: 0.0435\n",
      "Iteration: 4640; Percent complete: 77.3%; Average loss: 0.0428\n",
      "Iteration: 4650; Percent complete: 77.5%; Average loss: 0.0420\n",
      "Iteration: 4660; Percent complete: 77.7%; Average loss: 0.0419\n",
      "Iteration: 4670; Percent complete: 77.8%; Average loss: 0.0406\n",
      "Iteration: 4680; Percent complete: 78.0%; Average loss: 0.0430\n",
      "Iteration: 4690; Percent complete: 78.2%; Average loss: 0.0392\n",
      "Iteration: 4700; Percent complete: 78.3%; Average loss: 0.0391\n",
      "Iteration: 4710; Percent complete: 78.5%; Average loss: 0.0402\n",
      "Iteration: 4720; Percent complete: 78.7%; Average loss: 0.0404\n",
      "Iteration: 4730; Percent complete: 78.8%; Average loss: 0.0390\n",
      "Iteration: 4740; Percent complete: 79.0%; Average loss: 0.0390\n",
      "Iteration: 4750; Percent complete: 79.2%; Average loss: 0.0391\n",
      "Iteration: 4760; Percent complete: 79.3%; Average loss: 0.0385\n",
      "Iteration: 4770; Percent complete: 79.5%; Average loss: 0.0381\n",
      "Iteration: 4780; Percent complete: 79.7%; Average loss: 0.0394\n",
      "Iteration: 4790; Percent complete: 79.8%; Average loss: 0.0385\n",
      "Iteration: 4800; Percent complete: 80.0%; Average loss: 0.0357\n",
      "Iteration: 4810; Percent complete: 80.2%; Average loss: 0.0347\n",
      "Iteration: 4820; Percent complete: 80.3%; Average loss: 0.0363\n",
      "Iteration: 4830; Percent complete: 80.5%; Average loss: 0.0352\n",
      "Iteration: 4840; Percent complete: 80.7%; Average loss: 0.0337\n",
      "Iteration: 4850; Percent complete: 80.8%; Average loss: 0.0353\n",
      "Iteration: 4860; Percent complete: 81.0%; Average loss: 0.0362\n",
      "Iteration: 4870; Percent complete: 81.2%; Average loss: 0.0347\n",
      "Iteration: 4880; Percent complete: 81.3%; Average loss: 0.0369\n",
      "Iteration: 4890; Percent complete: 81.5%; Average loss: 0.0365\n",
      "Iteration: 4900; Percent complete: 81.7%; Average loss: 0.0359\n",
      "Iteration: 4910; Percent complete: 81.8%; Average loss: 0.0350\n",
      "Iteration: 4920; Percent complete: 82.0%; Average loss: 0.0360\n",
      "Iteration: 4930; Percent complete: 82.2%; Average loss: 0.0338\n",
      "Iteration: 4940; Percent complete: 82.3%; Average loss: 0.0353\n",
      "Iteration: 4950; Percent complete: 82.5%; Average loss: 0.0355\n",
      "Iteration: 4960; Percent complete: 82.7%; Average loss: 0.0392\n",
      "Iteration: 4970; Percent complete: 82.8%; Average loss: 0.0393\n",
      "Iteration: 4980; Percent complete: 83.0%; Average loss: 0.0377\n",
      "Iteration: 4990; Percent complete: 83.2%; Average loss: 0.0377\n",
      "Iteration: 5000; Percent complete: 83.3%; Average loss: 0.0340\n",
      "Iteration: 5010; Percent complete: 83.5%; Average loss: 0.0330\n",
      "Iteration: 5020; Percent complete: 83.7%; Average loss: 0.0367\n",
      "Iteration: 5030; Percent complete: 83.8%; Average loss: 0.0369\n",
      "Iteration: 5040; Percent complete: 84.0%; Average loss: 0.0375\n",
      "Iteration: 5050; Percent complete: 84.2%; Average loss: 0.0377\n",
      "Iteration: 5060; Percent complete: 84.3%; Average loss: 0.0344\n",
      "Iteration: 5070; Percent complete: 84.5%; Average loss: 0.0365\n",
      "Iteration: 5080; Percent complete: 84.7%; Average loss: 0.0341\n",
      "Iteration: 5090; Percent complete: 84.8%; Average loss: 0.0347\n",
      "Iteration: 5100; Percent complete: 85.0%; Average loss: 0.0344\n",
      "Iteration: 5110; Percent complete: 85.2%; Average loss: 0.0349\n",
      "Iteration: 5120; Percent complete: 85.3%; Average loss: 0.0358\n",
      "Iteration: 5130; Percent complete: 85.5%; Average loss: 0.0335\n",
      "Iteration: 5140; Percent complete: 85.7%; Average loss: 0.0342\n",
      "Iteration: 5150; Percent complete: 85.8%; Average loss: 0.0327\n",
      "Iteration: 5160; Percent complete: 86.0%; Average loss: 0.0324\n",
      "Iteration: 5170; Percent complete: 86.2%; Average loss: 0.0312\n",
      "Iteration: 5180; Percent complete: 86.3%; Average loss: 0.0297\n",
      "Iteration: 5190; Percent complete: 86.5%; Average loss: 0.0324\n",
      "Iteration: 5200; Percent complete: 86.7%; Average loss: 0.0301\n",
      "Iteration: 5210; Percent complete: 86.8%; Average loss: 0.0332\n",
      "Iteration: 5220; Percent complete: 87.0%; Average loss: 0.0329\n",
      "Iteration: 5230; Percent complete: 87.2%; Average loss: 0.0319\n",
      "Iteration: 5240; Percent complete: 87.3%; Average loss: 0.0315\n",
      "Iteration: 5250; Percent complete: 87.5%; Average loss: 0.0310\n",
      "Iteration: 5260; Percent complete: 87.7%; Average loss: 0.0294\n",
      "Iteration: 5270; Percent complete: 87.8%; Average loss: 0.0314\n",
      "Iteration: 5280; Percent complete: 88.0%; Average loss: 0.0312\n",
      "Iteration: 5290; Percent complete: 88.2%; Average loss: 0.0329\n",
      "Iteration: 5300; Percent complete: 88.3%; Average loss: 0.0319\n",
      "Iteration: 5310; Percent complete: 88.5%; Average loss: 0.0310\n",
      "Iteration: 5320; Percent complete: 88.7%; Average loss: 0.0311\n",
      "Iteration: 5330; Percent complete: 88.8%; Average loss: 0.0327\n",
      "Iteration: 5340; Percent complete: 89.0%; Average loss: 0.0337\n",
      "Iteration: 5350; Percent complete: 89.2%; Average loss: 0.0393\n",
      "Iteration: 5360; Percent complete: 89.3%; Average loss: 0.0405\n",
      "Iteration: 5370; Percent complete: 89.5%; Average loss: 0.0385\n",
      "Iteration: 5380; Percent complete: 89.7%; Average loss: 0.0406\n",
      "Iteration: 5390; Percent complete: 89.8%; Average loss: 0.0387\n",
      "Iteration: 5400; Percent complete: 90.0%; Average loss: 0.0445\n",
      "Iteration: 5410; Percent complete: 90.2%; Average loss: 0.0464\n",
      "Iteration: 5420; Percent complete: 90.3%; Average loss: 0.0460\n",
      "Iteration: 5430; Percent complete: 90.5%; Average loss: 0.0416\n",
      "Iteration: 5440; Percent complete: 90.7%; Average loss: 0.0414\n",
      "Iteration: 5450; Percent complete: 90.8%; Average loss: 0.0493\n",
      "Iteration: 5460; Percent complete: 91.0%; Average loss: 0.0440\n",
      "Iteration: 5470; Percent complete: 91.2%; Average loss: 0.0446\n",
      "Iteration: 5480; Percent complete: 91.3%; Average loss: 0.0392\n",
      "Iteration: 5490; Percent complete: 91.5%; Average loss: 0.0388\n",
      "Iteration: 5500; Percent complete: 91.7%; Average loss: 0.0396\n",
      "Iteration: 5510; Percent complete: 91.8%; Average loss: 0.0405\n",
      "Iteration: 5520; Percent complete: 92.0%; Average loss: 0.0394\n",
      "Iteration: 5530; Percent complete: 92.2%; Average loss: 0.0373\n",
      "Iteration: 5540; Percent complete: 92.3%; Average loss: 0.0381\n",
      "Iteration: 5550; Percent complete: 92.5%; Average loss: 0.0385\n",
      "Iteration: 5560; Percent complete: 92.7%; Average loss: 0.0441\n",
      "Iteration: 5570; Percent complete: 92.8%; Average loss: 0.0430\n",
      "Iteration: 5580; Percent complete: 93.0%; Average loss: 0.0424\n",
      "Iteration: 5590; Percent complete: 93.2%; Average loss: 0.0398\n",
      "Iteration: 5600; Percent complete: 93.3%; Average loss: 0.0382\n",
      "Iteration: 5610; Percent complete: 93.5%; Average loss: 0.0397\n",
      "Iteration: 5620; Percent complete: 93.7%; Average loss: 0.0366\n",
      "Iteration: 5630; Percent complete: 93.8%; Average loss: 0.0365\n",
      "Iteration: 5640; Percent complete: 94.0%; Average loss: 0.0341\n",
      "Iteration: 5650; Percent complete: 94.2%; Average loss: 0.0353\n",
      "Iteration: 5660; Percent complete: 94.3%; Average loss: 0.0342\n",
      "Iteration: 5670; Percent complete: 94.5%; Average loss: 0.0340\n",
      "Iteration: 5680; Percent complete: 94.7%; Average loss: 0.0358\n",
      "Iteration: 5690; Percent complete: 94.8%; Average loss: 0.0358\n",
      "Iteration: 5700; Percent complete: 95.0%; Average loss: 0.0328\n",
      "Iteration: 5710; Percent complete: 95.2%; Average loss: 0.0329\n",
      "Iteration: 5720; Percent complete: 95.3%; Average loss: 0.0313\n",
      "Iteration: 5730; Percent complete: 95.5%; Average loss: 0.0321\n",
      "Iteration: 5740; Percent complete: 95.7%; Average loss: 0.0310\n",
      "Iteration: 5750; Percent complete: 95.8%; Average loss: 0.0286\n",
      "Iteration: 5760; Percent complete: 96.0%; Average loss: 0.0276\n",
      "Iteration: 5770; Percent complete: 96.2%; Average loss: 0.0263\n",
      "Iteration: 5780; Percent complete: 96.3%; Average loss: 0.0284\n",
      "Iteration: 5790; Percent complete: 96.5%; Average loss: 0.0270\n",
      "Iteration: 5800; Percent complete: 96.7%; Average loss: 0.0261\n",
      "Iteration: 5810; Percent complete: 96.8%; Average loss: 0.0279\n",
      "Iteration: 5820; Percent complete: 97.0%; Average loss: 0.0267\n",
      "Iteration: 5830; Percent complete: 97.2%; Average loss: 0.0260\n",
      "Iteration: 5840; Percent complete: 97.3%; Average loss: 0.0267\n",
      "Iteration: 5850; Percent complete: 97.5%; Average loss: 0.0257\n",
      "Iteration: 5860; Percent complete: 97.7%; Average loss: 0.0259\n",
      "Iteration: 5870; Percent complete: 97.8%; Average loss: 0.0270\n",
      "Iteration: 5880; Percent complete: 98.0%; Average loss: 0.0272\n",
      "Iteration: 5890; Percent complete: 98.2%; Average loss: 0.0263\n",
      "Iteration: 5900; Percent complete: 98.3%; Average loss: 0.0256\n",
      "Iteration: 5910; Percent complete: 98.5%; Average loss: 0.0249\n",
      "Iteration: 5920; Percent complete: 98.7%; Average loss: 0.0253\n",
      "Iteration: 5930; Percent complete: 98.8%; Average loss: 0.0246\n",
      "Iteration: 5940; Percent complete: 99.0%; Average loss: 0.0257\n",
      "Iteration: 5950; Percent complete: 99.2%; Average loss: 0.0258\n",
      "Iteration: 5960; Percent complete: 99.3%; Average loss: 0.0237\n",
      "Iteration: 5970; Percent complete: 99.5%; Average loss: 0.0242\n",
      "Iteration: 5980; Percent complete: 99.7%; Average loss: 0.0253\n",
      "Iteration: 5990; Percent complete: 99.8%; Average loss: 0.0258\n",
      "Iteration: 6000; Percent complete: 100.0%; Average loss: 0.0269\n",
      "content/cb_model/Chat/2-4_512\n"
     ]
    }
   ],
   "source": [
    "save_dir = 'content/'\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 6000\n",
    "print_every = 10\n",
    "save_every = 2000\n",
    "loadFilename = None\n",
    "corpus_name=\"Chat\"\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "print(\"Starting Training!\")\n",
    "lossvalues = trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f51c0479588>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(lossvalues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bleu score Calculation** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.26053876229906775 0.17615635478300695\n",
      "Total Bleu Score for 1 grams on testing pairs:  0.2700490149407549\n",
      "Total Bleu Score for 2 grams on testing pairs:  0.15171761474908782\n"
     ]
    }
   ],
   "source": [
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "from nltk.translate.bleu_score import sentence_bleu,corpus_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "############################################################################\n",
    "# Difference between greedy search and beam search is here\n",
    "\n",
    "# greedy search\n",
    "# searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# beam search\n",
    "searcher = BeamSearchDecoder(encoder, decoder, voc, 10)\n",
    "############################################################################\n",
    "gram1_bleu_score = []\n",
    "gram2_bleu_score = []\n",
    "for i in range(0,len(testpairs),1):\n",
    "  \n",
    "  input_sentence = testpairs[i][0]\n",
    "  \n",
    "  reference = testpairs[i][1:]\n",
    "  templist = []\n",
    "  for k in range(len(reference)):\n",
    "    if(reference[k]!=''):\n",
    "      temp = reference[k].split(' ')\n",
    "      templist.append(temp)\n",
    "  \n",
    "  \n",
    "  input_sentence = normalizeString(input_sentence)\n",
    "  output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "  output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "  chencherry = SmoothingFunction()\n",
    "  score1 = sentence_bleu(templist,output_words,weights=(1, 0, 0, 0) ,smoothing_function=chencherry.method1)\n",
    "  score2 = sentence_bleu(templist,output_words,weights=(0.5, 0.5, 0, 0),smoothing_function=chencherry.method1) \n",
    "  gram1_bleu_score.append(score1)\n",
    "  gram2_bleu_score.append(score2)\n",
    "  if i%1000 == 0:\n",
    "    print(i,sum(gram1_bleu_score)/len(gram1_bleu_score),sum(gram2_bleu_score)/len(gram2_bleu_score))\n",
    "print(\"Total Bleu Score for 1 grams on testing pairs: \", sum(gram1_bleu_score)/len(gram1_bleu_score))  \n",
    "print(\"Total Bleu Score for 2 grams on testing pairs: \", sum(gram2_bleu_score)/len(gram2_bleu_score))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Chatting with BOT / Evaluation**\n",
    "Lets do chat with our newly trained chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-82a7ef414c95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msearcher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeamSearchDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mevaluateInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Me : I'm looking for a restaurant.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-ec18c2cf8348>\u001b[0m in \u001b[0;36mevaluateInput\u001b[0;34m(encoder, decoder, searcher, voc)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'q'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 860\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    861\u001b[0m         )\n\u001b[1;32m    862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    899\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 901\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "searcher = BeamSearchDecoder(encoder, decoder, voc, 10)\n",
    "evaluateInput(encoder, decoder, searcher, voc)\n",
    "\n",
    "# Me : I'm looking for a restaurant.\n",
    "# Bot: what area of town do you want ?\n",
    "# Me : I'm looking for a place to stay\n",
    "# Bot: we have options any preference in options\n",
    "# Me : I need a ride.\n",
    "# Bot: sure . i can help you with that ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab we learned about LSTM and all of its equations in action.\n",
    "\n",
    "Then we use LSTM cells in a model called seq2seq. The seq2seq model consists of encoder and decoder network both of which includes LSTM cells. The encoder encode sequence of words and embed them as a representation then the decoder takes the embedded representation and decode another sequence of words. We use this architecture to make a chat bot where it takes a sentence as input and output a response or answer.\n",
    "\n",
    "There are 2 search algorithm we can use in the decoder part. First is the greedy search which choose the word with maximum probability at every step but might not end up with the optimum sentence. Second is beam search which adds the k candidate to the greedy search.\n",
    "\n",
    "The first part we trained a chatbot from a movie dialogue corpus and for the homework I trained a chatbot with real a human-human conversations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference : Chat with our Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished importing...\n",
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 6389 sentence pairs\n",
      "Trimmed to 5998 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 2220\n",
      "keep_words 1535 / 2217 = 0.6924\n",
      "Trimmed from 5998 pairs to 5417, 0.9031 of total\n",
      "5000\n",
      "417\n",
      "Finished preprocessing...\n",
      "Finished setup...\n",
      "Evaluating...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  I am looking for a restaurant\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: what area of town could you like to dine at ? what area ? ! ! ! ! ! ! !\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Thank you\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: of course . is there a particular area of town you are interested in ? ? ! . more information ? do you have\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    885\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    810\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-962805d426b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;31m# Begin chatting (uncomment and run the following line to begin)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m \u001b[0mevaluateInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSOS_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEOS_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;31m# %%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/labs/RTML-gitlab/LAB11-LSTM/train_eval_loss.py\u001b[0m in \u001b[0;36mevaluateInput\u001b[0;34m(encoder, decoder, searcher, voc, device, SOS_token, MAX_LENGTH, EOS_token)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'q'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    857\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         )\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%%\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import pickle\n",
    "from torch.autograd import Variable\n",
    "from get_free_gpu import get_free_gpu\n",
    "\n",
    "import train_eval_loss\n",
    "from model import EncoderRNN, Attn, LuongAttnDecoderRNN, GreedySearchDecoder, Sentence, beam_decode, BeamSearchDecoder\n",
    "from lstm_cell import NaiveCustomLSTM\n",
    "import preprocess\n",
    "from preprocess import Voc\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "device = get_free_gpu()\n",
    "\n",
    "print(\"Finished importing...\")\n",
    "\n",
    "# global MAX_LENGTH, MIN_COUNT, PAD_token, SOS_token, EOS_token\n",
    "MAX_LENGTH = 30\n",
    "MIN_COUNT = 2 \n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "#%%\n",
    "from preprocess import loadPrepareData\n",
    "datafile = '../../data/multiwoz/multiwoz_all.txt'\n",
    "voc, pairs = loadPrepareData(datafile, MAX_LENGTH)\n",
    "\n",
    "# Print some pairs to validate\n",
    "# print(\"\\npairs:\")\n",
    "# for pair in pairs[:10]:\n",
    "#     print(pair)\n",
    "\n",
    "#%%\n",
    "# Trim vocabulary and pairs\n",
    "from preprocess import trimRareWords\n",
    "pairs_ = trimRareWords(voc, pairs, MIN_COUNT)\n",
    "\n",
    "testpairs = pairs_[5000:]\n",
    "pairs  = pairs_[:5000]\n",
    "\n",
    "print(len(pairs))\n",
    "print(len(testpairs))\n",
    "\n",
    "#%%\n",
    "from preprocess import batch2TrainData\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)], PAD_token)\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "#%%\n",
    "# pair_batch = pairs[:5]\n",
    "# print(pair_batch)\n",
    "# pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "# print(pair_batch)\n",
    "# print(target_variable)\n",
    "# print(mask)\n",
    "# print(max_target_len)\n",
    "\n",
    "print(\"Finished preprocessing...\")\n",
    "\n",
    "#%%\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "\n",
    "hidden_size = 512\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 4\n",
    "dropout = 0.5\n",
    "batch_size = 256 \n",
    "loadFilename = None\n",
    "\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "\n",
    "saved_dict = torch.load('content/cb_model/MultiWOZ/en2-de4_greedy/6000_checkpoint.tar')\n",
    "# saved_dict = torch.load('content/cb_model/MultiWOZ/en2-de4_beam/6000_checkpoint.tar')\n",
    "\n",
    "embedding.load_state_dict(saved_dict['embedding'])\n",
    "encoder.load_state_dict(saved_dict['en'])\n",
    "decoder.load_state_dict(saved_dict['de'])\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "#%%\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "\n",
    "encoder_optimizer.load_state_dict(saved_dict['en_opt'])\n",
    "decoder_optimizer.load_state_dict(saved_dict['de_opt'])\n",
    "\n",
    "print(\"Finished setup...\")\n",
    "#%%\n",
    "\n",
    "from train_eval_loss import evaluateInput\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder, device)\n",
    "# searcher = BeamSearchDecoder(encoder, decoder, voc, device, 10)\n",
    "print(\"Evaluating...\")\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, voc, device, SOS_token, MAX_LENGTH, EOS_token)\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
