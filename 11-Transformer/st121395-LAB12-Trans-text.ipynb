{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 12: Transformers, Speech Recognition\n",
    "# Chanapa Pananookooln | st121395\n",
    "\n",
    "## Lab Summary\n",
    "\n",
    "In this lab we learn about the Transformer implementation from pytorch tutorial and how to adapt it to the speech recognition task.\n",
    "Multiple layers of TransformerEncoderLayer are combined as TransformerEncoder. It will take the input and the input will be masked such that the model can only consider the earlier positions.\n",
    "\n",
    "There is also PositionalEncoding that contain the positions of the tokens in the sequence it will be the same size as the embedding so they can be summed. In the Transformer paper they used fuctions of sine and cos to encode the positions.\n",
    "\n",
    "We also used torchtext module to help the processing of text data and allows mroe efficient batch processing. \n",
    "\n",
    "To work with time sequence im pytorch the required tensor dimension for both inputs and targets is (time x batch_element)\n",
    "\n",
    "To initialize the model we hace to define these parameters :\n",
    "\n",
    "    ntokens # the size of vocabulary\n",
    "    emsize  # embedding dimension\n",
    "    nhid  the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "    nlayers # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "    nhead # the number of heads in the multiheadattention models\n",
    "    dropout  # the dropout value\n",
    "\n",
    "And our model is TransformerModel\n",
    "model = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout)\n",
    "\n",
    "Another thing to take note is that in language modling tasks, the negative log likelihood is called Perplexity or PPL.\n",
    "\n",
    "\n",
    "The speech recognition task's input is the audio and the output in this case is a probability matrix of characters which will be decoded to the most likely sequence of characer spoken in the audio.\n",
    " \n",
    "Firstly we need the torchaudio module to process the input audio.\n",
    "we use the LibriSpeech dataset where each sample contains waveform, sample_rate, utterance, speaker_id, chapter_id and utterance_id.\n",
    "\n",
    "Next we need to do some data augmentations to help increase the diversity of our dataset adn increase the dataset size. In this lab we chose to use the Spectrogram Augmentation.\n",
    "\n",
    "We also need a TextTransform functions to transform each character to integers and vice versa\n",
    "\n",
    "Then we create a data_processing function for transoforming the input audios and labels into spectrogram and sequence of labels which would then be feed to the model for training.\n",
    "\n",
    "The tutorial in this lab used Residual Convolutional Neural Networks (ResCNN) and Bidirectional Recurrent Neural Networks (BiRNN) as components in the speech recognition model.\n",
    "\n",
    "There are also many evaluation matrix for the speech recognition task such as Levenshtein distance in the word-level and character-level, word error rate (WER) and Character Error rate (CER)\n",
    "\n",
    "For the optimizer, AdamW is introduced to fix the weight decay problem of Adam. which results in faster convergence.\n",
    "\n",
    "And CTC Loss functions allow the blank label to allow the model to say that in that audio frame no character was said."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART I : Transformer With Torchtext @ Language Modeling Task on WikiText2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from IPython.display import display\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout = 0.1, max_len = 5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        \n",
    "        encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_iter :  WikiText2\n",
      "train_data after process tensor([  10, 3850, 3870,  ..., 2443, 4811,    4])\n",
      "train_data.shape after process torch.Size([2049990])\n",
      "train_data after batchify tensor([[   10,    60,   565,  ..., 11653,  2436,     2],\n",
      "        [ 3850,    13,   301,  ...,    48,    31,  1991],\n",
      "        [ 3870,   316,    20,  ...,    98,  7721,     5],\n",
      "        ...,\n",
      "        [  588,  4012,    60,  ...,     2,  1440, 12314],\n",
      "        [ 4988,    30,     5,  ...,  3166, 17107,  2061],\n",
      "        [    7,     9,     2,  ...,    63,    19,     3]], device='cuda:0')\n",
      "test_data.shape after batchify torch.Size([102499, 20])\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import torch\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "train_iter = WikiText2(split='train')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "counter = Counter()\n",
    "for line in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter)\n",
    "\n",
    "def data_process(raw_text_iter):\n",
    "  data = [torch.tensor([vocab[token] for token in tokenizer(item)],\n",
    "                       dtype=torch.long) for item in raw_text_iter]\n",
    "  return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "\n",
    "train_iter, val_iter, test_iter = WikiText2()\n",
    "print(\"train_iter : \", train_iter)\n",
    "\n",
    "train_data = data_process(train_iter)\n",
    "val_data = data_process(val_iter)\n",
    "test_data = data_process(test_iter)\n",
    "print('train_data after process', train_data)\n",
    "print('train_data.shape after process', train_data.shape)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 20\n",
    "eval_batch_size = 10\n",
    "\n",
    "train_data = batchify(train_data, batch_size)\n",
    "val_data = batchify(val_data, eval_batch_size)\n",
    "test_data = batchify(test_data, eval_batch_size)\n",
    "print('train_data after batchify', train_data)\n",
    "print('test_data.shape after batchify', train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate input and target sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data    = source[i   : i+seq_len]\n",
    "    target  = source[i+1 : i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(vocab.stoi) # the size of vocabulary\n",
    "\n",
    "emsize  = 200 # embedding dimension\n",
    "nhid    = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2   # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead   = 2   # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "\n",
    "model   = TransformerModel(ntokens, emsize, nhead, nhid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
    "        \n",
    "        data, targets = get_batch(train_data, i)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if data.size(0) != bptt:\n",
    "            src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            \n",
    "        output = model(data, src_mask)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        log_interval = 200\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(train_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    src_mask = model.generate_square_subsequent_mask(bptt).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            if data.size(0) != bptt:\n",
    "                src_mask = model.generate_square_subsequent_mask(data.size(0)).to(device)\n",
    "            output = eval_model(data, src_mask)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1), output_flat, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   200/ 2928 batches | lr 5.00 | ms/batch 10.20 | loss  5.67 | ppl   289.32\n",
      "| epoch   1 |   400/ 2928 batches | lr 5.00 | ms/batch  9.87 | loss  5.67 | ppl   288.72\n",
      "| epoch   1 |   600/ 2928 batches | lr 5.00 | ms/batch  9.86 | loss  5.44 | ppl   230.48\n",
      "| epoch   1 |   800/ 2928 batches | lr 5.00 | ms/batch  9.86 | loss  5.49 | ppl   243.01\n",
      "| epoch   1 |  1000/ 2928 batches | lr 5.00 | ms/batch  9.86 | loss  5.44 | ppl   230.74\n",
      "| epoch   1 |  1200/ 2928 batches | lr 5.00 | ms/batch  9.85 | loss  5.48 | ppl   239.40\n",
      "| epoch   1 |  1400/ 2928 batches | lr 5.00 | ms/batch  9.88 | loss  5.49 | ppl   242.60\n",
      "| epoch   1 |  1600/ 2928 batches | lr 5.00 | ms/batch  9.86 | loss  5.51 | ppl   248.02\n",
      "| epoch   1 |  1800/ 2928 batches | lr 5.00 | ms/batch  9.86 | loss  5.46 | ppl   234.58\n",
      "| epoch   1 |  2000/ 2928 batches | lr 5.00 | ms/batch  9.86 | loss  5.47 | ppl   237.37\n",
      "| epoch   1 |  2200/ 2928 batches | lr 5.00 | ms/batch  9.88 | loss  5.33 | ppl   207.05\n",
      "| epoch   1 |  2400/ 2928 batches | lr 5.00 | ms/batch  9.89 | loss  5.45 | ppl   233.22\n",
      "| epoch   1 |  2600/ 2928 batches | lr 5.00 | ms/batch  9.89 | loss  5.45 | ppl   233.62\n",
      "| epoch   1 |  2800/ 2928 batches | lr 5.00 | ms/batch  9.90 | loss  5.39 | ppl   218.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 34.70s | valid loss  5.59 | valid ppl   268.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |   200/ 2928 batches | lr 4.51 | ms/batch  9.95 | loss  5.35 | ppl   210.63\n",
      "| epoch   2 |   400/ 2928 batches | lr 4.51 | ms/batch  9.90 | loss  5.37 | ppl   213.90\n",
      "| epoch   2 |   600/ 2928 batches | lr 4.51 | ms/batch  9.92 | loss  5.16 | ppl   173.49\n",
      "| epoch   2 |   800/ 2928 batches | lr 4.51 | ms/batch  9.92 | loss  5.23 | ppl   186.51\n",
      "| epoch   2 |  1000/ 2928 batches | lr 4.51 | ms/batch  9.94 | loss  5.18 | ppl   177.37\n",
      "| epoch   2 |  1200/ 2928 batches | lr 4.51 | ms/batch  9.93 | loss  5.22 | ppl   185.41\n",
      "| epoch   2 |  1400/ 2928 batches | lr 4.51 | ms/batch  9.91 | loss  5.24 | ppl   188.55\n",
      "| epoch   2 |  1600/ 2928 batches | lr 4.51 | ms/batch  9.92 | loss  5.27 | ppl   194.80\n",
      "| epoch   2 |  1800/ 2928 batches | lr 4.51 | ms/batch  9.91 | loss  5.22 | ppl   184.88\n",
      "| epoch   2 |  2000/ 2928 batches | lr 4.51 | ms/batch  9.94 | loss  5.23 | ppl   186.22\n",
      "| epoch   2 |  2200/ 2928 batches | lr 4.51 | ms/batch  9.92 | loss  5.09 | ppl   162.32\n",
      "| epoch   2 |  2400/ 2928 batches | lr 4.51 | ms/batch  9.93 | loss  5.22 | ppl   185.32\n",
      "| epoch   2 |  2600/ 2928 batches | lr 4.51 | ms/batch  9.97 | loss  5.23 | ppl   186.09\n",
      "| epoch   2 |  2800/ 2928 batches | lr 4.51 | ms/batch 10.03 | loss  5.16 | ppl   174.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 34.79s | valid loss  5.57 | valid ppl   262.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |   200/ 2928 batches | lr 4.29 | ms/batch  9.98 | loss  5.15 | ppl   171.83\n",
      "| epoch   3 |   400/ 2928 batches | lr 4.29 | ms/batch  9.93 | loss  5.16 | ppl   174.87\n",
      "| epoch   3 |   600/ 2928 batches | lr 4.29 | ms/batch  9.93 | loss  4.96 | ppl   143.05\n",
      "| epoch   3 |   800/ 2928 batches | lr 4.29 | ms/batch  9.92 | loss  5.03 | ppl   152.58\n",
      "| epoch   3 |  1000/ 2928 batches | lr 4.29 | ms/batch  9.95 | loss  4.99 | ppl   146.83\n",
      "| epoch   3 |  1200/ 2928 batches | lr 4.29 | ms/batch  9.95 | loss  5.03 | ppl   153.54\n",
      "| epoch   3 |  1400/ 2928 batches | lr 4.29 | ms/batch  9.95 | loss  5.05 | ppl   156.79\n",
      "| epoch   3 |  1600/ 2928 batches | lr 4.29 | ms/batch  9.92 | loss  5.09 | ppl   162.56\n",
      "| epoch   3 |  1800/ 2928 batches | lr 4.29 | ms/batch  9.93 | loss  5.05 | ppl   155.59\n",
      "| epoch   3 |  2000/ 2928 batches | lr 4.29 | ms/batch  9.94 | loss  5.05 | ppl   156.77\n",
      "| epoch   3 |  2200/ 2928 batches | lr 4.29 | ms/batch  9.93 | loss  4.92 | ppl   136.77\n",
      "| epoch   3 |  2400/ 2928 batches | lr 4.29 | ms/batch  9.94 | loss  5.05 | ppl   155.79\n",
      "| epoch   3 |  2600/ 2928 batches | lr 4.29 | ms/batch  9.93 | loss  5.05 | ppl   155.59\n",
      "| epoch   3 |  2800/ 2928 batches | lr 4.29 | ms/batch  9.94 | loss  4.99 | ppl   146.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 34.83s | valid loss  5.56 | valid ppl   259.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   4 |   200/ 2928 batches | lr 4.07 | ms/batch 10.01 | loss  4.99 | ppl   146.77\n",
      "| epoch   4 |   400/ 2928 batches | lr 4.07 | ms/batch  9.96 | loss  5.00 | ppl   148.48\n",
      "| epoch   4 |   600/ 2928 batches | lr 4.07 | ms/batch  9.95 | loss  4.81 | ppl   122.68\n",
      "| epoch   4 |   800/ 2928 batches | lr 4.07 | ms/batch  9.94 | loss  4.87 | ppl   130.79\n",
      "| epoch   4 |  1000/ 2928 batches | lr 4.07 | ms/batch  9.92 | loss  4.85 | ppl   127.67\n",
      "| epoch   4 |  1200/ 2928 batches | lr 4.07 | ms/batch  9.95 | loss  4.90 | ppl   133.88\n",
      "| epoch   4 |  1400/ 2928 batches | lr 4.07 | ms/batch  9.98 | loss  4.91 | ppl   135.54\n",
      "| epoch   4 |  1600/ 2928 batches | lr 4.07 | ms/batch  9.96 | loss  4.95 | ppl   140.62\n",
      "| epoch   4 |  1800/ 2928 batches | lr 4.07 | ms/batch  9.95 | loss  4.91 | ppl   135.32\n",
      "| epoch   4 |  2000/ 2928 batches | lr 4.07 | ms/batch  9.95 | loss  4.91 | ppl   135.37\n",
      "| epoch   4 |  2200/ 2928 batches | lr 4.07 | ms/batch  9.98 | loss  4.77 | ppl   118.13\n",
      "| epoch   4 |  2400/ 2928 batches | lr 4.07 | ms/batch  9.98 | loss  4.90 | ppl   134.67\n",
      "| epoch   4 |  2600/ 2928 batches | lr 4.07 | ms/batch 10.01 | loss  4.91 | ppl   136.03\n",
      "| epoch   4 |  2800/ 2928 batches | lr 4.07 | ms/batch  9.97 | loss  4.85 | ppl   127.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 34.88s | valid loss  5.56 | valid ppl   259.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   5 |   200/ 2928 batches | lr 3.87 | ms/batch 10.01 | loss  4.86 | ppl   129.21\n",
      "| epoch   5 |   400/ 2928 batches | lr 3.87 | ms/batch  9.96 | loss  4.88 | ppl   131.33\n",
      "| epoch   5 |   600/ 2928 batches | lr 3.87 | ms/batch  9.96 | loss  4.68 | ppl   108.07\n",
      "| epoch   5 |   800/ 2928 batches | lr 3.87 | ms/batch  9.97 | loss  4.75 | ppl   115.13\n",
      "| epoch   5 |  1000/ 2928 batches | lr 3.87 | ms/batch  9.96 | loss  4.73 | ppl   112.95\n",
      "| epoch   5 |  1200/ 2928 batches | lr 3.87 | ms/batch  9.95 | loss  4.77 | ppl   118.28\n",
      "| epoch   5 |  1400/ 2928 batches | lr 3.87 | ms/batch  9.94 | loss  4.78 | ppl   119.44\n",
      "| epoch   5 |  1600/ 2928 batches | lr 3.87 | ms/batch  9.95 | loss  4.82 | ppl   124.54\n",
      "| epoch   5 |  1800/ 2928 batches | lr 3.87 | ms/batch  9.95 | loss  4.79 | ppl   120.45\n",
      "| epoch   5 |  2000/ 2928 batches | lr 3.87 | ms/batch  9.96 | loss  4.79 | ppl   120.88\n",
      "| epoch   5 |  2200/ 2928 batches | lr 3.87 | ms/batch  9.96 | loss  4.66 | ppl   105.23\n",
      "| epoch   5 |  2400/ 2928 batches | lr 3.87 | ms/batch 10.00 | loss  4.78 | ppl   119.49\n",
      "| epoch   5 |  2600/ 2928 batches | lr 3.87 | ms/batch  9.98 | loss  4.79 | ppl   120.31\n",
      "| epoch   5 |  2800/ 2928 batches | lr 3.87 | ms/batch  9.99 | loss  4.74 | ppl   114.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 34.90s | valid loss  5.61 | valid ppl   274.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   6 |   200/ 2928 batches | lr 3.68 | ms/batch 10.02 | loss  4.75 | ppl   115.50\n",
      "| epoch   6 |   400/ 2928 batches | lr 3.68 | ms/batch  9.96 | loss  4.76 | ppl   116.76\n",
      "| epoch   6 |   600/ 2928 batches | lr 3.68 | ms/batch  9.95 | loss  4.58 | ppl    97.88\n",
      "| epoch   6 |   800/ 2928 batches | lr 3.68 | ms/batch  9.95 | loss  4.64 | ppl   103.16\n",
      "| epoch   6 |  1000/ 2928 batches | lr 3.68 | ms/batch  9.96 | loss  4.63 | ppl   102.38\n",
      "| epoch   6 |  1200/ 2928 batches | lr 3.68 | ms/batch  9.96 | loss  4.67 | ppl   106.99\n",
      "| epoch   6 |  1400/ 2928 batches | lr 3.68 | ms/batch  9.96 | loss  4.68 | ppl   108.25\n",
      "| epoch   6 |  1600/ 2928 batches | lr 3.68 | ms/batch  9.97 | loss  4.72 | ppl   111.92\n",
      "| epoch   6 |  1800/ 2928 batches | lr 3.68 | ms/batch  9.96 | loss  4.69 | ppl   108.82\n",
      "| epoch   6 |  2000/ 2928 batches | lr 3.68 | ms/batch  9.98 | loss  4.69 | ppl   108.68\n",
      "| epoch   6 |  2200/ 2928 batches | lr 3.68 | ms/batch  9.99 | loss  4.56 | ppl    95.15\n",
      "| epoch   6 |  2400/ 2928 batches | lr 3.68 | ms/batch  9.96 | loss  4.67 | ppl   106.69\n",
      "| epoch   6 |  2600/ 2928 batches | lr 3.68 | ms/batch  9.96 | loss  4.69 | ppl   108.98\n",
      "| epoch   6 |  2800/ 2928 batches | lr 3.68 | ms/batch  9.96 | loss  4.63 | ppl   102.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 34.88s | valid loss  5.67 | valid ppl   289.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   7 |   200/ 2928 batches | lr 3.49 | ms/batch 10.03 | loss  4.65 | ppl   104.76\n",
      "| epoch   7 |   400/ 2928 batches | lr 3.49 | ms/batch  9.97 | loss  4.66 | ppl   105.68\n",
      "| epoch   7 |   600/ 2928 batches | lr 3.49 | ms/batch  9.96 | loss  4.49 | ppl    88.69\n",
      "| epoch   7 |   800/ 2928 batches | lr 3.49 | ms/batch  9.96 | loss  4.54 | ppl    93.99\n",
      "| epoch   7 |  1000/ 2928 batches | lr 3.49 | ms/batch  9.98 | loss  4.54 | ppl    93.25\n",
      "| epoch   7 |  1200/ 2928 batches | lr 3.49 | ms/batch  9.96 | loss  4.58 | ppl    97.12\n",
      "| epoch   7 |  1400/ 2928 batches | lr 3.49 | ms/batch 10.01 | loss  4.59 | ppl    98.15\n",
      "| epoch   7 |  1600/ 2928 batches | lr 3.49 | ms/batch 10.00 | loss  4.63 | ppl   102.91\n",
      "| epoch   7 |  1800/ 2928 batches | lr 3.49 | ms/batch 10.00 | loss  4.60 | ppl    99.20\n",
      "| epoch   7 |  2000/ 2928 batches | lr 3.49 | ms/batch  9.97 | loss  4.60 | ppl    99.14\n",
      "| epoch   7 |  2200/ 2928 batches | lr 3.49 | ms/batch  9.96 | loss  4.46 | ppl    86.89\n",
      "| epoch   7 |  2400/ 2928 batches | lr 3.49 | ms/batch  9.95 | loss  4.57 | ppl    96.80\n",
      "| epoch   7 |  2600/ 2928 batches | lr 3.49 | ms/batch  9.97 | loss  4.60 | ppl    99.06\n",
      "| epoch   7 |  2800/ 2928 batches | lr 3.49 | ms/batch  9.99 | loss  4.54 | ppl    93.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 34.94s | valid loss  5.69 | valid ppl   295.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   8 |   200/ 2928 batches | lr 3.32 | ms/batch 10.03 | loss  4.57 | ppl    96.10\n",
      "| epoch   8 |   400/ 2928 batches | lr 3.32 | ms/batch  9.96 | loss  4.57 | ppl    96.82\n",
      "| epoch   8 |   600/ 2928 batches | lr 3.32 | ms/batch  9.97 | loss  4.41 | ppl    82.16\n",
      "| epoch   8 |   800/ 2928 batches | lr 3.32 | ms/batch  9.95 | loss  4.46 | ppl    86.15\n",
      "| epoch   8 |  1000/ 2928 batches | lr 3.32 | ms/batch  9.98 | loss  4.45 | ppl    86.05\n",
      "| epoch   8 |  1200/ 2928 batches | lr 3.32 | ms/batch  9.97 | loss  4.49 | ppl    89.57\n",
      "| epoch   8 |  1400/ 2928 batches | lr 3.32 | ms/batch  9.98 | loss  4.50 | ppl    90.37\n",
      "| epoch   8 |  1600/ 2928 batches | lr 3.32 | ms/batch  9.96 | loss  4.55 | ppl    94.83\n",
      "| epoch   8 |  1800/ 2928 batches | lr 3.32 | ms/batch 10.01 | loss  4.51 | ppl    90.94\n",
      "| epoch   8 |  2000/ 2928 batches | lr 3.32 | ms/batch 10.10 | loss  4.52 | ppl    91.70\n",
      "| epoch   8 |  2200/ 2928 batches | lr 3.32 | ms/batch  9.98 | loss  4.38 | ppl    80.21\n",
      "| epoch   8 |  2400/ 2928 batches | lr 3.32 | ms/batch  9.95 | loss  4.49 | ppl    88.73\n",
      "| epoch   8 |  2600/ 2928 batches | lr 3.32 | ms/batch  9.99 | loss  4.51 | ppl    90.65\n",
      "| epoch   8 |  2800/ 2928 batches | lr 3.32 | ms/batch  9.96 | loss  4.46 | ppl    86.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 34.93s | valid loss  5.71 | valid ppl   302.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   9 |   200/ 2928 batches | lr 3.15 | ms/batch 10.02 | loss  4.48 | ppl    88.20\n",
      "| epoch   9 |   400/ 2928 batches | lr 3.15 | ms/batch  9.97 | loss  4.49 | ppl    89.28\n",
      "| epoch   9 |   600/ 2928 batches | lr 3.15 | ms/batch  9.96 | loss  4.33 | ppl    76.29\n",
      "| epoch   9 |   800/ 2928 batches | lr 3.15 | ms/batch  9.97 | loss  4.37 | ppl    79.36\n",
      "| epoch   9 |  1000/ 2928 batches | lr 3.15 | ms/batch  9.96 | loss  4.38 | ppl    80.17\n",
      "| epoch   9 |  1200/ 2928 batches | lr 3.15 | ms/batch  9.99 | loss  4.42 | ppl    82.69\n",
      "| epoch   9 |  1400/ 2928 batches | lr 3.15 | ms/batch  9.99 | loss  4.42 | ppl    83.26\n",
      "| epoch   9 |  1600/ 2928 batches | lr 3.15 | ms/batch  9.96 | loss  4.47 | ppl    87.31\n",
      "| epoch   9 |  1800/ 2928 batches | lr 3.15 | ms/batch  9.99 | loss  4.43 | ppl    84.35\n",
      "| epoch   9 |  2000/ 2928 batches | lr 3.15 | ms/batch  9.96 | loss  4.44 | ppl    85.02\n",
      "| epoch   9 |  2200/ 2928 batches | lr 3.15 | ms/batch  9.98 | loss  4.31 | ppl    74.41\n",
      "| epoch   9 |  2400/ 2928 batches | lr 3.15 | ms/batch 10.00 | loss  4.41 | ppl    82.45\n",
      "| epoch   9 |  2600/ 2928 batches | lr 3.15 | ms/batch  9.96 | loss  4.43 | ppl    84.15\n",
      "| epoch   9 |  2800/ 2928 batches | lr 3.15 | ms/batch  9.97 | loss  4.38 | ppl    80.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 34.93s | valid loss  5.72 | valid ppl   304.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  10 |   200/ 2928 batches | lr 2.99 | ms/batch 10.00 | loss  4.41 | ppl    82.41\n",
      "| epoch  10 |   400/ 2928 batches | lr 2.99 | ms/batch  9.95 | loss  4.42 | ppl    82.77\n",
      "| epoch  10 |   600/ 2928 batches | lr 2.99 | ms/batch  9.98 | loss  4.26 | ppl    70.65\n",
      "| epoch  10 |   800/ 2928 batches | lr 2.99 | ms/batch 10.01 | loss  4.30 | ppl    73.87\n",
      "| epoch  10 |  1000/ 2928 batches | lr 2.99 | ms/batch  9.99 | loss  4.31 | ppl    74.77\n",
      "| epoch  10 |  1200/ 2928 batches | lr 2.99 | ms/batch  9.98 | loss  4.35 | ppl    77.37\n",
      "| epoch  10 |  1400/ 2928 batches | lr 2.99 | ms/batch  9.97 | loss  4.35 | ppl    77.11\n",
      "| epoch  10 |  1600/ 2928 batches | lr 2.99 | ms/batch  9.98 | loss  4.39 | ppl    80.76\n",
      "| epoch  10 |  1800/ 2928 batches | lr 2.99 | ms/batch 10.01 | loss  4.37 | ppl    78.66\n",
      "| epoch  10 |  2000/ 2928 batches | lr 2.99 | ms/batch  9.99 | loss  4.37 | ppl    78.84\n",
      "| epoch  10 |  2200/ 2928 batches | lr 2.99 | ms/batch  9.97 | loss  4.23 | ppl    68.99\n",
      "| epoch  10 |  2400/ 2928 batches | lr 2.99 | ms/batch  9.96 | loss  4.34 | ppl    76.36\n",
      "| epoch  10 |  2600/ 2928 batches | lr 2.99 | ms/batch  9.97 | loss  4.36 | ppl    78.47\n",
      "| epoch  10 |  2800/ 2928 batches | lr 2.99 | ms/batch 10.01 | loss  4.31 | ppl    74.30\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 34.94s | valid loss  5.76 | valid ppl   317.03\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs        = 10 # number of epochs\n",
    "best_model    = model\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    torch.save(model.state_dict(), f'../../weights/transformer_with_torchtext/transformer_torch_text{epoch}.pth')\n",
    "\n",
    "    val_loss, output_flat, targets = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test loss  6.76 | test ppl   865.01\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "test_loss, output_flat, targets = evaluate(best_model, test_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    test_loss, math.exp(test_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on my own TEXT data on this Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We’ve', 'come', 'a', 'long', 'way', 'since', 'our', 'early', 'days', 'in', 'Älmhult,', 'Sweden,', 'but', 'IKEA', 'founder', 'Ingvar', 'Kamprad’s', 'dream', 'to', 'create', 'a', 'better', 'life', 'for', 'as', 'many', 'people', 'as', 'possible', '–', 'whatever', 'the', 'size', 'of', 'their', 'wallet', '–', 'is', 'and', 'will', 'always', 'be', 'our', 'driving', 'force.']\n",
      "tensor([   0,  741,    9,  162,  263,  153,  856,  115,  335,    7,    0,    3,\n",
      "        4450,    3,   39,    0, 3694,    0,    0, 2910,    8, 1057,    9,  876,\n",
      "         167,   18,   15,  102,  151,   15,  688,   41, 4719,    2,  862,    5,\n",
      "          37,    0,   41,   24,    6,  198,  949,   34,  856, 3075,  345,    4])\n",
      "torch.Size([4, 10])\n",
      "=========================================================================================\n",
      "| End of training | test loss  6.54 | test ppl   691.31\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../../weights/transformer_with_torchtext/transformer_torch_text49.pth'))\n",
    "\n",
    "my_text = 'We’ve come a long way since our early days in Älmhult, Sweden, but IKEA founder Ingvar Kamprad’s dream to create a better life for as many people as possible – whatever the size of their wallet – is and will always be our driving force.'\n",
    "\n",
    "my_text_split = my_text.split(' ')\n",
    "print(my_text_split)\n",
    "\n",
    "my_data_processed = data_process(my_text_split)\n",
    "print(my_data_processed)\n",
    "\n",
    "my_ready_data = batchify(my_data_processed, eval_batch_size)\n",
    "print(my_ready_data.shape)\n",
    "\n",
    "my_loss, output_flat, targets = evaluate(model, my_ready_data)\n",
    "print('=' * 89)\n",
    "print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n",
    "    my_loss, math.exp(my_loss)))\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "I was able to train this model for 3 epochs.\n",
    "\n",
    "After testing this model on my own text sample. The test loss and test ppl are very similar to the test loss that I got from the training set of the WikiText2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
