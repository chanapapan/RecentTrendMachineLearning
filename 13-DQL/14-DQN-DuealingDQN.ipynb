{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 14: Dueling DQN\n",
    "\n",
    "- Double & Dueling DQN using 2 DDQN\n",
    "    \n",
    "    for Atari image RGB + train_DoubleDQN + play_game_CNN + get_state2 + compute_td_loss_DoubleDQN\n",
    "        - SpaceInvaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "from tqdm import trange\n",
    "\n",
    "# Select GPU or CPU as device\n",
    "\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Epsilon annealing schedule generator\n",
    "\n",
    "def gen_eps_by_episode(epsilon_start, epsilon_final, epsilon_decay):\n",
    "    eps_by_episode = lambda episode: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * episode / epsilon_decay)\n",
    "    return eps_by_episode\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "eps_by_episode = gen_eps_by_episode(epsilon_start, epsilon_final, epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # Add batch index dimension to state representations\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(episode, rewards, losses):\n",
    "    # clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('episode %s. reward: %s' % (episode, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)   \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from PIL import Image\n",
    "image_size = 84\n",
    "\n",
    "\n",
    "transform = T.Compose([T.ToPILImage(),\n",
    "                       T.Resize((image_size, image_size), interpolation=Image.CUBIC),\n",
    "                       T.ToTensor()])\n",
    "\n",
    "# Convert to RGB image (3 channels)\n",
    "\n",
    "def get_state2(observation):\n",
    "    state = observation.transpose((2,0,1))\n",
    "    state = torch.from_numpy(state)\n",
    "    state = transform(state)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the CNN DQN\n",
    "\n",
    "We have the following characteristics:\n",
    "- Input channels: 3 (RGB channels)\n",
    "- Ouput: all possible actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDQN(nn.Module):\n",
    "    def __init__(self, n_channel, n_action):\n",
    "        super(CNNDQN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=n_channel, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        self.fc1= nn.Linear(7*7*64, 512)\n",
    "        self.fc2= nn.Linear(512, n_action)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        # get action from policy action and epsilon greedy\n",
    "        if random.random() > epsilon: # get action from old q-values\n",
    "            state   = autograd.Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True).to(device)\n",
    "            q_value = self.forward(state)\n",
    "            q_value = q_value.cpu()\n",
    "            action  = q_value.max(1)[1].item()            \n",
    "        else: # get random action\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify simulator to support CNN DQNs\n",
    "\n",
    "Let's modify our simulator interface to accept a CNN DQN model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game_CNN(model):\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    state = get_state(obs)\n",
    "    while(not done):\n",
    "        action = model.act(state, epsilon_final)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        next_state = get_state2(next_obs)\n",
    "        env.render()\n",
    "        time.sleep(0.1)\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify training step for double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss_DoubleDQN(current_model, target_model, batch_size, gamma=0.99):     # from input only a model, you must input 2 models: current_model, and target_model\n",
    "    # get data from replay mode\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # convert to tensors\n",
    "    # Autograd automatically supports Tensors with requires_grad set to True.\n",
    "    state      = autograd.Variable(torch.FloatTensor(np.float32(state))).to(device)\n",
    "    next_state = autograd.Variable(torch.FloatTensor(np.float32(next_state)), volatile=True).to(device)\n",
    "    action     = autograd.Variable(torch.LongTensor(action)).to(device)\n",
    "    reward     = autograd.Variable(torch.FloatTensor(reward)).to(device)\n",
    "    done       = autograd.Variable(torch.FloatTensor(done)).to(device)\n",
    "\n",
    "    # calculate q-values and next q-values from deeplearning\n",
    "    q_values      = current_model(state)\n",
    "    next_q_values = current_model(next_state)\n",
    "    # double DQN add here\n",
    "    next_q_state_values = target_model(next_state)\n",
    "    ############################################################\n",
    "\n",
    "    # get q-value from propagated action in each step\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    # double DQN different here\n",
    "    next_q_value     = next_q_state_values.gather(1, torch.max(next_q_values, 1)[1].unsqueeze(1)).squeeze(1)\n",
    "    ############################################################################\n",
    "    # calculate expected q-value from q-function\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    # calculate loss value\n",
    "    loss = (q_value - autograd.Variable(expected_q_value.data)).pow(2).mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify training loop for double DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_DoubleDQN(env, current_model, target_model, eps_by_episode, optimizer, replay_buffer, episodes = 10000, batch_size=32, gamma = 0.99):\n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    episode_reward = 0\n",
    "\n",
    "    obs = env.reset()\n",
    "    state = get_state2(obs)\n",
    "    tot_reward = 0\n",
    "    tr = trange(episodes+1, desc='Agent training', leave=True)\n",
    "    for episode in tr:\n",
    "        tr.set_description(\"Agent training (episode{}) Avg Reward {}\".format(episode+1,tot_reward/(episode+1)))\n",
    "        tr.refresh() \n",
    "\n",
    "        # get action with q-values\n",
    "        epsilon = eps_by_episode(episode)\n",
    "        action = current_model.act(state, epsilon)\n",
    "        \n",
    "        # input action into state\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        next_state = get_state2(next_obs)\n",
    "        # save data into buffer\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        tot_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "        obs = next_obs\n",
    "        episode_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "            state = get_state2(obs)\n",
    "            all_rewards.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "            \n",
    "        if len(replay_buffer) > batch_size:\n",
    "            loss = compute_td_loss_DoubleDQN(current_model, target_model, batch_size, gamma)    #######\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        if episode % 500 == 0: # update target_model weight. The '500' is hyperparameter, you can change it as you want\n",
    "            update_target(current_model, target_model)\n",
    "            \n",
    "    plot(episode, all_rewards, losses)  \n",
    "    return current_model, target_model, all_rewards, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling DQN or the real DDQNs\n",
    "\n",
    "When you think about dueling, it must be Dread Pirate Roberts and Enigo Montoya...\n",
    "\n",
    "<img src=\"img/princess-bride.png\" title=\"\" style=\"width: 600px;\" />\n",
    "\n",
    "Oops!!! sorry, just kidding ;P\n",
    "\n",
    "### What is DDQNs?\n",
    "\n",
    "To see the details, read the [Dueling DQNs (DDQNs) paper](https://arxiv.org/abs/1511.06581).\n",
    "\n",
    "DDQNs are different from the double DQNs. Both variations assume some form of duality,\n",
    "but while double DQN has two separate models, the DDQN is one model split at the base.\n",
    "\n",
    "<img src=\"img/RL2_DDQN3.png\" title=\"\" style=\"width: 800px;\" />\n",
    "\n",
    "DDQN extends the concept of a fixed Q target and extends that to a concept called *advantage*.\n",
    "The advantage indicates what additional value one action has against other actions.\n",
    "The Q-value in DDQNs is computed with the following two functions:\n",
    "\n",
    "$$Q(s,a)=V(s)+A(s,a)-\\frac{1}{|A|}\\sum_{a'=1}^{|A|} A(s,a')$$\n",
    "\n",
    " - $V(s)$: state-value function, the value of being in state $s$\n",
    " - $A(s,a)$: state-dependent action advantage function, estimating how much better it is to take an action $a$ than other actions $a'$ in the same state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(nn.Module):\n",
    "    def __init__(self, n_channel, n_action):\n",
    "        super(DDQN, self).__init__()        \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=n_channel, out_channels=32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1)\n",
    "        \n",
    "        # advantage layer: output is n_action as usual\n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(7*7*64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_action)\n",
    "        )\n",
    "        \n",
    "        # policy value: value action\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(7*7*64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        advantage = self.advantage(x)\n",
    "        value     = self.value(x)\n",
    "\n",
    "        # recalculate Q-value\n",
    "        return value + advantage - advantage.mean()\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            state   = autograd.Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True).to(device)\n",
    "            q_value = self.forward(state)\n",
    "            action  = q_value.max(1)[1].item()\n",
    "        else:\n",
    "            action = random.randrange(env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'SpaceInvaders-v0'\n",
    "env = gym.make(env_id)\n",
    "\n",
    "current_model = DDQN(3, env.action_space.n).to(device)\n",
    "target_model = DDQN(3, env.action_space.n).to(device)\n",
    "\n",
    "optimizer = optim.Adam(current_model.parameters())\n",
    "replay_buffer = ReplayBuffer(1000)\n",
    "\n",
    "update_target(current_model, target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Agent training (episode18) Avg Reward 0.0:   0%|                                    | 14/50001 [00:00<13:20, 62.46it/s]<ipython-input-12-dc51b58d6d0d>:37: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  state   = autograd.Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True).to(device)\n",
      "Agent training (episode33) Avg Reward 0.0:   0%|                                    | 26/50001 [00:00<13:48, 60.29it/s]<ipython-input-10-7ae0234ad0cb>:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  next_state = autograd.Variable(torch.FloatTensor(np.float32(next_state)), volatile=True).to(device)\n",
      "Agent training (episode133) Avg Reward 0.0:   0%|                                | 132/50001 [00:29<3:04:50,  4.50it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-74cf0e61c3fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcurrent_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_rewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_DoubleDQN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_by_episode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplay_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.99\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-7258cf6ae86f>\u001b[0m in \u001b[0;36mtrain_DoubleDQN\u001b[1;34m(env, current_model, target_model, eps_by_episode, optimizer, replay_buffer, episodes, batch_size, gamma)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_td_loss_DoubleDQN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcurrent_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m#######\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-7ae0234ad0cb>\u001b[0m in \u001b[0;36mcompute_td_loss_DoubleDQN\u001b[1;34m(current_model, target_model, batch_size, gamma)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[1;32m--> 221\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    223\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "current_model, target_model, all_rewards, losses = train_DoubleDQN(env, current_model, target_model, eps_by_episode, optimizer, replay_buffer, episodes = 50000, batch_size=32, gamma = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_game_CNN(current_model)\n",
    "time.sleep(3)\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
