{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab6 - st121413\n",
    "1. Get the demo up and running.\n",
    "2. Evaluate the pretrained COCO model on the COCO validation set we used last week.\n",
    "3. Download the Cityscapes dataset and run Mask R-CNN on it in inference model. Report your results and see what errors you find.\n",
    "4. Fine tune the COCO Mask R-CNN on Cityscapes and report the results. Are they close to what's reported in the Mask R-CNN paper?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get the demo running\n",
    "\n",
    "Firstly, I did run the mask-rcnn from the github using a appropriate dockerfile on Guppy.\n",
    "\n",
    "Since it is easier to run Mask RCNN on pytorch (in my opinion), I changed the model to torchvision one.\n",
    "\n",
    "## 2. Evaluate the pretrained COCO model on the COCO validation set we used last week.\n",
    "\n",
    "The procedure is simple, just follow along the lab manual.\n",
    "\n",
    "Lab manual: https://nbviewer.jupyter.org/github/dsai-asia/RTML/blob/main/Labs/06-Mask-R-CNN/Mask%20R-CNN.ipynb\n",
    "\n",
    "My Code: https://github.com/akraradets/2021JanRTML/blob/main/lab6/lab6%20-%20torchvision.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download the Cityscapes dataset and run Mask R-CNN on it in inference model. Report your results and see what errors you find.\n",
    "\n",
    "My Code: https://github.com/akraradets/2021JanRTML/blob/main/lab6/lab6%20-%20cityScape.ipynb\n",
    "\n",
    "At first, it is a dark path. I try to used the torchvision.Cityscapes but it required ton of modification.\n",
    "\n",
    "I think I have two choises.\n",
    "\n",
    "1. To write an entire evaluation method that compatible with Cityscpae dataset. I did try and put it away.\n",
    "\n",
    "2. To convert Cityscape 'targets' to be the same as COCO and resure the evaluation.\n",
    "\n",
    "I choose the second option.\n",
    "\n",
    "### Convert annotation format\n",
    "\n",
    "The convertion, at first, did not seem any easier than write a new evaluation method. During searching on Google, I found this gem.\n",
    "\n",
    "- https://github.com/TillBeemelmanns/cityscapes-to-coco-conversion\n",
    "\n",
    "There is another one of Cityscape to COCO but it is not as straight forward as this one. I converted the annotation of Cityscapes to COCO format and later found out that, in Cityscapes folder, there is already a converted annotation.\n",
    "\n",
    "Surprisingly, it is Chaichan who creates the annotation which use the same converter. Therefore, I deleted my coco_annotations folder to avoid the confusion of another students.\n",
    "\n",
    "### get_coco -> get_city\n",
    "\n",
    "I further modify the get_coco function to get_city in the coco_utils.py and use it instead of it predecessor\n",
    "\n",
    "- https://github.com/akraradets/2021JanRTML/blob/3cd71bba5b88d5fa5d1137e660b7bd7d778b83d4/lab6/coco_utils.py#L250\n",
    "\n",
    "### Result\n",
    "\n",
    "The evaluation on Cityscape using the pretrain weight is the following.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test:  [  0/125]  eta: 0:08:53  model_time: 0.8902 (0.8902)  evaluator_time: 1.8352 (1.8352)  time: 4.2664  data: 1.5106  max mem: 96\n",
    "# Test:  [100/125]  eta: 0:01:56  model_time: 0.9412 (0.9667)  evaluator_time: 1.9705 (2.0491)  time: 4.5169  data: 1.5770  max mem: 96\n",
    "# Test:  [124/125]  eta: 0:00:04  model_time: 0.6053 (0.9235)  evaluator_time: 1.0402 (1.9251)  time: 2.9955  data: 0.9301  max mem: 96\n",
    "# Test: Total time: 0:09:08 (4.3871 s / it)\n",
    "# Averaged stats: model_time: 0.6053 (0.9235)  evaluator_time: 1.0402 (1.9251)\n",
    "# Accumulating evaluation results...\n",
    "# DONE (t=0.44s).\n",
    "# Accumulating evaluation results...\n",
    "# DONE (t=0.42s).\n",
    "# IoU metric: bbox\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.121\n",
    "#  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.200\n",
    "#  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.125\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.036\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.132\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.230\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.095\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.185\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.206\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.059\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.229\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.338\n",
    "# IoU metric: segm\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.106\n",
    "#  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.176\n",
    "#  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.106\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.014\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.092\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.240\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.105\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.175\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.188\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.031\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.186\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.334"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fine tune the COCO Mask R-CNN on Cityscapes and report the results. Are they close to what's reported in the Mask R-CNN paper?\n",
    "\n",
    "### Fine Tuning\n",
    "\n",
    "The procedure is still still as simple as following the tutorial given by Prof. Matt.\n",
    "- https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html\n",
    "\n",
    "Since the annotation is now matching the COCO dataset, I can just use the \"train_one_epoch, evaluate\" from \"engine\".\n",
    "\n",
    "I did not perform any transformation asaide from ToTensor.\n",
    "\n",
    "The choise of Optimizer with its parameter is followed the tutorial. At first, I was going to follow the paper but it seemed to take long time to train so I decided to not doing so.\n",
    "\n",
    "My Code: https://github.com/akraradets/2021JanRTML/blob/main/lab6/lab6%20-%20maskrcnnFinetune.ipynb\n",
    "\n",
    "### Result\n",
    "\n",
    "At the time of wrting, I just finish the lab and share my '1-epoch' and '2-epoch' fine tuning results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' The result of fine tuning 1 epoch'''\n",
    "# Test:  [  0/250]  eta: 0:12:12  model_time: 0.6394 (0.6394)  evaluator_time: 1.3813 (1.3813)  time: 2.9282  data: 0.8077  max mem: 0\n",
    "# Test:  [100/250]  eta: 0:07:09  model_time: 0.6048 (0.6130)  evaluator_time: 1.3224 (1.4001)  time: 2.6861  data: 0.7027  max mem: 0\n",
    "# Test:  [200/250]  eta: 0:02:20  model_time: 0.5639 (0.5935)  evaluator_time: 1.2845 (1.3663)  time: 2.6758  data: 0.7844  max mem: 0\n",
    "# Test:  [249/250]  eta: 0:00:02  model_time: 0.3754 (0.5672)  evaluator_time: 0.7086 (1.2905)  time: 1.4914  data: 0.3736  max mem: 0\n",
    "# Test: Total time: 0:11:02 (2.6485 s / it)\n",
    "# Averaged stats: model_time: 0.3754 (0.5672)  evaluator_time: 0.7086 (1.2905)\n",
    "# Accumulating evaluation results...\n",
    "# DONE (t=0.55s).\n",
    "# Accumulating evaluation results...\n",
    "# DONE (t=0.55s).\n",
    "# IoU metric: bbox\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.260\n",
    "#  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.463\n",
    "#  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.256\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.075\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.284\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.458\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.204\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.350\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.377\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.106\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.372\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.634\n",
    "# IoU metric: segm\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.217\n",
    "#  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.410\n",
    "#  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.194\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.025\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.190\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.447\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.199\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.314\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.329\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.061\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.287\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.590"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' The result of fine tuning 2 epoch'''\n",
    "# Test:  [  0/250]  eta: 0:10:24  model_time: 0.4313 (0.4313)  evaluator_time: 1.0983 (1.0983)  time: 2.4980  data: 0.9599  max mem: 0\n",
    "# Test:  [100/250]  eta: 0:07:03  model_time: 0.5325 (0.5922)  evaluator_time: 1.2472 (1.3409)  time: 2.5828  data: 0.7450  max mem: 0\n",
    "# Test:  [200/250]  eta: 0:02:18  model_time: 0.5515 (0.5778)  evaluator_time: 1.2341 (1.2966)  time: 2.6597  data: 0.8256  max mem: 0\n",
    "# Test:  [249/250]  eta: 0:00:02  model_time: 0.2933 (0.5535)  evaluator_time: 0.5217 (1.2157)  time: 1.2275  data: 0.3729  max mem: 0\n",
    "# Test: Total time: 0:10:53 (2.6127 s / it)\n",
    "# Averaged stats: model_time: 0.2933 (0.5535)  evaluator_time: 0.5217 (1.2157)\n",
    "# Accumulating evaluation results...\n",
    "# DONE (t=1.05s).\n",
    "# Accumulating evaluation results...\n",
    "# DONE (t=0.56s).\n",
    "# IoU metric: bbox\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.285\n",
    "#  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.491\n",
    "#  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.285\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.100\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.303\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.456\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.209\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.369\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.399\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.147\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.409\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.600\n",
    "# IoU metric: segm\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.244\n",
    "#  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.450\n",
    "#  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.226\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.042\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.212\n",
    "#  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.457\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.199\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.320\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.337\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.089\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.313\n",
    "#  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.553"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
