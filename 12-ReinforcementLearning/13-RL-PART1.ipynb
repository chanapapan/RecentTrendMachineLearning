{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 13: Reinforcement Learning (RL)\n",
    "\n",
    "Today we'll have a gentle introduction to reinforcement learning.\n",
    "The material in today's lab comes from these references:\n",
    "\n",
    " - Pytorch 1.x Reinforcement Learning Cookbook (Packtpub)\n",
    " - Hands-On Reinforcement Learning for Games (Packtpub)\n",
    " - https://www.kdnuggets.com/2018/03/5-things-reinforcement-learning.html\n",
    " - Reinforcement Learning: An Introduction (Sutton et al.)\n",
    " - https://github.com/werner-duvaud/muzero-general (simulator code)\n",
    "\n",
    "\n",
    "## Reinforcement learning\n",
    "\n",
    "Reinforcement Learning (RL) is a machine learning technique that enables an agent to learn in an interactive environment by trial and error using feedback on its actions and experiences.\n",
    "RL uses rewards and punishment as signals for \"good\" and \"bad\" behavior.\n",
    "\n",
    "Generally, at each step, the agent outputs an action, which is input to the environment. The environment evolves according to its dynamics, the agent observes the new state of the environment\n",
    "and (optionally) a reward, and the process continues until hopefully the agent learns what behavior maximizes its reward.\n",
    "\n",
    "\n",
    "## Markov decision process (MDP)\n",
    "\n",
    "A MDP is a discrete-time stochastic control process. The MDP model is based on the idea of an environment that evolves as a Markov chain.\n",
    "\n",
    "### Markov chain\n",
    "\n",
    "A Markov chain is a model of the dynamics of a discrete time system that obeys the (first order) \"Markov property,\" meaning that the state $s^{t+1}$ at time\n",
    "$t+1$ is conditionally independent of the state at times $0, \\ldots, t-1$ given the state at time $t$, i.e.,\n",
    "\n",
    "$$ p(s^{t+1} \\mid s^t, s^{t-1}, \\ldots, s^0) = p(s^{t+1} \\mid s^t). $$\n",
    "\n",
    "Informally, we might say that the current state is all you need to know to predict the next state.\n",
    "\n",
    "More precisely, a Markov chain is defined by a set of possible states $S={s_0, s_1, \\ldots, s_n}$ and a transition matrix $T(s,s')$\n",
    "containing the propbabilities of state $s$ transitioning to state $s'$. Here is a visualization of a simple Markov chain:\n",
    "\n",
    "You might be interested in [this Markov chain simulator](https://setosa.io/ev/markov-chains/).\n",
    "\n",
    "Now, the dynamics of the environment in a MDP are slightly different from that of a simple Markov chain. We have to consider how the agent's\n",
    "actions affect the system's dynamics. At each time step, rather than just\n",
    "transitioning randomly to the next state, we add the agent's action as an external input or disturbance $a \\in A$, so (assuming a small number of discrete\n",
    "states and actions) the transition probabilities become a 3D tensor of size $|S|\\times |A|\\times |S|$\n",
    "mapping each state/action pair to a probability distribution over the states.\n",
    "\n",
    "### A simple MDP\n",
    "\n",
    "Suppose we have three states and two actions and that the state/action transition tensor is as follows:\n",
    "\n",
    "To complete our simple MDP, we need a *reward function* $R$ and a *discount factor* $\\gamma$. \n",
    "Suppose $R = [ 1, 0, -1 ]$ and $\\gamma = 0.5$. Let's define our MDP in Python with PyTorch tensors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# State transition function\n",
    "\n",
    "T = torch.tensor([[[0.8, 0.1, 0.1],\n",
    "                   [0.1, 0.6, 0.3]],\n",
    "                  [[0.7, 0.2, 0.1],\n",
    "                   [0.1, 0.8, 0.1]],\n",
    "                  [[0.6, 0.2, 0.2],\n",
    "                   [0.1, 0.4, 0.5]]])\n",
    "\n",
    "# Reward function\n",
    "\n",
    "R = torch.tensor([1.,0.,-1.])\n",
    "\n",
    "# Discount factor\n",
    "\n",
    "gamma = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The agent's goal\n",
    "\n",
    "Once the MDP is defined, the agent's goal is to maximize its expected reward. If we start in state $s^0$ and perform a series\n",
    "of actions $a^0, a^1, \\ldots a^{T-1}$ placing us in state $s^1, s^2, \\ldots s^T$, we obtain the total reward\n",
    "\n",
    "$$ \\sum_{t=0}^T \\gamma^{t} R(s^t).$$\n",
    "\n",
    "The agent's goal is to behave so as to maximize the expected total reward. To do so,\n",
    "it should come up with a policy $\\pi : S \\times A \\rightarrow \\mathbb{R}$ giving a probability distribution\n",
    "over actions that can be executed in each state, then when in state $s$, sample action $a$ according to that\n",
    "distribution $\\pi(s,\\cdot)$, and repeat.\n",
    "\n",
    "Now the agent's goal can be clearly specified as finding an optimal policy\n",
    "\n",
    "$$ \\pi^* = \\textrm{argmax}_\\pi \\mathbb{E}_{a^t \\sim \\pi(s^t), s^{t} \\sim T(s^{t-1},a^{t-1})}\\left[ \\sum_{t=0}^T \\gamma^{t} R(s^t) \\right]$$\n",
    "\n",
    "Under a particular policy $\\pi$, then, the *value* of state $s$ is the expected reward we obtain by following $\\pi$ from state $s$:\n",
    "\n",
    "$$ V^\\pi(s) = \\mathbb{E}_{a^t \\sim \\pi(s^t), s^{t} \\sim T(s^{t-1},a^{t-1}) \\mid s^0=s}\\left[ R(s) + \\sum_{t=1}^T \\gamma^{t} R(s^t) \\right]$$\n",
    "\n",
    "The value function clearly obeys the *Bellman equations*\n",
    "\n",
    "$$ V^\\pi(s) = R(s) + \\gamma \\sum_{s',a'} \\pi(s,a') T(s,a',s') V^\\pi(s'). $$ \n",
    "\n",
    "### How good is a policy? Policy evaluation\n",
    "\n",
    "To determine how good a particular policy is, we use policy evaluation.\n",
    "Policy evaluation is an iterative algorithm. It starts with arbitrary values for each state\n",
    "and then iteratively updates the values based on the Bellman equations until the values converge.\n",
    "\n",
    "You can see this algorithm's pseudocode in Sutton's book on page 75.\n",
    "\n",
    "Here we compute the value of the three states in our MDP assuming the agent always peforms the first action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.6786,  0.6260, -0.4821])\n"
     ]
    }
   ],
   "source": [
    "def policy_evaluation(policy, trans_matrix, rewards, gamma, threshold):\n",
    "    n_state = policy.shape[0]\n",
    "    V = torch.zeros(n_state)\n",
    "    while True:\n",
    "        V_temp = torch.zeros(n_state)\n",
    "        for state, actions in enumerate(policy):\n",
    "            for action, action_prob in enumerate(actions):\n",
    "                V_temp[state] += action_prob * (rewards[state] + gamma * torch.dot(trans_matrix[state, action], V))\n",
    "        max_delta = torch.max(torch.abs(V-V_temp))\n",
    "        V = V_temp.clone()\n",
    "        if max_delta <= threshold:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "threshold = 0.0001\n",
    "policy_optimal = torch.tensor([[1.0, 0.0],\n",
    "                               [1.0, 0.0],\n",
    "                               [1.0, 0.0]])\n",
    "V = policy_evaluation(policy_optimal, T, R, gamma, threshold)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy iteration\n",
    "\n",
    "Policy iteration starts with a random policy then uses policy evaluation and the resulting values to iteratively improve the policy until an optimal policy is obtained. It is, however,\n",
    "slow, due to the policy evaluation loop within the policy iteration loop.\n",
    "\n",
    "Here's an implementation using a slightly different formulation fo the reward as a function $R(s,a,s')$ of the current state $s$, action taken $a$, and resulting state $s'$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1352, 0.2405, 0.1829])\n",
      "tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "def policy_evaluation(policy, trans_matrix, rewards, gamma, threshold):\n",
    "    n_state = policy.shape[0]\n",
    "    V = torch.zeros(n_state)\n",
    "    while True:\n",
    "        V_temp = torch.zeros(n_state)\n",
    "        for state in range(n_state):\n",
    "            action = int(policy[state].item())\n",
    "            for new_state in range(n_state):\n",
    "                trans_prop = trans_matrix[state, action, new_state]\n",
    "                reward = rewards[state, action, new_state]\n",
    "                V_temp[state] += trans_prop * (reward + gamma * V[new_state])\n",
    "        max_delta = torch.max(torch.abs(V-V_temp))\n",
    "        V = V_temp.clone()\n",
    "        if max_delta <= threshold:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def policy_improvement(trans_matrix, rewards, gamma):\n",
    "    n_state = trans_matrix.shape[0]\n",
    "    n_action = trans_matrix.shape[1]\n",
    "    policy = torch.zeros(n_state)\n",
    "\n",
    "    for state in range(n_state):\n",
    "        v_actions = torch.zeros(n_action)\n",
    "        for action in range(n_action):\n",
    "            for new_state in range(n_state):\n",
    "                trans_prop = trans_matrix[state, action, new_state]\n",
    "                reward = rewards[state, action, new_state]\n",
    "                v_actions[action] += trans_prop * (reward + gamma * V[new_state])\n",
    "        policy[state] = torch.argmax(v_actions)\n",
    "    return policy\n",
    "\n",
    "def policy_iteration(trans_matrix, rewards, gamma, threshold):\n",
    "    n_state = trans_matrix.shape[0]\n",
    "    n_action = trans_matrix.shape[1]\n",
    "    policy = torch.randint(high=n_action, size=(n_state,)).float()\n",
    "    while True:\n",
    "        V = policy_evaluation(policy, trans_matrix, rewards, gamma, threshold)\n",
    "        policy_improved = policy_improvement(trans_matrix, rewards, gamma)\n",
    "        if torch.equal(policy_improved, policy):\n",
    "            return V, policy_improved\n",
    "        policy = policy_improved\n",
    "\n",
    "# Reward R(s,a,a') example\n",
    "\n",
    "R2 = torch.tensor([[[0.1,0.,-0.2],\n",
    "                   [0.2,0.,-0.1]],\n",
    "                  [[0.3,0.,-0.5],\n",
    "                   [0.1,0.,-0.2]],\n",
    "                  [[0.2,0.,-0.1],\n",
    "                   [1.,0.,-1.]]])\n",
    "\n",
    "V_optimal, optimal_policy = policy_iteration(T, R2, gamma, threshold)\n",
    "print(V_optimal)\n",
    "print(optimal_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration\n",
    "\n",
    "Value iteration is a much more efficient way to obtain the optimial policy. It calculates the value\n",
    "of each state on the assumption that the agent will deterministically select the action $a$ in state $s$\n",
    "that maximizes the expected reward after that. Once this value converges, the optimal policy is just\n",
    "to select the best action according to that value function:\n",
    "\n",
    "$$ V^*(s)=R(s) + \\max_a \\gamma \\sum_{s'} T(s,a,s') V^*(s') $$\n",
    "\n",
    "$$ \\pi^*(s) = \\mathrm{argmax}_a \\sum_{s'} T(s, a, s') V^*(s').$$\n",
    "\n",
    "Instead of taking the expectation (average) of values across all actions, we pick the action that achieves the maximal policy values.\n",
    "As above, we use the slightly different formulation in which the reward $R(s,a,s')$ is given based on initial state, action, and resulting state:\n",
    "\n",
    "$$ V^*(s) = \\max_a \\sum_{s'} T(s,a,s') [ R(s,a,s') + \\gamma V^*(s')]$$\n",
    "$$ \\pi^* = \\mathrm{argmax}_a \\sum_{s'}T(s,a,s')[R(s,a,s')+\\gamma V^*(s')]$$\n",
    "\n",
    "Here's sample code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1352, 0.2405, 0.1829])\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(trans_matrix, rewards, gamma, threshold):\n",
    "    n_state = trans_matrix.shape[0]\n",
    "    n_action = trans_matrix.shape[1]\n",
    "    V = torch.zeros(n_state)\n",
    "    while True:\n",
    "        V_temp = torch.zeros(n_state)\n",
    "        for state in range(n_state):\n",
    "            v_actions = torch.zeros(n_action)\n",
    "            for action in range(n_action):\n",
    "                for new_state in range(n_state):\n",
    "                    trans_prop = trans_matrix[state, action, new_state]\n",
    "                    reward = rewards[state, action, new_state]\n",
    "                    v_actions[action] += trans_prop * (reward + gamma * V[new_state])\n",
    "            V_temp[state] = torch.max(v_actions)\n",
    "        max_delta = torch.max(torch.abs(V-V_temp))\n",
    "        V = V_temp.clone()\n",
    "        if max_delta <= threshold:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "V = value_iteration(T, R2, gamma, threshold)\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the optimum policy after Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1352, 0.2405, 0.1829])\n",
      "[0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def value_iteration(trans_matrix, rewards, gamma, threshold):\n",
    "    n_state = trans_matrix.shape[0]\n",
    "    n_action = trans_matrix.shape[1]\n",
    "    V = torch.zeros(n_state)\n",
    "    while True:\n",
    "        V_temp = torch.zeros(n_state)\n",
    "        for state in range(n_state):\n",
    "            v_actions = torch.zeros(n_action)\n",
    "            for action in range(n_action):\n",
    "                for new_state in range(n_state):\n",
    "                    trans_prop = trans_matrix[state, action, new_state]\n",
    "                    reward = rewards[state, action, new_state]\n",
    "                    v_actions[action] += trans_prop * (reward + gamma * V[new_state])\n",
    "            V_temp[state] = torch.max(v_actions)\n",
    "        max_delta = torch.max(torch.abs(V-V_temp))\n",
    "        V = V_temp.clone()\n",
    "        if max_delta <= threshold:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "# State transition function\n",
    "\n",
    "T = torch.tensor([[[0.8, 0.1, 0.1],\n",
    "                   [0.1, 0.6, 0.3]],\n",
    "                  [[0.7, 0.2, 0.1],\n",
    "                   [0.1, 0.8, 0.1]],\n",
    "                  [[0.6, 0.2, 0.2],\n",
    "                   [0.1, 0.4, 0.5]]])\n",
    "\n",
    "R2 = torch.tensor([[[0.1,0.,-0.2],\n",
    "                   [0.2,0.,-0.1]],\n",
    "                  [[0.3,0.,-0.5],\n",
    "                   [0.1,0.,-0.2]],\n",
    "                  [[0.2,0.,-0.1],\n",
    "                   [1.,0.,-1.]]])\n",
    "\n",
    "gamma = 0.5\n",
    "threshold = 0.0001\n",
    "\n",
    "#%%\n",
    "V = value_iteration(T, R2, gamma, threshold)\n",
    "\n",
    "print(V)\n",
    "# %%\n",
    "\n",
    "n_state = T.shape[0]\n",
    "n_action = T.shape[1]\n",
    "\n",
    "#%%\n",
    "opt_policy = [0]*n_state\n",
    "for state in range(n_state):\n",
    "    exp_reward = [0] * n_action\n",
    "    for action in range(n_action):\n",
    "        exp_reward[action] = 0\n",
    "        for new_state in range(n_state):\n",
    "            prob = T[state, action, new_state]\n",
    "            reward = R2[state, action, new_state]\n",
    "            exp_reward[action] += prob * reward * V[new_state]\n",
    "    opt_policy[state] = np.argmax(exp_reward)\n",
    "\n",
    "#%%\n",
    "print(opt_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
