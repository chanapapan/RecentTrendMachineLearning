{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 13: Reinforcement Learning (RL)\n",
    "\n",
    "# GYM & ENV + MC on/off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI Gym\n",
    "\n",
    "Install Gym:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym\n",
    "# or\n",
    "# !git clone https://github.com/openai/gym\n",
    "# !cd gym\n",
    "# pip install -a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see visualizations easily, you should run the simulator locally, not in Jupyter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atari games environment\n",
    "\n",
    "Atari games includes video games such as Alien, Pong, and Space Race. Here is the example of using the Space Invaders game.\n",
    "First, install the atari simulation environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install gym[atari]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Space Invaders environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        ...,\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[80, 89, 22],\n",
       "        [80, 89, 22],\n",
       "        [80, 89, 22],\n",
       "        ...,\n",
       "        [80, 89, 22],\n",
       "        [80, 89, 22],\n",
       "        [80, 89, 22]],\n",
       "\n",
       "       [[80, 89, 22],\n",
       "        [80, 89, 22],\n",
       "        [80, 89, 22],\n",
       "        ...,\n",
       "        [80, 89, 22],\n",
       "        [80, 89, 22],\n",
       "        [80, 89, 22]],\n",
       "\n",
       "       [[80, 89, 22],\n",
       "        [80, 89, 22],\n",
       "        [80, 89, 22],\n",
       "        ...,\n",
       "        [80, 89, 22],\n",
       "        [80, 89, 22],\n",
       "        [80, 89, 22]]], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "# create environment\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "\n",
    "# reset environments\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render the environment locally. (You cannot render in Jupyter, or let us know if you find a way!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After rendering, you'll want to close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "env.render()\n",
    "\n",
    "time.sleep(5) # wait before close\n",
    "\n",
    "# close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole (inverted pendulum):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "# create environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# reset environments\n",
    "env.reset()\n",
    "\n",
    "# render the environment\n",
    "env.render()\n",
    "\n",
    "time.sleep(5) # wait before close\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the action space available, and try to get a sample from the action space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Action Sapce :  Discrete(2)\n",
      "Sample from the action space :  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Available Action Sapce : \", env.action_space)\n",
    "\n",
    "action = env.action_space.sample() # random sample the action\n",
    "print(\"Sample from the action space : \", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute an action using `step()`.\n",
    "The `step` method returns the next state after the action is taken.\n",
    " - **new_state**: The new observation\n",
    " - **reward**: The reward associated with that action in that state.\n",
    " - **is_done**: A flag to tell the game end (True).\n",
    " - **info**: extra information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New state :  [-0.00476398  0.15695053 -0.00624808 -0.28541128]\n",
      "Reward :  1.0\n",
      "is_done :  False\n",
      "Info :  {}\n"
     ]
    }
   ],
   "source": [
    "new_state, reward, is_done, info = env.step(action)\n",
    "print(\"New state : \", new_state)\n",
    "print(\"Reward : \", reward)\n",
    "print(\"is_done : \", is_done)\n",
    "print(\"Info : \", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try stepping and play the game\n",
    "\n",
    "Let's make a *while* loop with a random agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_done = False\n",
    "# env.reset()\n",
    "\n",
    "# while not is_done: # continue stepping until terminal\n",
    "#     action = env.action_space.sample() # random sample the action\n",
    "#     new_state, reward, is_done, info = env.step(action)\n",
    "#     print(info)\n",
    "    \n",
    "#     env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo (MC) Method\n",
    "\n",
    "Monte Carlo method is a model-free which have no require any prior knowledge of the environment. MC method is more scalable than MDP. MC control is used for finding the optimal policy when a policy is not given. There are 2 basically of MC control: on-policy and off-policy. On-policy method learns about the optimal policy by executing the policy and evaluating and improving it, while Off-policy method learns about the optimal policy using data generated by another policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On-policy Monte Carlo control\n",
    "\n",
    "On-policy Monte Carlo works look-a-like to policy iteration which has 2 phases: evaluation and improvement.\n",
    " - Evaluation phase: it evaluates the **action-values** (called **Q-function** $Q(s,a)$) instead of evaluates the value function.\n",
    " - Improvement phase: the policy is updated by assigning the optimal action to each stage: $\\pi(s)=argmax_a Q(s,a)$\n",
    "\n",
    "In this code below, we add **epsilon-greedy** policy which it will not exploit the best action all the time. The equations are:\n",
    " - Epsilon ($\\epsilon$):\n",
    "\n",
    "      $\\pi(s,a)=\\frac{\\epsilon}{|A|}$  When $|A|$ is the number of all possible actions.\n",
    " - Greedy:\n",
    " \n",
    "      $\\pi(s,a)=1-\\epsilon + \\frac{\\epsilon}{|A|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "from collections import defaultdict\n",
    "\n",
    "env = gym.make('Blackjack-v0')\n",
    "\n",
    "def run_episode(env, Q, epsilon, n_action): # play 1 episode = 1 game\n",
    "    '''\n",
    "    Return 3 lists: states, actions, rewards\n",
    "    '''\n",
    "    state = env.reset()\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    states = []\n",
    "    is_done = False\n",
    "    \n",
    "    # without epsilon-greedy\n",
    "    # action = torch.randint(0, n_action, [1]).item()\n",
    "    ##################################################\n",
    "    \n",
    "    while not is_done:\n",
    "        # with epsilon-greedy\n",
    "        probs = torch.ones(n_action) * epsilon / n_action\n",
    "        \n",
    "        best_action = torch.argmax(Q[state]).item()\n",
    "        probs[best_action] += 1.0 - epsilon\n",
    "        \n",
    "        action = torch.multinomial(probs, 1).item() # select the action from multinomial distribution\n",
    "        #######################################################\n",
    "        \n",
    "        actions.append(action)\n",
    "        states.append(state)\n",
    "        \n",
    "        state, reward, is_done, info = env.step(action)\n",
    "        \n",
    "        rewards.append(reward)\n",
    "        \n",
    "    return states, actions, rewards\n",
    "\n",
    "\n",
    "\n",
    "def mc_control_on_policy(env, gamma, n_episode, epsilon):\n",
    "    \n",
    "    n_action = env.action_space.n\n",
    "    G_sum = defaultdict(float)\n",
    "    N = defaultdict(int)\n",
    "    Q = defaultdict(lambda: torch.empty(env.action_space.n))\n",
    "    \n",
    "    for episode in range(n_episode):\n",
    "        states_t, actions_t, rewards_t = run_episode(env, Q, epsilon, n_action)\n",
    "        return_t = 0\n",
    "        G = {}\n",
    "        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):\n",
    "            return_t = gamma * return_t + reward_t\n",
    "            G[(state_t, action_t)] = return_t\n",
    "            for state_action, return_t in G.items():\n",
    "                state, action = state_action\n",
    "                if state[0] <= 21:\n",
    "                    G_sum[state_action] += return_t\n",
    "                    N[state_action] += 1\n",
    "                    Q[state][action] = G_sum[state_action] / N[state_action]\n",
    "    policy = {}\n",
    "    for state, actions in Q.items():\n",
    "        policy[state] = torch.argmax(actions).item()\n",
    "    return Q, policy\n",
    "\n",
    "\n",
    "def simulate_episode(env, policy):\n",
    "    state = env.reset()\n",
    "    is_done= False\n",
    "    while not is_done:\n",
    "        action = policy[state]\n",
    "        state, reward, is_done, info = env.step(action)\n",
    "        if is_done:\n",
    "            return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after episode 100, win  48  lose  41\n"
     ]
    }
   ],
   "source": [
    "gamma = 1\n",
    "n_episode = 500000\n",
    "epsilon = 0.1\n",
    "optimal_Q, optimal_policy = mc_control_on_policy(env, gamma, n_episode, epsilon)\n",
    "# print(optimal_policy)\n",
    "# print(optimal_Q)\n",
    "\n",
    "n_episode = 100\n",
    "n_win_optimal = 0\n",
    "n_lose_optimal = 0\n",
    "for _ in range(n_episode):\n",
    "    reward = simulate_episode(env, optimal_policy)\n",
    "    if reward == 1:\n",
    "        n_win_optimal += 1\n",
    "    elif reward == -1:\n",
    "        n_lose_optimal += 1\n",
    "print('after episode 100, win ', n_win_optimal, ' lose ', n_lose_optimal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Off-policy Monte Carlo control\n",
    "\n",
    "The Off-policy method optimizes the **target policy** ($\\pi$) using data generated by another policy (**behavior policy** ($b$)).\n",
    " - Target policy: exploitation purposes, greedy with respect to its current Q-function.\n",
    " - Behavior policy: exploration purposes, generate behavior which the target policy used for learning. The behavior policy can be anything to confirm that it can explore all possibilities, then all actions and all states can be chosen with non-zero probabilities.\n",
    "\n",
    "The weight importand for state-action pair is calculated as:\n",
    "\n",
    "$w_t=\\sum_{k=t}[\\pi(a_k|s_k)/b(a_k|s_k)]$\n",
    " - $\\pi(a_k|s_k)$: probabilities of taking action $a_k$ in state $s_k$\n",
    " - $b(a_k|s_k)$: probabilities under the behavior policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gym\n",
    "from collections import defaultdict\n",
    "\n",
    "env = gym.make('Blackjack-v0')\n",
    "\n",
    "def gen_random_policy(n_action):\n",
    "    probs = torch.ones(n_action) / n_action\n",
    "    def policy_function(state):\n",
    "        return probs\n",
    "    return policy_function\n",
    "\n",
    "random_policy = gen_random_policy(env.action_space.n)\n",
    "\n",
    "def run_episode(env, behavior_policy):\n",
    "    state = env.reset()\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    states = []\n",
    "    is_done = False\n",
    "    while not is_done:\n",
    "        probs = behavior_policy(state)\n",
    "        action = torch.multinomial(probs, 1).item()\n",
    "        actions.append(action)\n",
    "        states.append(state)\n",
    "        state, reward, is_done, info = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if is_done:\n",
    "            break\n",
    "    return states, actions, rewards\n",
    "\n",
    "def mc_control_off_policy(env, gamma, n_episode, behavior_policy):\n",
    "    n_action = env.action_space.n\n",
    "    G_sum = defaultdict(float)\n",
    "    N = defaultdict(int)\n",
    "    Q = defaultdict(lambda: torch.empty(n_action))\n",
    "    for episode in range(n_episode):\n",
    "        W = {}\n",
    "        w = 1\n",
    "        states_t, actions_t, rewards_t = run_episode(env, behavior_policy)\n",
    "        return_t = 0\n",
    "        G = {}\n",
    "        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):\n",
    "            return_t = gamma * return_t + reward_t\n",
    "            G[(state_t, action_t)] = return_t\n",
    "            w *= 1./ behavior_policy(state_t)[action_t]\n",
    "            W[(state_t, action_t)] = w\n",
    "            if action_t != torch.argmax(Q[state_t]).item():\n",
    "                break\n",
    "            \n",
    "        for state_action, return_t in G.items():\n",
    "            state, action = state_action\n",
    "            if state[0] <= 21:\n",
    "                G_sum[state_action] += return_t * W[state_action]\n",
    "                N[state_action] += 1\n",
    "                Q[state][action] = G_sum[state_action] / N[state_action]\n",
    "    policy = {}\n",
    "    for state, actions in Q.items():\n",
    "        policy[state] = torch.argmax(actions).item()\n",
    "    return Q, policy\n",
    "\n",
    "\n",
    "\n",
    "def simulate_episode(env, policy):\n",
    "    state = env.reset()\n",
    "    is_done= False\n",
    "    while not is_done:\n",
    "        action = policy[state]\n",
    "        state, reward, is_done, info = env.step(action)\n",
    "        if is_done:\n",
    "            return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after episode 100, win  41  lose  51\n"
     ]
    }
   ],
   "source": [
    "gamma = 1\n",
    "n_episode = 500000\n",
    "optimal_Q, optimal_policy = mc_control_off_policy(env, gamma, n_episode, random_policy)\n",
    "# print(optimal_policy)\n",
    "# print(optimal_Q)\n",
    "\n",
    "n_episode = 100\n",
    "n_win_optimal = 0\n",
    "n_lose_optimal = 0\n",
    "for _ in range(n_episode):\n",
    "    reward = simulate_episode(env, optimal_policy)\n",
    "    if reward == 1:\n",
    "        n_win_optimal += 1\n",
    "    elif reward == -1:\n",
    "        n_lose_optimal += 1\n",
    "print('after episode 100, win ', n_win_optimal, ' lose ', n_lose_optimal)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
