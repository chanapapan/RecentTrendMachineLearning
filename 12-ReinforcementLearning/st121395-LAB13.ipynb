{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTML LAB 13 Reinforcement Learning\n",
    "# Chanapa Pananookooln | st121395"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the tic-tac-toe game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game(AbstractGame):\n",
    "    \"\"\"\n",
    "    Game wrapper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, seed=None):\n",
    "        self.board = numpy.zeros((3, 3), dtype=\"int32\")\n",
    "        self.player = 0     ## player O as 0 and player X as 1\n",
    "\n",
    "    def to_play(self):\n",
    "        \"\"\"\n",
    "        Return the current player.\n",
    "\n",
    "        Returns:\n",
    "            The current player, it should be an element of the players list in the config. \n",
    "        \"\"\"\n",
    "        return self.player\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the game for a new game.\n",
    "        \n",
    "        Returns:\n",
    "            Initial observation of the game.\n",
    "        \"\"\"\n",
    "        self.board = numpy.zeros((3, 3), dtype=\"int32\")\n",
    "        self.player = 1     ## player O as 1 and player X as -1\n",
    "        return self.get_observation()\n",
    "\n",
    "    def get_observation(self):\n",
    "        \"\"\"\n",
    "        Return current state of the game\n",
    "        Returns:\n",
    "            The current observation\n",
    "        \"\"\"\n",
    "        board_player1 = numpy.where(self.board == 1, 1, 0)\n",
    "        board_player2 = numpy.where(self.board == -1, 1, 0)\n",
    "        board_to_play = numpy.full((3, 3), self.player)\n",
    "        return numpy.array([board_player1, board_player2, board_to_play], dtype=\"int32\")\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Apply action to the game.\n",
    "        \n",
    "        Args:\n",
    "            action : action of the action_space to take.\n",
    "\n",
    "        Returns:\n",
    "            The new observation, the reward and a boolean if the game has ended.\n",
    "        \"\"\"\n",
    "        # action is a number 0-8 for declare row and column of position in the board\n",
    "        row = action // 3\n",
    "        col = action % 3\n",
    "\n",
    "        # Check that the action is illegal action, unless the player should loss from illegal action.\n",
    "        if not (action in self.legal_actions()):\n",
    "            return self.get_observation(), -1, True\n",
    "\n",
    "        # input the action of current player into the board\n",
    "        self.board[row, col] = self.player\n",
    "\n",
    "        # Check that the game is finished in 2 condition: have a winner, or no any moves left\n",
    "        have_win = self.have_winner()\n",
    "        done = have_win or len(self.legal_actions()) == 0\n",
    "\n",
    "        # If have the winner, the current player should be a winner.\n",
    "        reward = 1 if have_win else 0\n",
    "\n",
    "        # change current player\n",
    "        self.player *= -1\n",
    "\n",
    "        return self.get_observation(), reward, done\n",
    "\n",
    "    def legal_actions(self):\n",
    "        \"\"\"\n",
    "        Should return the legal actions at each turn, if it is not available, it can return\n",
    "        the whole action space. At each turn, the game have to be able to handle one of returned actions.\n",
    "        \n",
    "        For complex game where calculating legal moves is too long, the idea is to define the legal actions\n",
    "        equal to the action space but to return a negative reward if the action is illegal.\n",
    "    \n",
    "        Returns:\n",
    "            An array of integers, subset of the action space.\n",
    "        \"\"\"\n",
    "        legal = []\n",
    "        for i in range(9):\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "            if self.board[row, col] == 0:\n",
    "                legal.append(i)\n",
    "        return legal\n",
    "\n",
    "    def have_winner(self):\n",
    "        \"\"\"\n",
    "        Check that have winner.\n",
    "    \n",
    "        Returns:\n",
    "            True if there is a winner, otherwise False\n",
    "        \"\"\"\n",
    "        # Horizontal and vertical checks\n",
    "        for i in range(3):\n",
    "            if (self.board[i, :] == self.player * numpy.ones(3, dtype=\"int32\")).all():\n",
    "                return True\n",
    "            if (self.board[:, i] == self.player * numpy.ones(3, dtype=\"int32\")).all():\n",
    "                return True\n",
    "\n",
    "        # Diagonal checks\n",
    "        if (\n",
    "            self.board[0, 0] == self.player\n",
    "            and self.board[1, 1] == self.player\n",
    "            and self.board[2, 2] == self.player\n",
    "        ):\n",
    "            return True\n",
    "        if (\n",
    "            self.board[2, 0] == self.player\n",
    "            and self.board[1, 1] == self.player\n",
    "            and self.board[0, 2] == self.player\n",
    "        ):\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Display the game observation.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def human_to_action(self):\n",
    "        \"\"\"\n",
    "        For multiplayer games, ask the user for a legal action\n",
    "        and return the corresponding action number.\n",
    "\n",
    "        Returns:\n",
    "            An integer from the action space.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def expert_agent(self):\n",
    "        \"\"\"\n",
    "        Hard coded agent that MuZero faces to assess his progress in multiplayer games.\n",
    "        It doesn't influence training\n",
    "\n",
    "        Returns:\n",
    "            Action as an integer to take in the current game state\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def action_to_string(self, action_number):\n",
    "        \"\"\"\n",
    "        Convert an action number to a string representing the action.\n",
    "        \n",
    "        Args:\n",
    "            action_number: an integer from the action space.\n",
    "\n",
    "        Returns:\n",
    "            String representing the action.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN module\n",
    "fully connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd \n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "from tqdm import trange\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_state, n_action, env, device):\n",
    "        super(DQN, self).__init__()        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(n_state, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, n_action)\n",
    "        )\n",
    "        self.env = env\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def act(self, state, epsilon):\n",
    "        # Get an epsilon greedy action for given state\n",
    "        if random.random() > epsilon: # Use argmax_a Q(s,a)\n",
    "            state = autograd.Variable(torch.FloatTensor(state).unsqueeze(0), volatile=True).to(self.device)\n",
    "            q_value = self.forward(state)\n",
    "            q_value = q_value.cpu()\n",
    "            action = q_value.max(1)[1].item()            \n",
    "        else: # get random action\n",
    "            action = random.randrange(self.env.action_space.n)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer class\n",
    "for storing recent state-action-reward tuples in a buffer and train on random subsamples from the buffer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        # Add batch index dimension to state representations\n",
    "        state = np.expand_dims(state, 0)\n",
    "        next_state = np.expand_dims(next_state, 0)            \n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for calculating the loss and training and plot the results of training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_loss(model, optimizer, batch_size, replay_buffer, device, gamma=0.99):\n",
    "\n",
    "    # Get batch from replay buffer\n",
    "    state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "\n",
    "    # Convert to tensors. Creating Variables is not necessary with more recent PyTorch versions.\n",
    "    state      = autograd.Variable(torch.FloatTensor(np.float32(state))).to(device)\n",
    "    next_state = autograd.Variable(torch.FloatTensor(np.float32(next_state)), volatile=True).to(device)\n",
    "    action     = autograd.Variable(torch.LongTensor(action)).to(device)\n",
    "    reward     = autograd.Variable(torch.FloatTensor(reward)).to(device)\n",
    "    done       = autograd.Variable(torch.FloatTensor(done)).to(device)\n",
    "\n",
    "    # Calculate Q(s) and Q(s')\n",
    "    q_values      = model(state)\n",
    "    next_q_values = model(next_state)\n",
    "\n",
    "    # Get Q(s,a) and max_a' Q(s',a')\n",
    "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value     = next_q_values.max(1)[0]\n",
    "    # Calculate target for Q(s,a): r + gamma max_a' Q(s',a')\n",
    "    # Note that the done signal is used to terminate recursion at end of episode.\n",
    "    expected_q_value = reward + gamma * next_q_value * (1 - done)\n",
    "    \n",
    "    # Calculate MSE loss. Variables are not needed in recent PyTorch versions.\n",
    "    loss = (q_value - autograd.Variable(expected_q_value.data)).pow(2).mean()\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def plot(episode, rewards, losses):\n",
    "    # clear_output(True)\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('episode %s. reward: %s' % (episode, np.mean(rewards[-10:])))\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('loss')\n",
    "    plt.plot(losses)   \n",
    "    plt.show()\n",
    "\n",
    "def train(env, model, eps_by_episode, optimizer, replay_buffer, device, episodes = 10000, batch_size=32, gamma = 0.99):\n",
    "    losses = []\n",
    "    all_rewards = []\n",
    "    episode_reward = 0\n",
    "    tot_reward = 0\n",
    "    tr = trange(episodes+1, desc='Agent training', leave=True)\n",
    "\n",
    "    # Get initial state input\n",
    "    state = env.reset()\n",
    "\n",
    "    # Execute episodes iterations\n",
    "    for episode in tr:\n",
    "        tr.set_description(\"Agent training (episode{}) Avg Reward {}\".format(episode+1,tot_reward/(episode+1)))\n",
    "        tr.refresh() \n",
    "\n",
    "        # Get initial epsilon greedy action\n",
    "        epsilon = eps_by_episode(episode)\n",
    "        action = model.act(state, epsilon)\n",
    "        \n",
    "        # Take a step\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Append experience to replay buffer\n",
    "        replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        tot_reward += reward\n",
    "        episode_reward += reward\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "        # Start a new episode if done signal is received\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            all_rewards.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "\n",
    "        # Train on a batch if we've got enough experience\n",
    "        if len(replay_buffer) > batch_size:\n",
    "            loss = compute_td_loss(model,optimizer, batch_size, replay_buffer, device, gamma)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "    plot(episode, all_rewards, losses)  \n",
    "    return model,all_rewards, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of the main file for training the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def gen_eps_by_episode(epsilon_start, epsilon_final, epsilon_decay):\n",
    "    eps_by_episode = lambda episode: epsilon_final + (epsilon_start - epsilon_final) * math.exp(-1. * episode / epsilon_decay)\n",
    "    return eps_by_episode\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 500\n",
    "eps_by_episode = gen_eps_by_episode(epsilon_start, epsilon_final, epsilon_decay)\n",
    "\n",
    "import importlib\n",
    "\n",
    "game_name = 'tictactoe'\n",
    "game_module = importlib.import_module(\"games.\" + game_name)\n",
    "env = game_module.Game()\n",
    "\n",
    "model = DQN(env.observation_space.shape[0], env.action_space.n, env, device).to(device)\n",
    "    \n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "replay_buffer = ReplayBuffer(1000)\n",
    "\n",
    "model, all_rewards, losses = train(env, model, eps_by_episode, optimizer, replay_buffer, device,episodes = 10000, batch_size=32, gamma = 0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "I did not success in implementing the function for transforming the tic-tac-toe state into a one hot representation.\n",
    "\n",
    "The representation should have a shape that is compatible so it can be input into the DQN network.\n",
    "\n",
    "I also was not success in defining the rewards for tic-tac-toe\n",
    "But here I have here the DQN network, the replay buffer and parts of the main file for training.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
